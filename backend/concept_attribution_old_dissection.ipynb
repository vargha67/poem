{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fju_7FVOsM_e"
      },
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3M-_AXBfsnAM"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import OrderedDict\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from imageio import imwrite\n",
        "from sklearn.feature_selection import SelectKBest, VarianceThreshold, mutual_info_classif\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader, Subset, sampler\n",
        "from torchvision import datasets, transforms, models\n",
        "from torchvision.datasets.folder import pil_loader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ciyu6mjlu_Rk"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Cuda available:\", torch.cuda.is_available())\n",
        "\n",
        "seed = 2021\n",
        "torch.manual_seed(seed)\n",
        "if device.type == 'cuda':\n",
        "    torch.cuda.manual_seed_all(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stkhwEOFwLA6"
      },
      "source": [
        "settings_path = '/content/drive/My Drive/Python Projects/POEM Pipeline Results/settings.pkl'\n",
        "with open(settings_path, 'rb') as f:\n",
        "    settings = pickle.load(f)\n",
        "\n",
        "current_setting_path = '/content/drive/My Drive/Python Projects/POEM Pipeline Results/current_setting.txt'\n",
        "with open(current_setting_path, 'r') as f:\n",
        "    current_setting_title = f.read().splitlines()[0]\n",
        "    print('Current setting:', current_setting_title)\n",
        "\n",
        "model_settings = settings['model_settings']\n",
        "model_dataset_settings = settings['model_dataset_settings']\n",
        "\n",
        "title_parts = current_setting_title.split('_')\n",
        "model_name = title_parts[0]\n",
        "dataset_name = '_'.join(title_parts[1:]) \n",
        "dataset_pure_name = dataset_name.split('_')[0]\n",
        "\n",
        "current_setting = model_dataset_settings[current_setting_title] \n",
        "num_classes = current_setting['num_classes']\n",
        "layer_names = [model_settings[model_name]['target_layer']]\n",
        "\n",
        "image_size = 224\n",
        "batch_size = 32\n",
        "\n",
        "min_iou = 0.04\n",
        "quantile_thresh = 0.99\n",
        "min_thresh_pixels = 1\n",
        "n_top_channels_per_concept = 3\n",
        "overlay_opacity = 0.5\n",
        "\n",
        "filter_concepts = False\n",
        "low_variance_thresh = 0.99\n",
        "max_concepts = 10\n",
        "\n",
        "norm_mean = (0.485, 0.456, 0.406)\n",
        "norm_std = (0.229, 0.224, 0.225)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_imported_packages(packages_path):\n",
        "\n",
        "    # Saving imported packages and their versions: \n",
        "    import sys\n",
        "    modules_info = []\n",
        "\n",
        "    for module in sys.modules:\n",
        "        if len(module.split('.')) > 1:   # ignoring subpackages\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            modules_info.append((module, sys.modules[module].__version__))\n",
        "        except:\n",
        "            try:\n",
        "                if type(sys.modules[module].version) is str:\n",
        "                    modules_info.append((module, sys.modules[module].version))\n",
        "                else:\n",
        "                    modules_info.append((module, sys.modules[module].version()))\n",
        "            except:\n",
        "                try:\n",
        "                    modules_info.append((module, sys.modules[module].VERSION))\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "    modules_info.sort(key=lambda x: x[0])\n",
        "    with open(packages_path, 'w') as f:\n",
        "        for m in modules_info:\n",
        "            f.write('{} {}\\n'.format(m[0], m[1]))"
      ],
      "metadata": {
        "id": "rhtGFeqmzJ25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vgg16_model (*args, **kwargs):\n",
        "\n",
        "    # A version of vgg16 model where layers are given their research names: \n",
        "    model = models.vgg16(*args, **kwargs)\n",
        "    model.features = nn.Sequential(OrderedDict(zip([\n",
        "        'conv1_1', 'relu1_1',\n",
        "        'conv1_2', 'relu1_2',\n",
        "        'pool1',\n",
        "        'conv2_1', 'relu2_1',\n",
        "        'conv2_2', 'relu2_2',\n",
        "        'pool2',\n",
        "        'conv3_1', 'relu3_1',\n",
        "        'conv3_2', 'relu3_2',\n",
        "        'conv3_3', 'relu3_3',\n",
        "        'pool3',\n",
        "        'conv4_1', 'relu4_1',\n",
        "        'conv4_2', 'relu4_2',\n",
        "        'conv4_3', 'relu4_3',\n",
        "        'pool4',\n",
        "        'conv5_1', 'relu5_1',\n",
        "        'conv5_2', 'relu5_2',\n",
        "        'conv5_3', 'relu5_3',\n",
        "        'pool5'],\n",
        "        model.features)))\n",
        "\n",
        "    model.classifier = nn.Sequential(OrderedDict(zip([\n",
        "        'fc6', 'relu6',\n",
        "        'drop6',\n",
        "        'fc7', 'relu7',\n",
        "        'drop7',\n",
        "        'fc8a'],\n",
        "        model.classifier)))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "_6Ny91HtjAwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJ0vAw6Z8lEd"
      },
      "source": [
        "def load_model (model_path):\n",
        "\n",
        "    if model_name == 'vgg16':\n",
        "        model = vgg16_model(num_classes=num_classes)\n",
        "    else:\n",
        "        model = models.__dict__[model_name](num_classes=num_classes)\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "jr8Zs1AIgehH"
      },
      "source": [
        "class ImageDataset (Dataset):\n",
        "    \n",
        "    def __init__(self, images_path, file_names, labels, transform):\n",
        "        self.images_path = images_path\n",
        "        self.file_names = file_names\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.file_names)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        fname = self.file_names[index]\n",
        "        path = os.path.join(self.images_path, fname)\n",
        "        img = Image.open(path)\n",
        "        \n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        \n",
        "        if self.labels is not None:\n",
        "            label = self.labels[index]\n",
        "            return img, label\n",
        "        \n",
        "        return img, fname"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomImageFolder (datasets.ImageFolder):\n",
        "\n",
        "    # Override this method to also return image paths\n",
        "    def __getitem__(self, index):\n",
        "        original_tuple = super(CustomImageFolder, self).__getitem__(index)\n",
        "        path = self.imgs[index][0]\n",
        "        tuple_with_path = (original_tuple + (path,))\n",
        "        return tuple_with_path"
      ],
      "metadata": {
        "id": "omOOrtcN4-0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9XV60Q_wd-y"
      },
      "source": [
        "def load_data (dataset_dir=None): \n",
        "    \n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(image_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=norm_mean, std=norm_std)\n",
        "    ])\n",
        "\n",
        "    # data_set = None\n",
        "\n",
        "    # if dataset == 'imagenette':\n",
        "    #     data_set = datasets.ImageFolder(root='imagenette2-320/train', transform=transform)\n",
        "\n",
        "    # elif dataset == 'places2':\n",
        "    #     data_set = datasets.ImageFolder(root='places2/train', transform=transform)\n",
        "\n",
        "    # elif dataset == 'cifar10':\n",
        "    #     data_set = datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
        "\n",
        "    # elif dataset == 'indoor':\n",
        "    #     data_set = CustomImageFolder(root='indoor_subset', transform=transform)\n",
        "\n",
        "    # elif dataset == 'imagewoof':\n",
        "    #     data_set = datasets.ImageFolder(root='imagewoof2-320/train', transform=transform)\n",
        "\n",
        "    #     if (target_classes != None) and (len(target_classes) > 0):\n",
        "    #         class_indexes_dict = data_set.class_to_idx\n",
        "    #         data_indexes = data_set.targets\n",
        "\n",
        "    #         target_class_indexes = [v for k,v in class_indexes_dict.items() if k in target_classes]\n",
        "    #         target_data_indexes = [i for i,x in enumerate(data_indexes) if x in target_class_indexes]\n",
        "\n",
        "    #         data_set = Subset(data_set, target_data_indexes)\n",
        "\n",
        "    # else: \n",
        "    #     data = pd.read_csv(labels_path)\n",
        "    #     file_names = data['File'].tolist()\n",
        "    #     labels = data['Label'].tolist()\n",
        "        \n",
        "    #     data_set = ImageDataset(images_path, file_names, labels, transform=transform)\n",
        "\n",
        "    dataset = CustomImageFolder(root=dataset_dir, transform=transform)\n",
        "    data_loader = DataLoader(dataset, batch_size=batch_size)\n",
        "    print('Processing {} data examples in {} batches with class index {}'.format(len(dataset), len(data_loader), dataset.class_to_idx))\n",
        "    \n",
        "    return data_loader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hcqNr3QxOxK"
      },
      "source": [
        "def load_channels_data (tally_path, thresholds_path):\n",
        "\n",
        "    tally_data = pd.read_csv(tally_path)\n",
        "    tally_data = tally_data[tally_data['score'] > min_iou]\n",
        "\n",
        "    channels_list = tally_data['unit'].tolist()\n",
        "    concepts_list = tally_data['label'].tolist()\n",
        "\n",
        "    channel_concept_map = {k:v for k,v in zip(channels_list, concepts_list)}\n",
        "    channels = list(set(channels_list))\n",
        "    concepts = list(set(concepts_list))\n",
        "\n",
        "    channels.sort()\n",
        "    concepts.sort()\n",
        "\n",
        "    thresholds = np.load(thresholds_path)\n",
        "    channel_thresh_map = {i+1:t for i,t in enumerate(thresholds) if i+1 in channels}\n",
        "\n",
        "    print('Processing {} concepts and {} channels'.format(len(concepts), len(channels)))\n",
        "    print('channel_concept_map:', channel_concept_map)\n",
        "    print('channel_thresh_map:', channel_thresh_map)\n",
        "\n",
        "    return channel_concept_map, channel_thresh_map, channels, concepts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_file_name_from_path (image_path):\n",
        "\n",
        "    ind = image_path.rfind('/')\n",
        "    image_fname = image_path[ind+1:]\n",
        "    return image_fname"
      ],
      "metadata": {
        "id": "263CCkTiYaco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_class_titles (ds_name):\n",
        "\tctitles = {}\n",
        "\tname_parts = ds_name.split('_')\n",
        "\tif len(name_parts) <= 1:\n",
        "\t\treturn ctitles\n",
        "\t\n",
        "\tn_classes = len(name_parts[1:])\n",
        "\tfor i,p in enumerate(name_parts[1:]):\n",
        "\t\tctitles[i] = p\n",
        "\t\t\n",
        "\treturn ctitles"
      ],
      "metadata": {
        "id": "Kyph7DQboMFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvhRIG6_MmM4"
      },
      "source": [
        "def extract_concepts_from_batch (acts, channel_concept_map, channel_thresh_map, channels, concepts):\n",
        "\n",
        "    batch_concepts_counts = []\n",
        "    batch_channels_counts = []\n",
        "    num_images = acts.shape[0]\n",
        "    \n",
        "    for i in range(num_images):\n",
        "        act = acts[i]\n",
        "        num_channels = act.shape[0]\n",
        "        image_concepts_counts = {con:0 for con in concepts}\n",
        "        image_channels_counts = {ch:0 for ch in channels}\n",
        "\n",
        "        for ch in channels:   # range(num_channels)\n",
        "            ch_index = ch - 1\n",
        "            channel_activation = act[ch_index]\n",
        "            channel_thresh = channel_thresh_map[ch] if ch in channel_thresh_map else None\n",
        "            channel_concept = channel_concept_map[ch] if ch in channel_concept_map else None\n",
        "\n",
        "            if (channel_activation is None) or (channel_thresh is None) or (channel_concept is None):\n",
        "                print('Error: Missing activation, concept ({}), or threshold ({}) for channel {}!'.format(channel_concept, channel_thresh, ch))\n",
        "                continue\n",
        "\n",
        "            num_high_thresh = np.sum(channel_activation > channel_thresh)\n",
        "            if num_high_thresh >= min_thresh_pixels:\n",
        "                image_concepts_counts[channel_concept] += 1\n",
        "                image_channels_counts[ch] = num_high_thresh\n",
        "                if ch not in channels:\n",
        "                    print('Error: channel {} not among channels list!'.format(ch))\n",
        "\n",
        "        batch_concepts_counts.append(image_concepts_counts)\n",
        "        batch_channels_counts.append(image_channels_counts)\n",
        "\n",
        "    return batch_concepts_counts, batch_channels_counts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_concepts (model, data_loader, channel_concept_map, channel_thresh_map, channels, concepts):\n",
        "\n",
        "    batch_activations = []\n",
        "\n",
        "    def get_activations(module, input, output):\n",
        "        batch_activations.append(output.data.cpu().numpy())\n",
        "\n",
        "    for name in layer_names:\n",
        "        #model._modules.get(name).register_forward_hook(get_activations)\n",
        "        layer = model._modules.get(name)\n",
        "        if layer is None:\n",
        "            for n,l in model.named_modules():\n",
        "                if n == name:\n",
        "                    layer = l\n",
        "                    print('Target layer found:', n)\n",
        "                    break\n",
        "        layer.register_forward_hook(get_activations)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    concepts_counts = {con:0 for con in concepts}\n",
        "    channels_counts = {ch:0 for ch in channels}\n",
        "    concepts_counts_by_class = {c:{con:0 for con in concepts} for c in classes}\n",
        "\n",
        "    concepts_rows_list = []\n",
        "    channels_rows_list = []\n",
        "    image_channels_counts_list = []\n",
        "    acts_list = []\n",
        "    num_images = 0\n",
        "    total_acc = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i, (images, labels, paths) in tqdm(enumerate(data_loader)):\n",
        "            del batch_activations[:]\n",
        "            images_gpu = images.to(device)\n",
        "            labels_gpu = labels.to(device)\n",
        "        \n",
        "            output = model(images_gpu)\n",
        "\n",
        "            _, preds = torch.max(output, 1)\n",
        "            total_acc += (preds == labels_gpu).float().sum()\n",
        "            preds = preds.cpu().numpy()\n",
        "            labels = labels.numpy()\n",
        "            images = images.numpy()\n",
        "            acts = batch_activations[0]   # currently assume there is only one target layer\n",
        "\n",
        "            if i == 0:\n",
        "                print('images: {}, labels: {}, preds: {}'.format(images.shape, labels.shape, preds.shape))\n",
        "                print('batch_activations.shape: {} * {}'.format(len(batch_activations), batch_activations[0].shape))\n",
        "                print('paths:', paths)\n",
        "\n",
        "            batch_concepts_counts, batch_channels_counts = extract_concepts_from_batch(acts, channel_concept_map, channel_thresh_map, channels, concepts)\n",
        "\n",
        "            for j in range(len(batch_concepts_counts)):\n",
        "                num_images += 1\n",
        "                image_concepts_counts = batch_concepts_counts[j]\n",
        "                image_channels_counts = batch_channels_counts[j]\n",
        "                pred = preds[j]\n",
        "                label = labels[j]\n",
        "                path = paths[j]\n",
        "                image = images[j]\n",
        "                act = acts[j]\n",
        "                fname = get_file_name_from_path(path)\n",
        "\n",
        "                if i == 0 and j == 0:\n",
        "                    print('Image {} with label {}, pred {}, act shape {}, and concepts counts {}'\n",
        "                        .format(path, label, pred, act.shape, image_concepts_counts))\n",
        "                    \n",
        "                acts_list.append(act)\n",
        "                image_channels_counts_list.append(image_channels_counts)\n",
        "\n",
        "                image_concepts_row = {k:1 if v > 0 else 0 for k,v in image_concepts_counts.items()}\n",
        "                image_concepts_row['pred'] = pred\n",
        "                image_concepts_row['label'] = label\n",
        "                image_concepts_row['id'] = num_images\n",
        "                image_concepts_row['file'] = fname\n",
        "                image_concepts_row['path'] = path\n",
        "                concepts_rows_list.append(image_concepts_row)\n",
        "\n",
        "                image_channels_row = {k:1 if v > 0 else 0 for k,v in image_channels_counts.items()}\n",
        "                image_channels_row['pred'] = pred\n",
        "                image_channels_row['label'] = label\n",
        "                image_channels_row['id'] = num_images\n",
        "                image_channels_row['file'] = fname\n",
        "                image_channels_row['path'] = path\n",
        "                channels_rows_list.append(image_channels_row)\n",
        "                \n",
        "                for con in concepts:\n",
        "                    cnt = image_concepts_counts[con]\n",
        "                    val = 1 if cnt > 0 else 0\n",
        "                    concepts_counts[con] += val\n",
        "                    concepts_counts_by_class[pred][con] += val\n",
        "\n",
        "                for ch in channels:\n",
        "                    cnt = image_channels_counts[ch]\n",
        "                    channels_counts[ch] += 1 if cnt > 0 else 0\n",
        "\n",
        "    total_acc = total_acc / num_images\n",
        "    print('\\nExtracted concepts from {} images with accuracy {:.3f}.'.format(num_images, total_acc))\n",
        "    print('\\nConcept counts:', concepts_counts)\n",
        "    for c,counts in concepts_counts_by_class.items():\n",
        "        print('\\nConcept counts of class {}: {}'.format(c, counts))\n",
        "    print('\\nChannel counts:', channels_counts)\n",
        "\n",
        "    concepts_df = pd.DataFrame(concepts_rows_list)\n",
        "    channels_df = pd.DataFrame(channels_rows_list)\n",
        "\n",
        "    return concepts_df, channels_df, acts_list, image_channels_counts_list"
      ],
      "metadata": {
        "id": "DTvXUb-JBt5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_extracted_concepts (concepts_df, channels_df, channel_concept_map):\n",
        "\n",
        "    preds_df = concepts_df['pred']\n",
        "    meta_cols = ['pred', 'label', 'id', 'file', 'path']\n",
        "    meta_df = concepts_df[meta_cols]\n",
        "    concept_cols = list(set(concepts_df.columns) - set(meta_cols))\n",
        "    concept_cols.sort()\n",
        "    cons_df = concepts_df[concept_cols]\n",
        "\n",
        "    initial_concepts = list(cons_df.columns)\n",
        "    print('Initial concepts ({}): {}'.format(len(initial_concepts), initial_concepts))\n",
        "    \n",
        "    var_selector = VarianceThreshold(threshold=(low_variance_thresh * (1 - low_variance_thresh)))\n",
        "    var_selector.fit(cons_df)\n",
        "    var_col_indices = var_selector.get_support(indices=True)\n",
        "    cons_df = cons_df.iloc[:,var_col_indices]\n",
        "    var_filtered_concepts = list(cons_df.columns)\n",
        "    var_removed_concepts = set(initial_concepts) - set(var_filtered_concepts)\n",
        "    print('Concepts removed by variance filtering ({}): {}'.format(len(var_removed_concepts), var_removed_concepts))\n",
        "\n",
        "    k = max_concepts if len(var_filtered_concepts) > max_concepts else 'all'\n",
        "    mut_selector = SelectKBest(mutual_info_classif, k=k)\n",
        "    mut_selector.fit(cons_df, preds_df)\n",
        "    mut_col_indices = mut_selector.get_support(indices=True)\n",
        "    cons_df = cons_df.iloc[:,mut_col_indices]\n",
        "    filtered_concepts = list(cons_df.columns)\n",
        "    mut_removed_concepts = set(var_filtered_concepts) - set(filtered_concepts)\n",
        "    print('Concepts removed by mutual info filtering ({}): {}'.format(len(mut_removed_concepts), mut_removed_concepts))\n",
        "    print('Concepts reduced from {} to {} by concept filtering.'.format(len(initial_concepts), len(filtered_concepts)))\n",
        "    print('Final concepts after filtering ({}): {}'.format(len(filtered_concepts), filtered_concepts))\n",
        "\n",
        "    filtered_concepts_df = pd.concat([cons_df, meta_df], axis=1)\n",
        "    display(filtered_concepts_df.head())\n",
        "\n",
        "    channel_cols = list(set(channels_df.columns) - set(meta_cols))\n",
        "    filtered_channels = [ch for ch in channel_cols if (ch in channel_concept_map) and (channel_concept_map[ch] in filtered_concepts)]\n",
        "    print('Channels reduced from {} to {} by concept filtering.'.format(len(channel_cols), len(filtered_channels)))\n",
        "\n",
        "    cols_to_keep = filtered_channels + meta_cols\n",
        "    filtered_channels_df = channels_df[cols_to_keep]\n",
        "    display(filtered_channels_df.head())\n",
        "\n",
        "    return filtered_concepts_df, filtered_channels_df, filtered_concepts, filtered_channels"
      ],
      "metadata": {
        "id": "k0G6-OBwB1eS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_activation_images (image_index, image_path, image_fname, acts, image_channels_counts, channel_concept_map, channel_thresh_map, concepts, output_dir):\n",
        "\n",
        "    image_activated_channels = [k for k,v in image_channels_counts.items() if v > 0]\n",
        "    if len(image_activated_channels) == 0:\n",
        "        print('Image {} with path {} has no activated channels!'.format(image_index, image_path))\n",
        "        return 0\n",
        "\n",
        "    # # Un-normalizing the image back to its original form: \n",
        "    # torch_image = torch.from_numpy(image)\n",
        "    # torch_image.mul_(torch.as_tensor(norm_std).view(-1,1,1)).add_(torch.as_tensor(norm_mean).view(-1,1,1))   # normalization actually did the reverse: torch_image.sub_(norm_mean).div_(norm_std)\n",
        "    # image = torch_image.numpy()\n",
        "\n",
        "    # Preferred to reopen the image and apply the initial resize transform to it without the normalization step, instead of manual un-normalization:\n",
        "    image = Image.open(image_path)\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(image_size),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "    image = transform(image).numpy()\n",
        "\n",
        "    # Changing the shape of image from CxHxW to HxWxC format: \n",
        "    image = np.transpose(image, (1, 2, 0))   # image.permute(1, 2, 0) in PyTorch\n",
        "\n",
        "    # Normalizing the pixel values to (0, 255) range required later for saving the image: \n",
        "    image_min, image_max = np.min(image), np.max(image)\n",
        "    image = (((image - image_min) / (image_max - image_min)) * 255).astype(np.uint8)\n",
        "    if image_index in [1,2]:\n",
        "        print('image_min: {}, image_max: {}, new_image.min: {}, new_image.max: {}'.format(image_min, image_max, np.min(image), np.max(image)))\n",
        "\n",
        "    image_concept_channels = {con:[] for con in concepts}\n",
        "    for ch in image_activated_channels:\n",
        "        channel_concept = channel_concept_map[ch]\n",
        "        num_high_thresh = image_channels_counts[ch]\n",
        "\n",
        "        if (channel_concept is None) or (num_high_thresh is None):\n",
        "            #print('Error: Missing concept ({}) or number of high-thresh pixels ({}) for channel {}!'.format(channel_concept, num_high_thresh, ch))\n",
        "            continue\n",
        "            \n",
        "        image_concept_channels[channel_concept].append((ch, num_high_thresh))\n",
        "\n",
        "    cnt = 0\n",
        "    for con,lst in image_concept_channels.items():\n",
        "        if len(lst) == 0:\n",
        "            continue\n",
        "\n",
        "        top_channel_nums = sorted(lst, key=lambda x: x[1], reverse=True)[:n_top_channels_per_concept]\n",
        "        top_channels = [k for k,v in top_channel_nums]\n",
        "        if image_index in [1,2]:\n",
        "            print('Top channels for concept {}: {}'.format(con, top_channel_nums))\n",
        "\n",
        "        for i,ch in enumerate(top_channels):\n",
        "            ch_index = ch - 1\n",
        "            channel_activation = acts[ch_index]\n",
        "            channel_thresh = channel_thresh_map[ch]\n",
        "            channel_concept = con\n",
        "\n",
        "            if (channel_activation is None) or (channel_thresh is None):\n",
        "                print('Error: Missing activation or threshold ({}) for channel {}!'.format(channel_thresh, ch))\n",
        "                continue\n",
        "\n",
        "            should_print = (image_index in [1,2]) and (i == 0)\n",
        "\n",
        "            mask = np.array(Image.fromarray(channel_activation).resize(size=(image.shape[1], image.shape[0]), resample=Image.BILINEAR))   # size=image.shape[:2]\n",
        "            mask = mask > channel_thresh\n",
        "            new_image = (mask[:, :, np.newaxis] * overlay_opacity + (1 - overlay_opacity)) * image\n",
        "\n",
        "            ind = image_fname.rfind('.')\n",
        "            image_fname_raw = image_fname[:ind]\n",
        "            new_fname = image_fname_raw + '_' + str(ch) + '_' + channel_concept + '.jpg'\n",
        "            new_path = output_dir + '/' + new_fname\n",
        "\n",
        "            final_image = new_image.astype(np.uint8)\n",
        "            if should_print:\n",
        "                print('new_image.min: {}, new_image.max: {}, final_image.min: {}, final_image.max: {}'\n",
        "                    .format(np.min(new_image), np.max(new_image), np.min(final_image), np.max(final_image)))\n",
        "\n",
        "            # new_image_scaled = (((new_image - new_image.min()) / (new_image.max() - new_image.min())) * 255).astype(np.uint8)\n",
        "            # if should_print:\n",
        "            #     new_min, new_max = np.min(new_image), np.max(new_image)\n",
        "            #     scaled_min, scaled_max = np.min(new_image_scaled), np.max(new_image_scaled)\n",
        "            #     print('new_min: {}, new_max: {}, scaled_min: {}, scaled_max: {}'.format(new_min, new_max, scaled_min, scaled_max))\n",
        "\n",
        "            imwrite(new_path, final_image)\n",
        "            cnt += 1\n",
        "    \n",
        "    print('Saved {} activation images for image {} with path {}'.format(cnt, image_index, image_path))\n",
        "    return cnt"
      ],
      "metadata": {
        "id": "qD4gQHpKiRVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_image_concepts_dataset (concepts_df, channels_df, image_channels_counts_list, acts_list, channel_concept_map, channel_thresh_map, filtered_concepts, \n",
        "                                 filtered_channels, concepts_output_path, channels_output_path, activation_images_path):\n",
        "\n",
        "    num_images = len(concepts_df.index)\n",
        "\n",
        "    filtered_concepts_counts = {con:0 for con in filtered_concepts}\n",
        "    filtered_channels_counts = {ch:0 for ch in filtered_channels}\n",
        "    filtered_concepts_counts_by_class = {c:{con:0 for con in filtered_concepts} for c in classes}\n",
        "\n",
        "    num_act_images_saved = 0\n",
        "\n",
        "    for i,con_row in concepts_df.iterrows():\n",
        "        ch_row = channels_df.iloc[i]\n",
        "        pred = con_row['pred']\n",
        "        id = con_row['id']\n",
        "        path = con_row['path']\n",
        "        fname = con_row['file']\n",
        "        act = acts_list[i]\n",
        "        image_channels_counts = image_channels_counts_list[i]\n",
        "        filtered_image_channels_counts = {ch:image_channels_counts[ch] for ch in image_channels_counts if ch in filtered_channels}\n",
        "\n",
        "        for con in filtered_concepts:\n",
        "            val = con_row[con]\n",
        "            filtered_concepts_counts[con] += val\n",
        "            filtered_concepts_counts_by_class[pred][con] += val\n",
        "\n",
        "        for ch in filtered_channels:\n",
        "            val = ch_row[ch]\n",
        "            filtered_channels_counts[ch] += val\n",
        "\n",
        "        num_act_images = save_activation_images(id, path, fname, act, filtered_image_channels_counts, channel_concept_map, channel_thresh_map, \n",
        "                                                filtered_concepts, activation_images_path)\n",
        "        num_act_images_saved += num_act_images\n",
        "\n",
        "    print('Saved {} activation images for {} images.'.format(num_act_images_saved, num_images))\n",
        "    print('\\nFiltered concept counts:', filtered_concepts_counts)\n",
        "    for c,counts in filtered_concepts_counts_by_class.items():\n",
        "        print('\\nFiltered concept counts of class {}: {}'.format(c, counts))\n",
        "    print('\\nFiltered channel counts:', filtered_channels_counts)\n",
        "\n",
        "    concepts_df.to_csv(concepts_output_path, index=False)\n",
        "    channels_df.to_csv(channels_output_path, index=False)"
      ],
      "metadata": {
        "id": "ZQnR80fJCC2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIegVXcdwLqK"
      },
      "source": [
        "model_file = 'model.pth'\n",
        "dataset_dir = 'dataset'\n",
        "drive_result_path = '/content/drive/My Drive/Python Projects/POEM Pipeline Results/' + model_name + '_' + dataset_name + '_old'\n",
        "\n",
        "dataset_file = dataset_dir + '.zip'\n",
        "drive_dataset_dir = drive_result_path + '/' + dataset_file\n",
        "!cp \"$drive_dataset_dir\" '.'\n",
        "!unzip -qq -n $dataset_file -d '.'\n",
        "\n",
        "model_path = drive_result_path + '/' + model_file\n",
        "!cp \"$model_path\" '.'\n",
        "\n",
        "result_dir = 'identification_results'\n",
        "result_file = result_dir + '.zip'\n",
        "result_path = drive_result_path + \"/\" + result_file\n",
        "!cp \"$result_path\" '.'\n",
        "!unzip -qq -n $result_file -d '.'\n",
        "\n",
        "activation_images_path = 'activation_images'\n",
        "if os.path.exists(activation_images_path):\n",
        "    shutil.rmtree(activation_images_path)\n",
        "os.makedirs(activation_images_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yKQO3WnJZKb"
      },
      "source": [
        "data_loader = load_data(dataset_dir)\n",
        "\n",
        "class_titles = extract_class_titles(dataset_name)\n",
        "classes = list(class_titles.keys())\n",
        "\n",
        "tally_path = './' + result_dir + '/tally.csv'\n",
        "thresholds_path = './' + result_dir + '/quantile.npy'\n",
        "channel_concept_map, channel_thresh_map, channels, concepts = load_channels_data(tally_path, thresholds_path)\n",
        "\n",
        "model = load_model(model_file)\n",
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "concepts_df, channels_df, acts_list, image_channels_counts_list = extract_concepts(model, data_loader, channel_concept_map, channel_thresh_map, channels, concepts)\n",
        "\n",
        "if filter_concepts:\n",
        "    concepts_df, channels_df, concepts, channels = filter_extracted_concepts(concepts_df, channels_df, channel_concept_map)"
      ],
      "metadata": {
        "id": "tf_PcX9O1FLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "concepts_output_path = 'image_concepts.csv'\n",
        "channels_output_path = 'image_channels.csv'\n",
        "\n",
        "save_image_concepts_dataset(concepts_df, channels_df, image_channels_counts_list, acts_list, channel_concept_map, channel_thresh_map, concepts, channels, \n",
        "                            concepts_output_path, channels_output_path, activation_images_path)"
      ],
      "metadata": {
        "id": "5cWE2tLy1Jsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp $concepts_output_path \"$drive_result_path\"\n",
        "!cp $channels_output_path \"$drive_result_path\"\n",
        "\n",
        "activation_images_file = activation_images_path + '.zip'\n",
        "!zip -qq -r $activation_images_file $activation_images_path\n",
        "!cp $activation_images_file '$drive_result_path'\n",
        "\n",
        "packages_path = drive_result_path + '/attribution_packages.log'\n",
        "save_imported_packages(packages_path)"
      ],
      "metadata": {
        "id": "vbndP_eRzav4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}