{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fju_7FVOsM_e"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKcGG5HZnrzU"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "# From PyTorch 1.11, the THC namespace is removed which prevents the PreciseRoIPooling library used in segmentation model to be built successfully\n",
        "# pip install torch==1.10.1+cu102 torchvision==0.11.2+cu102 torchaudio==0.10.1 torchtext>=0.11.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "pip install torch==1.10.1+cu102 torchvision==0.11.2+cu102 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "!(stat -t /usr/local/lib/*/dist-packages/google/colab > /dev/null 2>&1) && exit \n",
        "pip install ninja 2>> install.log\n",
        "git clone https://github.com/davidbau/dissect.git dissect 2>> install.log\n",
        "#pip list -v >> attribution_packages.log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TMr4v47GnshX"
      },
      "outputs": [],
      "source": [
        "try: # set up path\n",
        "    import google.colab, sys, torch\n",
        "    sys.path.append('/content/dissect')\n",
        "    if not torch.cuda.is_available():\n",
        "        print(\"Change runtime type to include a GPU.\")  \n",
        "except:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3M-_AXBfsnAM"
      },
      "outputs": [],
      "source": [
        "import torch, os, pickle, shutil, json, netdissect\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from collections import OrderedDict\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from imageio import imwrite\n",
        "from IPython.display import display\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import datasets, transforms, models\n",
        "from torchvision.datasets.folder import pil_loader\n",
        "from sklearn.feature_selection import SelectKBest, VarianceThreshold, mutual_info_classif\n",
        "\n",
        "from netdissect import nethook, renormalize, parallelfolder, upsample, imgviz, show\n",
        "from netdissect.easydict import EasyDict\n",
        "from netdissect.workerpool import WorkerPool\n",
        "from netdissect.imgsave import SaveImageWorker\n",
        "from experiment import dissect_experiment as experiment\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "print('PyTorch version:', torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ciyu6mjlu_Rk"
      },
      "outputs": [],
      "source": [
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# print(\"Cuda available:\", torch.cuda.is_available())\n",
        "\n",
        "# seed = 2021\n",
        "# torch.manual_seed(seed)\n",
        "# if device.type == 'cuda':\n",
        "#     torch.cuda.manual_seed_all(seed)\n",
        "#     torch.backends.cudnn.benchmark = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "stkhwEOFwLA6"
      },
      "outputs": [],
      "source": [
        "settings_path = '/content/drive/My Drive/Python Projects/POEM Pipeline Results/settings.pkl'\n",
        "with open(settings_path, 'rb') as f:\n",
        "    settings = pickle.load(f)\n",
        "\n",
        "current_setting_path = '/content/drive/My Drive/Python Projects/POEM Pipeline Results/current_setting.txt'\n",
        "with open(current_setting_path, 'r') as f:\n",
        "    current_setting_title = f.read().splitlines()[0]\n",
        "    print('Current setting:', current_setting_title)\n",
        "\n",
        "model_settings = settings['model_settings']\n",
        "model_dataset_settings = settings['model_dataset_settings']\n",
        "\n",
        "title_parts = current_setting_title.split('_')\n",
        "model_name = title_parts[0]\n",
        "dataset_name = '_'.join(title_parts[1:]) \n",
        "dataset_pure_name = dataset_name.split('_')[0]\n",
        "\n",
        "current_setting = model_dataset_settings[current_setting_title] \n",
        "num_classes = current_setting['num_classes']\n",
        "use_dissection_models = False\n",
        "target_layer = model_settings[model_name]['target_layer'] if not use_dissection_models else experiment.instrumented_layername(EasyDict(model=model_name))\n",
        "\n",
        "seg_model_name = 'netpc'\n",
        "image_size = 224\n",
        "batch_size = 32\n",
        "\n",
        "min_iou = 0.04\n",
        "activation_high_thresh = 0.95\n",
        "min_thresh_pixels = 10\n",
        "n_top_channels_per_concept = 3\n",
        "overlay_opacity = 0.5\n",
        "\n",
        "check_seg_overlap = True\n",
        "overlap_mode = 'overlap_to_activation_ratio'   # 'overlap_pixels_count', 'overlap_to_union_ratio', 'overlap_to_activation_ratio', 'overlap_to_segmentation_ratio'\n",
        "min_overlap_ratio = 0.5\n",
        "min_overlap_pixels = 5\n",
        "category_index_map = {\n",
        "    'object': 0,\n",
        "    'material': 1,\n",
        "    'part': 2,\n",
        "    'color': 3\n",
        "}\n",
        "\n",
        "check_gradients = True\n",
        "pool_gradients = False\n",
        "\n",
        "filter_concepts = True\n",
        "low_variance_thresh = 0.99\n",
        "max_concepts = 10\n",
        "\n",
        "binning_features = False\n",
        "activation_low_thresh = 0.7\n",
        "high_value = 2 if binning_features else 1\n",
        "mid_value = 1\n",
        "low_value = 0\n",
        "\n",
        "binning_classes = False\n",
        "certainty_thresh = 0.6\n",
        "\n",
        "norm_mean = (0.485, 0.456, 0.406)\n",
        "norm_std = (0.229, 0.224, 0.225)\n",
        "\n",
        "args = EasyDict(model=model_name, dataset=dataset_name, seg=seg_model_name, layer=target_layer, quantile=activation_high_thresh)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-mx9aeYclnD"
      },
      "outputs": [],
      "source": [
        "def save_imported_packages(packages_path):\n",
        "\n",
        "    # Saving imported packages and their versions: \n",
        "    import sys\n",
        "    modules_info = []\n",
        "\n",
        "    for module in sys.modules:\n",
        "        if len(module.split('.')) > 1:   # ignoring subpackages\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            modules_info.append((module, sys.modules[module].__version__))\n",
        "        except:\n",
        "            try:\n",
        "                if type(sys.modules[module].version) is str:\n",
        "                    modules_info.append((module, sys.modules[module].version))\n",
        "                else:\n",
        "                    modules_info.append((module, sys.modules[module].version()))\n",
        "            except:\n",
        "                try:\n",
        "                    modules_info.append((module, sys.modules[module].VERSION))\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "    modules_info.sort(key=lambda x: x[0])\n",
        "    with open(packages_path, 'w') as f:\n",
        "        for m in modules_info:\n",
        "            f.write('{} {}\\n'.format(m[0], m[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YueNOWlVyn5M"
      },
      "outputs": [],
      "source": [
        "def get_target_layer_name (final=False):\n",
        "\n",
        "    if final and use_dissection_models:\n",
        "        return experiment.instrumented_layername(args)\n",
        "    else:\n",
        "        if model_name is 'resnet18':\n",
        "            return 'layer4.1.conv2'   # 'layer4'\n",
        "        elif model_name is 'resnet50':\n",
        "            return 'layer4.2.conv3'\n",
        "        elif model_name is 'vgg16':\n",
        "            return 'features.conv5_3'   # 'features.28'\n",
        "        elif model_name is 'alexnet':\n",
        "            return 'conv5'\n",
        "\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7-916mCYnWw"
      },
      "outputs": [],
      "source": [
        "def vgg16_model (*args, **kwargs):\n",
        "\n",
        "    # A version of vgg16 model where layers are given their research names: \n",
        "    model = models.vgg16(*args, **kwargs)\n",
        "    model.features = nn.Sequential(OrderedDict(zip([\n",
        "        'conv1_1', 'relu1_1',\n",
        "        'conv1_2', 'relu1_2',\n",
        "        'pool1',\n",
        "        'conv2_1', 'relu2_1',\n",
        "        'conv2_2', 'relu2_2',\n",
        "        'pool2',\n",
        "        'conv3_1', 'relu3_1',\n",
        "        'conv3_2', 'relu3_2',\n",
        "        'conv3_3', 'relu3_3',\n",
        "        'pool3',\n",
        "        'conv4_1', 'relu4_1',\n",
        "        'conv4_2', 'relu4_2',\n",
        "        'conv4_3', 'relu4_3',\n",
        "        'pool4',\n",
        "        'conv5_1', 'relu5_1',\n",
        "        'conv5_2', 'relu5_2',\n",
        "        'conv5_3', 'relu5_3',\n",
        "        'pool5'],\n",
        "        model.features)))\n",
        "\n",
        "    model.classifier = nn.Sequential(OrderedDict(zip([\n",
        "        'fc6', 'relu6',\n",
        "        'drop6',\n",
        "        'fc7', 'relu7',\n",
        "        'drop7',\n",
        "        'fc8a'],\n",
        "        model.classifier)))\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8GM10WR8wXd"
      },
      "outputs": [],
      "source": [
        "def load_model (model_file=None):\n",
        "\n",
        "    if use_dissection_models:\n",
        "        model = experiment.load_model(args)\n",
        "    else:\n",
        "        if model_name == 'vgg16':\n",
        "            model = vgg16_model(num_classes=num_classes)\n",
        "        else:\n",
        "            model = models.__dict__[model_name](num_classes=num_classes)\n",
        "        model.load_state_dict(torch.load(model_file))\n",
        "        model = nethook.InstrumentedModel(model).cuda().eval()\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "omOOrtcN4-0t"
      },
      "outputs": [],
      "source": [
        "class CustomImageFolder (datasets.ImageFolder):\n",
        "\n",
        "    # Override this method to also return image paths\n",
        "    def __getitem__(self, index):\n",
        "        original_tuple = super(CustomImageFolder, self).__getitem__(index)\n",
        "        path = self.imgs[index][0]\n",
        "        tuple_with_path = (original_tuple + (path,))\n",
        "        return tuple_with_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9XV60Q_wd-y"
      },
      "outputs": [],
      "source": [
        "def load_data (dataset_dir=None): \n",
        "    \n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(image_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=norm_mean, std=norm_std)\n",
        "    ])\n",
        "\n",
        "    dataset = None\n",
        "    if use_dissection_models:\n",
        "        dataset = experiment.load_dataset(args)\n",
        "    else:\n",
        "        dataset = CustomImageFolder(root=dataset_dir, transform=transform)\n",
        "        #dataset = parallelfolder.ParallelImageFolders([dataset_dir], classification=True, shuffle=True, transform=transform)\n",
        "\n",
        "    data_loader = DataLoader(dataset, batch_size=batch_size)\n",
        "    print('Processing {} data examples in {} batches with class index {}'.format(len(dataset), len(data_loader), dataset.class_to_idx))\n",
        "    \n",
        "    return dataset, data_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_hcqNr3QxOxK"
      },
      "outputs": [],
      "source": [
        "def load_channels_data (tally_path):\n",
        "\n",
        "    f = open(tally_path)\n",
        "    tally_data = json.load(f)\n",
        "    if \"units\" not in tally_data:\n",
        "        print('Error: units not present in loaded tally data from path {}'.format(tally_path))\n",
        "        return None\n",
        "\n",
        "    f.close()\n",
        "    channels_data = tally_data[\"units\"]\n",
        "    channels_map = {}\n",
        "\n",
        "    for ch_item in channels_data:\n",
        "        if (('iou' not in ch_item) or ('unit' not in ch_item) or ('label' not in ch_item) or \n",
        "            ('cat' not in ch_item) or ('low_thresh' not in ch_item) or ('high_thresh' not in ch_item)): \n",
        "            print('Error: incomplete data in channel item:', ch_item)\n",
        "            continue\n",
        "\n",
        "        channel = ch_item['unit'] + 1\n",
        "        channels_map[channel] = {\n",
        "            'concept': ch_item['label'],\n",
        "            'category': ch_item['cat'],\n",
        "            'low_thresh': ch_item['low_thresh'],\n",
        "            'high_thresh': ch_item['high_thresh'],\n",
        "            'is_valid': True if ch_item['iou'] > min_iou else False\n",
        "        }\n",
        "\n",
        "        # iou = ch_item['iou']\n",
        "        # if iou > min_iou:\n",
        "        #     concept = ch_item['label']\n",
        "        #     category = ch_item['cat']\n",
        "        #     channel_concept_map[channel] = concept\n",
        "        #     channel_category_map[channel] = category\n",
        "\n",
        "    channels = list(set([k for k,v in channels_map.items()]))\n",
        "    concepts = list(set([v['concept'] for k,v in channels_map.items()]))\n",
        "\n",
        "    channels.sort()\n",
        "    concepts.sort()\n",
        "\n",
        "    # thresholds = np.load(thresholds_path)\n",
        "    # channel_thresh_map = {i+1:t for i,t in enumerate(thresholds) if i+1 in channels}\n",
        "\n",
        "    print('Processing {} concepts and {} channels'.format(len(concepts), len(channels)))\n",
        "    print('channels_map:', channels_map)\n",
        "\n",
        "    return channels_map, channels, concepts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "263CCkTiYaco"
      },
      "outputs": [],
      "source": [
        "def get_file_name_from_path (image_path):\n",
        "\n",
        "    ind = image_path.rfind('/')\n",
        "    image_fname = image_path[ind+1:]\n",
        "    return image_fname"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXQ9FHc83G1F"
      },
      "outputs": [],
      "source": [
        "def save_image_set (images, filenames):\n",
        "\n",
        "    pool = WorkerPool(worker=SaveImageWorker)\n",
        "    for img, fname in zip(images, filenames):\n",
        "        pool.add(img, fname)\n",
        "    \n",
        "    pool.join()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGuKKCidwh4y"
      },
      "outputs": [],
      "source": [
        "def get_binned_predictions (logits):\n",
        "\n",
        "    # If the softmax probability for any of the other classes is higher than the certainty_ratio (e.g. 2/3) of the top class probability, \n",
        "    # then it will be considered as `maybe` instead of certain. \n",
        "    # Number for maybe status of each class is equal to the class number plus total number of classes (e.g. 2 for 0 class in a binary setting).\n",
        "\n",
        "    certainty_ratio = (1.0 - certainty_thresh) / certainty_thresh\n",
        "\n",
        "    probs = torch.softmax(logits, dim=1)\n",
        "    top_probs, preds = torch.max(probs, dim=1)\n",
        "    top_probs = (top_probs * certainty_ratio).view(-1, 1).expand(-1, probs.shape[1])\n",
        "    probs_mask = probs > top_probs\n",
        "    probs_mask_sum = torch.sum(probs_mask, dim=1)\n",
        "    \n",
        "    for i in range(preds.shape[0]):\n",
        "        s = probs_mask_sum[i]\n",
        "        if s > 1: \n",
        "            preds[i] = preds[i] + num_classes\n",
        "\n",
        "    return preds.cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NA5zdWqpNI9t"
      },
      "outputs": [],
      "source": [
        "def extract_class_titles (ds_name):\n",
        "\tctitles = {}\n",
        "\tname_parts = ds_name.split('_')\n",
        "\tif len(name_parts) <= 1:\n",
        "\t\treturn ctitles\n",
        "\t\n",
        "\tn_classes = len(name_parts[1:])\n",
        "\tfor i,p in enumerate(name_parts[1:]):\n",
        "\t\tctitles[i] = p\n",
        "\t\tif binning_classes:\n",
        "\t\t\tctitles[i + n_classes] = 'maybe ' + p\n",
        "\t\t\n",
        "\treturn ctitles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3IP7QgMRjgr"
      },
      "outputs": [],
      "source": [
        "def plot_activation_histogram (act):\n",
        "\n",
        "    hist, bin_edges = torch.histogram(act, bins=10)\n",
        "    hist = hist.tolist()\n",
        "    bin_edges = bin_edges.tolist()\n",
        "    print(hist)\n",
        "    print(bin_edges)\n",
        "    x = []\n",
        "    for i in range(len(bin_edges)-1):\n",
        "        x.append('{:.4f}-{:.4f}'.format(bin_edges[i], bin_edges[i+1]))\n",
        "\n",
        "    plt.figure(figsize=(15, 15))\n",
        "    plt.bar(x, hist, align=\"center\")\n",
        "    plt.xlabel('Activation/Gradient Value')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_sample_image_activations (dataset, image, channel, seg, act, grad, act_grad, upact_grad, channels_map, seg_concept_index_map, image_threshs):\n",
        "\n",
        "    act = act[None, :, :, :]\n",
        "    grad = grad[None, :, :, :]\n",
        "    act_grad = act_grad[None, :, :, :]\n",
        "    iv = imgviz.ImageVisualizer(size=(image_size, image_size), image_size=(image_size, image_size), source=dataset)\n",
        "\n",
        "    img = renormalize.as_image(image, source=dataset)\n",
        "    ch_index = channel - 1\n",
        "    ch_info = channels_map[channel]\n",
        "    concept = ch_info['concept']\n",
        "    category = ch_info['category']\n",
        "    act_thresh = ch_info['high_thresh']\n",
        "    grad_thresh = image_threshs['high_thresh']\n",
        "    print('Visualizing filter {} mapped to concept {} from category {}, with activation thresh {} and gradient thresh {}'\n",
        "        .format(channel, concept, category, act_thresh, grad_thresh))\n",
        "\n",
        "    act_grad_high_mask = (upact_grad[ch_index] > grad_thresh)\n",
        "\n",
        "    seg_concept_index = seg_concept_index_map[concept]\n",
        "    cat_index = category_index_map[category]\n",
        "    target_seg = seg[cat_index]\n",
        "    target_seg_concept_mask = (target_seg == seg_concept_index)\n",
        "\n",
        "    overlap_mask = act_grad_high_mask & target_seg_concept_mask\n",
        "\n",
        "    print('Input image:')\n",
        "    show([[img]])\n",
        "\n",
        "    print('Segmentation mask:')\n",
        "    show([[iv.segmentation(target_seg_concept_mask)]])   # seg[0]\n",
        "\n",
        "    print('Activation-gradients mask:')\n",
        "    show([[iv.segmentation(act_grad_high_mask)]])\n",
        "\n",
        "    print('Segmentation and activation-gradient overlap mask:')\n",
        "    show([[iv.segmentation(overlap_mask)]])\n",
        "\n",
        "    print('Activations heatmap:')\n",
        "    show([[iv.heatmap(act, (0, ch_index), mode='nearest')]])\n",
        "\n",
        "    print('Gradients heatmap:')\n",
        "    print(grad[0, ch_index])\n",
        "    print(grad[0, ch_index].min(), grad[0, ch_index].max())\n",
        "    show([[iv.heatmap(grad, (0, ch_index), mode='nearest')]])\n",
        "\n",
        "    print('Activation-gradients heatmap:')\n",
        "    show([[iv.heatmap(act_grad, (0, ch_index), mode='nearest')]])\n",
        "\n",
        "    print('Segmentation highlighted:')\n",
        "    show([[iv.masked_image(image, target_seg_concept_mask.float(), level=0.99)]])\n",
        "\n",
        "    print('Activations highlighted:')\n",
        "    show([[iv.masked_image(image, act, (0, ch_index), level=act_thresh)]])\n",
        "\n",
        "    print('Activation-gradients highlighted:')\n",
        "    show([[iv.masked_image(image, act_grad, (0, ch_index), level=grad_thresh)]])\n",
        "\n",
        "    print('Segmentation and activation-gradient overlap highlighted:')\n",
        "    show([[iv.masked_image(image, overlap_mask.float(), level=0.99)]])"
      ],
      "metadata": {
        "id": "YPOgVIKBDn7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klOT5Ed5QkMf"
      },
      "outputs": [],
      "source": [
        "def extract_concepts_from_image (act, upact, seg, path, channels_map, seg_concept_index_map, channels, concepts, image_threshs=None):\n",
        "\n",
        "    num_channels = act.shape[0]\n",
        "    image_concepts = {con:low_value for con in concepts}  # used to hold the high/mid/low value of each concept for this image (used for image concepts file and saving activation images process)\n",
        "    image_channels = {ch:low_value for ch in channels}    # used to hold the high/mid/low value of each channel for this image (used for image channels file and saving activation images process)\n",
        "    image_concepts_counts = {con:0 for con in concepts}   # used to count how many channels related to each concept were high for this image (just used for stat keeping purposes)\n",
        "    image_channels_counts = {ch:0 for ch in channels}     # used to keep the number of mid/high thresh pixels for each channel for this image (used in saving activation images process)\n",
        "\n",
        "    for ch in channels:   # range(num_channels)\n",
        "        ch_index = ch - 1\n",
        "        ch_upact = upact[ch_index]\n",
        "        ch_info = channels_map[ch]\n",
        "        is_valid = ch_info['is_valid']\n",
        "        channel_concept = ch_info['concept']\n",
        "        channel_category = ch_info['category']\n",
        "        channel_high_thresh = image_threshs['high_thresh'] if check_gradients else ch_info['high_thresh']\n",
        "        channel_low_thresh = image_threshs['low_thresh'] if check_gradients else ch_info['low_thresh']\n",
        "\n",
        "        if (ch_upact is None) or (not is_valid) or (channel_concept is None) or (channel_category is None) or (channel_high_thresh is None):\n",
        "            continue\n",
        "\n",
        "        # Checking whether the channel concept can be considered as high value for this image: \n",
        "\n",
        "        is_high = False\n",
        "        ch_upact_high_mask = (ch_upact > channel_high_thresh)\n",
        "        num_high_thresh = np.sum(ch_upact_high_mask.numpy())\n",
        "\n",
        "        if num_high_thresh >= min_thresh_pixels:\n",
        "            if check_seg_overlap:\n",
        "                seg_concept_index = seg_concept_index_map[channel_concept] if channel_concept in seg_concept_index_map else None\n",
        "                cat_index = category_index_map[channel_category] if channel_category in category_index_map else None\n",
        "                if (seg_concept_index is None) or (cat_index is None):\n",
        "                    print('Error: Missing segmentation concept index ({}) or category index ({}) for channel {} mapped to concept {} from category {}'\n",
        "                        .format(seg_concept_index, cat_index, ch, channel_concept, channel_category))\n",
        "                else:\n",
        "                    target_seg = seg[cat_index]\n",
        "                    target_seg_concept_mask = (target_seg == seg_concept_index)\n",
        "                    num_concept_seg = np.sum(target_seg_concept_mask.numpy())\n",
        "                    overlap_mask = ch_upact_high_mask & target_seg_concept_mask\n",
        "                    num_overlap = np.sum(overlap_mask.numpy())\n",
        "\n",
        "                    if overlap_mode is 'overlap_to_union_ratio':\n",
        "                        union_mask = ch_upact_high_mask | target_seg_concept_mask\n",
        "                        num_union = np.sum(union_mask.numpy())\n",
        "                        overlap_ratio = num_overlap / num_union\n",
        "                        if overlap_ratio >= min_overlap_ratio:\n",
        "                            is_high = True\n",
        "\n",
        "                    elif overlap_mode is 'overlap_to_activation_ratio':\n",
        "                        overlap_ratio = num_overlap / num_high_thresh\n",
        "                        if overlap_ratio >= min_overlap_ratio:\n",
        "                            is_high = True\n",
        "\n",
        "                    elif overlap_mode is 'overlap_to_segmentation_ratio':\n",
        "                        overlap_ratio = num_overlap / num_concept_seg\n",
        "                        if overlap_ratio >= min_overlap_ratio:\n",
        "                            is_high = True\n",
        "\n",
        "                    else:\n",
        "                        if num_overlap >= min_overlap_pixels:\n",
        "                            is_high = True\n",
        "\n",
        "            else:\n",
        "                is_high = True\n",
        "\n",
        "        if is_high:\n",
        "            image_channels[ch] = high_value\n",
        "            image_concepts[channel_concept] = high_value\n",
        "            image_concepts_counts[channel_concept] += 1\n",
        "            image_channels_counts[ch] = num_high_thresh\n",
        "            continue\n",
        "\n",
        "        # if (image_concepts[channel_concept] == high_value) or (image_concepts[channel_concept] == mid_value):\n",
        "        #     continue\n",
        "        \n",
        "        if not binning_features:\n",
        "            continue\n",
        "\n",
        "        # Checking whether the channel concept can be considered as mid value for this image: \n",
        "\n",
        "        is_mid = False\n",
        "        ch_upact_mid_mask = (ch_upact > channel_low_thresh)\n",
        "        num_mid_thresh = np.sum(ch_upact_mid_mask.numpy())\n",
        "\n",
        "        if num_mid_thresh >= min_thresh_pixels:\n",
        "            if check_seg_overlap:\n",
        "                seg_concept_index = seg_concept_index_map[channel_concept] if channel_concept in seg_concept_index_map else None\n",
        "                cat_index = category_index_map[channel_category] if channel_category in category_index_map else None\n",
        "                if (seg_concept_index is None) or (cat_index is None):\n",
        "                    print('Error: Missing segmentation concept index ({}) or category index ({}) for channel {} mapped to concept {} from category {}'\n",
        "                        .format(seg_concept_index, cat_index, ch, channel_concept, channel_category))\n",
        "                else:\n",
        "                    target_seg = seg[cat_index]\n",
        "                    target_seg_concept_mask = (target_seg == seg_concept_index)\n",
        "                    num_concept_seg = np.sum(target_seg_concept_mask.numpy())\n",
        "                    overlap_mask = ch_upact_mid_mask & target_seg_concept_mask\n",
        "                    num_overlap = np.sum(overlap_mask.numpy())\n",
        "\n",
        "                    if overlap_mode is 'overlap_to_union_ratio':\n",
        "                        union_mask = ch_upact_mid_mask | target_seg_concept_mask\n",
        "                        num_union = np.sum(union_mask.numpy())\n",
        "                        overlap_ratio = num_overlap / num_union\n",
        "                        if overlap_ratio >= min_overlap_ratio:\n",
        "                            is_mid = True\n",
        "\n",
        "                    elif overlap_mode is 'overlap_to_activation_ratio':\n",
        "                        overlap_ratio = num_overlap / num_mid_thresh\n",
        "                        if overlap_ratio >= min_overlap_ratio:\n",
        "                            is_mid = True\n",
        "\n",
        "                    elif overlap_mode is 'overlap_to_segmentation_ratio':\n",
        "                        overlap_ratio = num_overlap / num_concept_seg\n",
        "                        if overlap_ratio >= min_overlap_ratio:\n",
        "                            is_mid = True\n",
        "\n",
        "                    else:\n",
        "                        if num_overlap >= min_overlap_pixels:\n",
        "                            is_mid = True\n",
        "\n",
        "            else:\n",
        "                is_mid = True\n",
        "\n",
        "        if is_mid:\n",
        "            image_channels[ch] = mid_value\n",
        "            image_channels_counts[ch] = num_mid_thresh\n",
        "            if image_concepts[channel_concept] != high_value:\n",
        "                image_concepts[channel_concept] = mid_value\n",
        "\n",
        "    return image_concepts, image_channels, image_concepts_counts, image_channels_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTvXUb-JBt5g"
      },
      "outputs": [],
      "source": [
        "def extract_concepts (model, segmodel, upfn, renorm, data_loader, channels_map, seg_concept_index_map, channels, concepts):\n",
        "\n",
        "    activations = []\n",
        "    gradients = []\n",
        "\n",
        "    def activations_hook (module, input, output):\n",
        "        #print('activations_hook called with output:', output.shape)\n",
        "        activations.append(output.detach().cpu())\n",
        "        if check_gradients:\n",
        "            output.register_hook(gradients_hook)\n",
        "\n",
        "    def gradients_hook (grad):\n",
        "        #print('gradients_hook called with grad:', grad.shape)\n",
        "        gradients.append(grad.detach().cpu())\n",
        "\n",
        "    layer = model._modules.get(target_layer)\n",
        "    if layer is None:\n",
        "        for n,l in model.named_modules():\n",
        "            if n == 'model.' + target_layer:\n",
        "                layer = l\n",
        "                print('Target layer found:', n)\n",
        "                break\n",
        "    layer.register_forward_hook(activations_hook)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    concepts_counts = {con:0 for con in concepts}\n",
        "    channels_counts = {ch:0 for ch in channels}\n",
        "    concepts_counts_by_class = {c:{con:0 for con in concepts} for c in classes}\n",
        "\n",
        "    concepts_rows_list = []\n",
        "    channels_rows_list = []\n",
        "    image_channels_counts_list = []\n",
        "    image_threshs_list = []\n",
        "    acts_list = []\n",
        "    num_images = 0\n",
        "    total_acc = 0\n",
        "    \n",
        "    #with torch.no_grad():\n",
        "    for i, (images, labels, paths) in tqdm(enumerate(data_loader)):\n",
        "        del activations[:]\n",
        "        del gradients[:]\n",
        "        images_gpu = images.cuda()\n",
        "        labels_gpu = labels.cuda()\n",
        "\n",
        "        model.zero_grad()\n",
        "    \n",
        "        output = model(images_gpu)\n",
        "        _, preds = torch.max(output, 1)\n",
        "\n",
        "        acts = activations[0]\n",
        "        raw_acts = acts\n",
        "        #acts = model.retained_layer(target_layer)\n",
        "\n",
        "        grads = None\n",
        "        if check_gradients:\n",
        "            one_hot = torch.zeros_like(output).cuda()\n",
        "            one_hot.scatter_(1, preds[:, None], 1.0)\n",
        "            output.backward(gradient=one_hot, retain_graph=True)\n",
        "            grads = gradients[0]\n",
        "\n",
        "            wgrads = grads\n",
        "            if pool_gradients:\n",
        "                wgrads = torch.mean(grads, dim=[2,3], keepdims=True)\n",
        "\n",
        "            acts = acts * wgrads\n",
        "            acts = F.relu(acts)\n",
        "            if i == 0:\n",
        "                print('output: {}, preds: {}, one_hot: {}, grads: {}, wgrads: {}'\n",
        "                    .format(output.shape, preds.shape, one_hot.shape, grads.shape, wgrads.shape))\n",
        "\n",
        "        total_acc += (preds == labels_gpu).float().sum()\n",
        "        preds = preds.cpu().numpy()\n",
        "        labels = labels.numpy()\n",
        "        #images = images.numpy()\n",
        "\n",
        "        if binning_classes:\n",
        "            preds = get_binned_predictions(output)\n",
        "\n",
        "        upacts = upfn(acts)\n",
        "        segs = None\n",
        "        if check_seg_overlap:\n",
        "            segs = segmodel.segment_batch(renorm(images_gpu), downsample=4).cpu()\n",
        "\n",
        "        if i == 0:\n",
        "            print('images: {}, labels: {}, preds: {}'.format(images.shape, labels.shape, preds.shape))\n",
        "            print('acts: {}, upacts: {}, grads: {}, segs: {}'.format(acts.shape, upacts.shape, grads.shape if grads != None else 0, segs.shape if segs != None else 0))\n",
        "\n",
        "        for j in range(images.shape[0]):\n",
        "            num_images += 1\n",
        "            pred = preds[j]\n",
        "            label = labels[j]\n",
        "            path = paths[j]\n",
        "            image = images[j]\n",
        "            act = acts[j]\n",
        "            upact = upacts[j]\n",
        "            raw_act = raw_acts[j]\n",
        "            seg = segs[j] if segs != None else None\n",
        "            grad = grads[j] if grads != None else None\n",
        "            fname = get_file_name_from_path(path)\n",
        "\n",
        "            # In case of checking gradients, we compute the specific activation/gradient threshold for an image based on the range of the values of all the channels of the image: \n",
        "            image_threshs = {'high_thresh': 0, 'low_thresh': 0}\n",
        "            if check_gradients:\n",
        "                image_threshs['high_thresh'] = torch.quantile(act, q=activation_high_thresh).item()\n",
        "                if binning_features:\n",
        "                    image_threshs['low_thresh'] = torch.quantile(act, q=activation_low_thresh).item()\n",
        "\n",
        "            image_concepts, image_channels, image_concepts_counts, image_channels_counts = extract_concepts_from_image(act, upact, seg, path, channels_map, seg_concept_index_map, \n",
        "                                                                                                                       channels, concepts, image_threshs)\n",
        "\n",
        "            if (i == 0) and (j == 0):\n",
        "                print('Image {} with label {}, pred {}, act {}, upact {}, high threshold {}, low threshold {}, and concepts {}'\n",
        "                    .format(path, label, pred, act.shape, upact.shape, image_threshs['high_thresh'], image_threshs['low_thresh'], image_concepts))\n",
        "                print('Image {} with min activation {}, max activation {}, and thresholds {}'.format(fname, act.min(), act.max(), image_threshs))\n",
        "                plot_activation_histogram(act)\n",
        "\n",
        "            # Test visualizations: \n",
        "            # if check_seg_overlap and check_gradients and (fname == '00000133.jpg'):\n",
        "            #     channel = 170\n",
        "            #     plot_sample_image_activations(data_loader.dataset, image, channel, seg, raw_act, grad, act, upact, channels_map, seg_concept_index_map, image_threshs)\n",
        "            #     return\n",
        "\n",
        "            if check_seg_overlap and check_gradients and ('sea' in image_concepts) and (image_concepts['sea'] == high_value):\n",
        "                channel = 144\n",
        "                print('Image {} with pred {} and label {}'.format(fname, pred, label))\n",
        "                plot_sample_image_activations(data_loader.dataset, image, channel, seg, raw_act, grad, act, upact, channels_map, seg_concept_index_map, image_threshs)\n",
        "                #return\n",
        "\n",
        "            acts_list.append(act)\n",
        "            image_channels_counts_list.append(image_channels_counts)\n",
        "            image_threshs_list.append(image_threshs)\n",
        "\n",
        "            image_concepts_row = image_concepts   # {k:1 if v > 0 else 0 for k,v in image_concepts_counts.items()}\n",
        "            image_concepts_row['pred'] = pred\n",
        "            image_concepts_row['label'] = label\n",
        "            image_concepts_row['id'] = num_images\n",
        "            image_concepts_row['file'] = fname\n",
        "            image_concepts_row['path'] = path\n",
        "            concepts_rows_list.append(image_concepts_row)\n",
        "\n",
        "            image_channels_row = image_channels   # {k:1 if v > 0 else 0 for k,v in image_channels_counts.items()}\n",
        "            image_channels_row['pred'] = pred\n",
        "            image_channels_row['label'] = label\n",
        "            image_channels_row['id'] = num_images\n",
        "            image_channels_row['file'] = fname\n",
        "            image_channels_row['path'] = path\n",
        "            channels_rows_list.append(image_channels_row)\n",
        "            \n",
        "            for con in concepts:\n",
        "                cnt = image_concepts_counts[con]\n",
        "                val = 1 if cnt > 0 else 0\n",
        "                concepts_counts[con] += val\n",
        "                concepts_counts_by_class[pred][con] += val\n",
        "\n",
        "            for ch in channels:\n",
        "                cnt = image_channels_counts[ch]\n",
        "                channels_counts[ch] += 1 if cnt > 0 else 0\n",
        "\n",
        "    total_acc = total_acc / num_images\n",
        "    print('\\nExtracted concepts from {} images with accuracy {:.3f}.'.format(num_images, total_acc))\n",
        "    print('\\nConcept counts:', concepts_counts)\n",
        "    for c,counts in concepts_counts_by_class.items():\n",
        "        print('\\nConcept counts of class {}: {}'.format(c, counts))\n",
        "    print('\\nChannel counts:', channels_counts)\n",
        "\n",
        "    concepts_df = pd.DataFrame(concepts_rows_list)\n",
        "    channels_df = pd.DataFrame(channels_rows_list)\n",
        "\n",
        "    return concepts_df, channels_df, acts_list, image_channels_counts_list, image_threshs_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0G6-OBwB1eS"
      },
      "outputs": [],
      "source": [
        "def filter_extracted_concepts (concepts_df, channels_df, channels_map):\n",
        "\n",
        "    preds_df = concepts_df['pred']\n",
        "    meta_cols = ['pred', 'label', 'id', 'file', 'path']\n",
        "    meta_df = concepts_df[meta_cols]\n",
        "    concept_cols = list(set(concepts_df.columns) - set(meta_cols))\n",
        "    concept_cols.sort()\n",
        "    cons_df = concepts_df[concept_cols]\n",
        "\n",
        "    initial_concepts = list(cons_df.columns)\n",
        "    print('Initial concepts ({}): {}'.format(len(initial_concepts), initial_concepts))\n",
        "    \n",
        "    var_selector = VarianceThreshold(threshold=(low_variance_thresh * (1 - low_variance_thresh)))\n",
        "    var_selector.fit(cons_df)\n",
        "    var_col_indices = var_selector.get_support(indices=True)\n",
        "    cons_df = cons_df.iloc[:,var_col_indices]\n",
        "    var_filtered_concepts = list(cons_df.columns)\n",
        "    var_removed_concepts = set(initial_concepts) - set(var_filtered_concepts)\n",
        "    print('Concepts removed by variance filtering ({}): {}'.format(len(var_removed_concepts), var_removed_concepts))\n",
        "\n",
        "    k = max_concepts if len(var_filtered_concepts) > max_concepts else 'all'\n",
        "    mut_selector = SelectKBest(mutual_info_classif, k=k)\n",
        "    mut_selector.fit(cons_df, preds_df)\n",
        "    mut_col_indices = mut_selector.get_support(indices=True)\n",
        "    cons_df = cons_df.iloc[:,mut_col_indices]\n",
        "    filtered_concepts = list(cons_df.columns)\n",
        "    mut_removed_concepts = set(var_filtered_concepts) - set(filtered_concepts)\n",
        "    print('Concepts removed by mutual info filtering ({}): {}'.format(len(mut_removed_concepts), mut_removed_concepts))\n",
        "    print('Concepts reduced from {} to {} by concept filtering.'.format(len(initial_concepts), len(filtered_concepts)))\n",
        "    print('Final concepts after filtering ({}): {}'.format(len(filtered_concepts), filtered_concepts))\n",
        "\n",
        "    filtered_concepts_df = pd.concat([cons_df, meta_df], axis=1)\n",
        "    display(filtered_concepts_df.head())\n",
        "\n",
        "    channel_cols = list(set(channels_df.columns) - set(meta_cols))\n",
        "    filtered_channels = [ch for ch in channel_cols if (channels_map[ch]['concept'] in filtered_concepts)]\n",
        "    print('Channels reduced from {} to {} by concept filtering.'.format(len(channel_cols), len(filtered_channels)))\n",
        "\n",
        "    cols_to_keep = filtered_channels + meta_cols\n",
        "    filtered_channels_df = channels_df[cols_to_keep]\n",
        "    display(filtered_channels_df.head())\n",
        "\n",
        "    return filtered_concepts_df, filtered_channels_df, filtered_concepts, filtered_channels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9eR94p6bJjMD"
      },
      "outputs": [],
      "source": [
        "def save_activation_images_of_image (iv, image_index, image_path, image_fname, acts, img_concepts_row, img_channels_row, \n",
        "                                     image_channels_counts, channels_map, concepts, output_dir, image_threshs=None):\n",
        "\n",
        "    acts = acts[None, :, :, :]   # as required by iv.masked_image\n",
        "    if image_index == 1:\n",
        "        print('acts.shape in save_activation_images:', acts.shape)\n",
        "\n",
        "    image_activated_channels = [k for k,v in image_channels_counts.items() if v > 0]   # Only keep those channels which have been either mid or high for the image\n",
        "    if len(image_activated_channels) == 0:\n",
        "        print('Image {} with path {} has no activated channels!'.format(image_index, image_path))\n",
        "        return 0\n",
        "\n",
        "    image_concept_channels = {con:[] for con in concepts}\n",
        "    for ch in image_activated_channels:\n",
        "        ch_info = channels_map[ch]\n",
        "        is_valid = ch_info['is_valid']\n",
        "        channel_concept = ch_info['concept']\n",
        "        con_value = img_concepts_row[channel_concept]\n",
        "        ch_value = img_channels_row[ch]\n",
        "        num_high_thresh = image_channels_counts[ch]   # In case of binning features, it can be the count of either mid or high pixels, depending on whether the channel has been mid or high for the image\n",
        "\n",
        "        if not is_valid:   # In case the channel IoU with the concept is lower than the min threshold, we don't need an image saved for the channel\n",
        "            continue\n",
        "\n",
        "        if con_value != ch_value:   # In case the concept is high for the image and the channel is mid, we don't need an image saved for the channel\n",
        "            continue\n",
        "            \n",
        "        image_concept_channels[channel_concept].append((ch, num_high_thresh))\n",
        "\n",
        "    image = pil_loader(image_path)   # Image.open(image_path)\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(image_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=norm_mean, std=norm_std)\n",
        "    ])\n",
        "    image = transform(image)\n",
        "\n",
        "    images = []\n",
        "    filenames = []\n",
        "    for con,lst in image_concept_channels.items():\n",
        "        if len(lst) == 0:\n",
        "            continue\n",
        "\n",
        "        top_channel_nums = sorted(lst, key=lambda x: x[1], reverse=True)[:n_top_channels_per_concept]\n",
        "        top_channels = [k for k,v in top_channel_nums]\n",
        "        if image_index == 1:\n",
        "            print('Top channels for concept {}: {}'.format(con, top_channel_nums))\n",
        "\n",
        "        for i,ch in enumerate(top_channels):\n",
        "            ch_index = ch - 1\n",
        "            ch_info = channels_map[ch]\n",
        "            ch_value = img_channels_row[ch]\n",
        "\n",
        "            channel_thresh = image_threshs['high_thresh'] if check_gradients else ch_info['high_thresh']\n",
        "            feature_value_title = ''\n",
        "\n",
        "            if binning_features:\n",
        "                if ch_value == 2:\n",
        "                    feature_value_title = 'high_'\n",
        "                else:\n",
        "                    feature_value_title = 'mid_'\n",
        "                    channel_thresh = image_threshs['low_thresh'] if check_gradients else ch_info['low_thresh']\n",
        "\n",
        "            if (channel_thresh is None):\n",
        "                print('Error: Missing activation or threshold ({}) for channel {}!'.format(channel_thresh, ch))\n",
        "                continue\n",
        "\n",
        "            new_image = iv.masked_image(image, acts, (0, ch_index), level=channel_thresh)\n",
        "\n",
        "            # mask = np.array(Image.fromarray(channel_activation).resize(size=(image.shape[1], image.shape[0]), resample=Image.BILINEAR))   # size=image.shape[:2]\n",
        "            # mask = mask > channel_thresh\n",
        "            # new_image = (mask[:, :, np.newaxis] * overlay_opacity + (1 - overlay_opacity)) * image\n",
        "\n",
        "            ind = image_fname.rfind('.')\n",
        "            image_fname_raw = image_fname[:ind]\n",
        "            new_fname = image_fname_raw + '_' + feature_value_title + str(ch) + '_' + con + '.jpg'\n",
        "            new_path = output_dir + '/' + new_fname\n",
        "\n",
        "            # final_image = new_image.astype(np.uint8)\n",
        "            # imwrite(new_path, final_image)\n",
        "\n",
        "            images.append(new_image)\n",
        "            filenames.append(new_path)\n",
        "\n",
        "            new_image.save(new_path, optimize=True, quality=99)\n",
        "\n",
        "    #save_image_set(images, filenames)\n",
        "    print('Saved {} activation images for image {} with path {}'.format(len(images), image_index, image_path))\n",
        "    return len(images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQnR80fJCC2M"
      },
      "outputs": [],
      "source": [
        "def save_image_concepts_dataset (concepts_df, channels_df, image_channels_counts_list, image_threshs_list, acts_list, upfn, dataset, channels_map, \n",
        "                                 filtered_concepts, filtered_channels, concepts_output_path, channels_output_path, activation_images_path):\n",
        "\n",
        "    iv = imgviz.ImageVisualizer(size=(image_size, image_size), image_size=(image_size, image_size), source=dataset)   # renormalizer=renorm\n",
        "\n",
        "    num_images = len(concepts_df.index)\n",
        "\n",
        "    filtered_concepts_counts = {con:0 for con in filtered_concepts}\n",
        "    filtered_channels_counts = {ch:0 for ch in filtered_channels}\n",
        "    filtered_concepts_counts_by_class = {c:{con:0 for con in filtered_concepts} for c in classes}\n",
        "    \n",
        "    num_act_images_saved = 0\n",
        "\n",
        "    for i,con_row in concepts_df.iterrows():\n",
        "        ch_row = channels_df.iloc[i]\n",
        "        pred = con_row['pred']\n",
        "        id = con_row['id']\n",
        "        path = con_row['path']\n",
        "        fname = con_row['file']\n",
        "        act = acts_list[i]\n",
        "        upact = upfn(torch.unsqueeze(act, dim=0))[0]\n",
        "        image_channels_counts = image_channels_counts_list[i]\n",
        "        filtered_image_channels_counts = {ch:image_channels_counts[ch] for ch in image_channels_counts if ch in filtered_channels}\n",
        "        image_threshs = image_threshs_list[i]\n",
        "\n",
        "        num_act_images = save_activation_images_of_image(iv, id, path, fname, upact, con_row, ch_row, filtered_image_channels_counts, channels_map, \n",
        "                                                         filtered_concepts, activation_images_path, image_threshs)\n",
        "        num_act_images_saved += num_act_images\n",
        "\n",
        "        for con in filtered_concepts:\n",
        "            con_val = con_row[con]\n",
        "            val = 1 if con_val == high_value else 0\n",
        "            filtered_concepts_counts[con] += val\n",
        "            filtered_concepts_counts_by_class[pred][con] += val\n",
        "\n",
        "        for ch in filtered_channels:\n",
        "            val = ch_row[ch]\n",
        "            filtered_channels_counts[ch] += 1 if val == high_value else 0\n",
        "\n",
        "    print('Saved {} activation images for {} images.'.format(num_act_images_saved, num_images))\n",
        "    print('\\nFiltered concept counts:', filtered_concepts_counts)\n",
        "    for c,counts in filtered_concepts_counts_by_class.items():\n",
        "        print('\\nFiltered concept counts of class {}: {}'.format(c, counts))\n",
        "    print('\\nFiltered channel counts:', filtered_channels_counts)\n",
        "\n",
        "    concepts_df.to_csv(concepts_output_path, index=False)\n",
        "    channels_df.to_csv(channels_output_path, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RIegVXcdwLqK"
      },
      "outputs": [],
      "source": [
        "model_file = 'model.pth'\n",
        "dataset_dir = 'dataset'\n",
        "drive_result_path = '/content/drive/My Drive/Python Projects/POEM Pipeline Results/' + model_name + '_' + dataset_name\n",
        "\n",
        "if not use_dissection_models:\n",
        "    dataset_file = dataset_dir + '.zip'\n",
        "    drive_dataset_dir = drive_result_path + '/' + dataset_file\n",
        "    !cp \"$drive_dataset_dir\" '.'\n",
        "    !unzip -qq -n $dataset_file -d '.'\n",
        "\n",
        "    drive_model_path = drive_result_path + '/' + model_file\n",
        "    !cp \"$drive_model_path\" '.'\n",
        "\n",
        "result_dir = 'identification_results'\n",
        "result_file = result_dir + '.zip'\n",
        "result_path = drive_result_path + \"/\" + result_file\n",
        "!cp \"$result_path\" '.'\n",
        "!unzip -qq -n $result_file -d '.'\n",
        "\n",
        "activation_images_path = 'activation_images'\n",
        "if os.path.exists(activation_images_path):\n",
        "    shutil.rmtree(activation_images_path)\n",
        "os.makedirs(activation_images_path)\n",
        "\n",
        "# Optional: loading the segmenter models to avoid downloading them from netdissect server: \n",
        "segmodel_dir = 'segmodel'\n",
        "segmodel_file = segmodel_dir + '.zip'\n",
        "drive_segmodel_path = '/content/drive/My Drive/Python Projects/Pretrained Models/' + segmodel_file\n",
        "target_segmodel_dir = 'datasets'\n",
        "if not os.path.exists(target_segmodel_dir):\n",
        "    os.makedirs(target_segmodel_dir)\n",
        "\n",
        "target_segmodel_file = target_segmodel_dir + '/' + segmodel_file\n",
        "!cp \"$drive_segmodel_path\" $target_segmodel_dir\n",
        "!unzip -qq -n $target_segmodel_file -d $target_segmodel_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7XlFj_w8_gF"
      },
      "outputs": [],
      "source": [
        "model = load_model(model_file)\n",
        "model.retain_layer(target_layer)\n",
        "\n",
        "dataset, data_loader = load_data(dataset_dir)\n",
        "class_titles = extract_class_titles(dataset_name)\n",
        "classes = list(class_titles.keys())\n",
        "\n",
        "tally_path = './' + result_dir + '/report.json'\n",
        "thresholds_path = './' + result_dir + '/channel_quantiles.npy'\n",
        "channels_map, channels, concepts = load_channels_data(tally_path)\n",
        "\n",
        "upfn = experiment.make_upfn(args, dataset, model, target_layer)\n",
        "renorm = renormalize.renormalizer(dataset, target='zc')\n",
        "\n",
        "segmodel = None\n",
        "seg_concept_index_map = {}\n",
        "if check_seg_overlap:\n",
        "    segmodel, seglabels, segcatlabels = experiment.setting.load_segmenter(seg_model_name)\n",
        "    for i,lbl in enumerate(seglabels):\n",
        "        seg_concept_index_map[lbl] = i\n",
        "\n",
        "model.stop_retaining_layers([target_layer])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8yKQO3WnJZKb"
      },
      "outputs": [],
      "source": [
        "concepts_df, channels_df, acts_list, image_channels_counts_list, image_threshs_list = extract_concepts(model, segmodel, upfn, renorm, data_loader, channels_map, \n",
        "                                                                                                       seg_concept_index_map, channels, concepts)\n",
        "\n",
        "if filter_concepts:\n",
        "    concepts_df, channels_df, concepts, channels = filter_extracted_concepts(concepts_df, channels_df, channels_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6tBfQ47rA5v"
      },
      "outputs": [],
      "source": [
        "concepts_output_path = 'image_concepts.csv'\n",
        "channels_output_path = 'image_channels.csv'\n",
        "\n",
        "save_image_concepts_dataset(concepts_df, channels_df, image_channels_counts_list, image_threshs_list, acts_list, upfn, dataset, channels_map, \n",
        "                            concepts, channels, concepts_output_path, channels_output_path, activation_images_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TCsTnd-YcPDt"
      },
      "outputs": [],
      "source": [
        "!cp $concepts_output_path \"$drive_result_path\"\n",
        "!cp $channels_output_path \"$drive_result_path\"\n",
        "\n",
        "activation_images_file = activation_images_path + '.zip'\n",
        "!zip -qq -r $activation_images_file $activation_images_path\n",
        "!cp $activation_images_file '$drive_result_path'\n",
        "\n",
        "packages_path = drive_result_path + '/attribution_packages.log'\n",
        "save_imported_packages(packages_path)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}