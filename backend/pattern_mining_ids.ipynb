{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_WsQteQ47bk"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# code for IDS with deterministic local search\n",
        "# requires installation of python package apyori: https://pypi.org/project/apyori/\n",
        "!pip install apyori\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import datetime\n",
        "import random \n",
        "import itertools\n",
        "from apyori import apriori"
      ],
      "metadata": {
        "id": "lDD9-wa8-uXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# rule is of the form if A == a and B == b, then class_1\n",
        "# one of the member variables is itemset - a set of patterns {(A,a), (B,b)}\n",
        "# the other member variable is class_label (e.g., class_1)\n",
        "class rule:\n",
        "    \n",
        "    def __init__(self,feature_list,value_list,class_label):\n",
        "        self.itemset = set()\n",
        "        self.class_label = None\n",
        "        self.cover = 0   # refers to support\n",
        "        self.correct_cover = 0   # refers to confidence\n",
        "        self.accurate_cover = 0   # refers to accuracy\n",
        "        self.class_covers = None\n",
        "        self.add_item(feature_list,value_list)\n",
        "        self.set_class_label(class_label)\n",
        "    \n",
        "    def add_item(self,feature_list,value_list):\n",
        "        \n",
        "        if len(feature_list) != len(value_list):\n",
        "            print(\"Some error in inputting feature value pairs\")\n",
        "            return\n",
        "        for i in range(0,len(feature_list)):\n",
        "            self.itemset.add((feature_list[i],value_list[i]))\n",
        "    \n",
        "    # def print_rule(self, n_rows):\n",
        "    #     s = \"If \"\n",
        "    #     for item in self.itemset:\n",
        "    #         s += str(item[0]) + \"=\" +str(item[1]) + \" & \"\n",
        "    #     s = s[:-3]\n",
        "    #     s += \", then \"\n",
        "    #     s += str(self.class_label)\n",
        "    #     if self.cover and self.correct_cover and self.accurate_cover: \n",
        "    #         sup = self.cover / n_rows\n",
        "    #         conf = self.correct_cover\n",
        "    #         acc = self.accurate_cover\n",
        "    #         s += \" (sup: {}, conf: {}, acc: {}, class covers: {})\".format(sup, conf, acc, self.class_covers)\n",
        "    #     print(s)\n",
        "        \n",
        "    def all_predicates_same(self, r):\n",
        "        return self.itemset == r.itemset\n",
        "    \n",
        "    def class_label_same(self,r):\n",
        "        return self.class_label == r.class_label\n",
        "            \n",
        "    def set_class_label(self,label):\n",
        "        self.class_label = label\n",
        "        \n",
        "    def get_length(self):\n",
        "        return len(self.itemset)\n",
        "    \n",
        "    def get_cover(self, df):\n",
        "        dfnew = df.copy()\n",
        "        for pattern in self.itemset: \n",
        "            dfnew = dfnew[dfnew[pattern[0]] == pattern[1]]\n",
        "        return list(dfnew.index.values)\n",
        "\n",
        "    def get_correct_cover(self, df, Y):\n",
        "        indexes_points_covered = self.get_cover(df) # indices of all points satisfying the rule\n",
        "        Y_arr = pd.Series(Y)                    # make a series of all Y labels\n",
        "        labels_covered_points = list(Y_arr[indexes_points_covered])   # get a list only of Y labels of the points covered\n",
        "        correct_cover = []\n",
        "        for ind in range(0,len(labels_covered_points)):\n",
        "            if labels_covered_points[ind] == self.class_label:\n",
        "                correct_cover.append(indexes_points_covered[ind])\n",
        "        return correct_cover, indexes_points_covered\n",
        "    \n",
        "    def get_incorrect_cover(self, df, Y):\n",
        "        correct_cover, full_cover = self.get_correct_cover(df, Y)\n",
        "        return (sorted(list(set(full_cover) - set(correct_cover))))\n",
        "        \n",
        "    # new methods: \n",
        "    def compute_cover_counts(self, X, Y, Y_true): \n",
        "        indexes = self.get_cover(X)\n",
        "        Y_list = Y.iloc[indexes].tolist()\n",
        "        Y_true_list = Y_true.iloc[indexes].tolist()\n",
        "\n",
        "        sup = len(indexes)\n",
        "        conf = 0\n",
        "        acc = 0\n",
        "        class_covers = {}\n",
        "        for i,y in enumerate(Y_list):\n",
        "            if y in class_covers: \n",
        "                class_covers[y] = class_covers[y] + 1\n",
        "            else:\n",
        "                class_covers[y] = 1\n",
        "\n",
        "            if y == self.class_label:\n",
        "                conf += 1\n",
        "                y_true = Y_true_list[i]\n",
        "                if y == y_true:\n",
        "                    acc += 1\n",
        "        \n",
        "        self.cover = sup\n",
        "        self.correct_cover = conf / sup   # (class_covers[self.class_label] / self.cover) if self.class_label in class_covers else 0\n",
        "        self.accurate_cover = acc / conf\n",
        "        self.class_covers = {k:(v/sup) for k,v in class_covers.items()}\n",
        "    \n",
        "    # def set_cover_counts(self, cover, correct_cover):\n",
        "        # self.cover = cover\n",
        "        # self.correct_cover = correct_cover\n",
        "        \n",
        "    def matches_data(self, data):\n",
        "        for attr in self.itemset:\n",
        "            if data[attr[0]] != attr[1]:\n",
        "                return False\n",
        "        return True"
      ],
      "metadata": {
        "id": "7d4u-8Nm--EI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# below function basically takes a data frame and a support threshold and returns itemsets which satisfy the threshold\n",
        "def run_apriori(df, support_thres):\n",
        "    # the idea is to basically make a list of strings out of df and run apriori api on it \n",
        "    # return the frequent itemsets\n",
        "    dataset = []\n",
        "    for i in range(0,df.shape[0]):\n",
        "        temp = []\n",
        "        for col_name in df.columns:\n",
        "            temp.append(col_name+\"=\"+str(df[col_name][i]))\n",
        "        dataset.append(temp)\n",
        "\n",
        "    results = list(apriori(dataset, min_support=support_thres))\n",
        "    \n",
        "    list_itemsets = []\n",
        "    for ele in results:\n",
        "        temp = []\n",
        "        for pred in ele.items:\n",
        "            temp.append(pred)\n",
        "        list_itemsets.append(temp)\n",
        "\n",
        "    return list_itemsets"
      ],
      "metadata": {
        "id": "5SPeud0P_NYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function converts a list of itemsets (stored as list of lists of strings) into rule objects\n",
        "def createrules(freq_itemsets, labels_set):\n",
        "    # create a list of rule objects from frequent itemsets \n",
        "    list_of_rules = []\n",
        "    for one_itemset in freq_itemsets:\n",
        "        feature_list = []\n",
        "        value_list = []\n",
        "        for pattern in one_itemset:\n",
        "            fea_val = pattern.split(\"=\")\n",
        "            feature_list.append(fea_val[0])\n",
        "            value_list.append(fea_val[1])\n",
        "        for each_label in labels_set:\n",
        "            temp_rule = rule(feature_list,value_list,each_label)\n",
        "            list_of_rules.append(temp_rule)\n",
        "\n",
        "    return list_of_rules"
      ],
      "metadata": {
        "id": "smyKqxJ7_UXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute the maximum length of any rule in the candidate rule set\n",
        "def max_rule_length(list_rules):\n",
        "    len_arr = []\n",
        "    for r in list_rules:\n",
        "        len_arr.append(r.get_length())\n",
        "    return max(len_arr)"
      ],
      "metadata": {
        "id": "g2q1oZ8X_zKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute the number of points which are covered both by r1 and r2 w.r.t. data frame df\n",
        "def overlap(r1, r2, df):\n",
        "    return sorted(list(set(r1.get_cover(df)).intersection(set(r2.get_cover(df)))))"
      ],
      "metadata": {
        "id": "-uMsoSboAAq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# computes the objective value of a given solution set\n",
        "def func_evaluation(soln_set, list_rules, df, Y, lambda_array):\n",
        "    # evaluate the objective function based on rules in solution set \n",
        "    # soln set is a set of indexes which when used to index elements in list_rules point to the exact rules in the solution set\n",
        "    # compute f1 through f7 and we assume there are 7 lambdas in lambda_array\n",
        "    f = [] #stores values of f1 through f7; \n",
        "    \n",
        "    # f0 term\n",
        "    f0 = len(list_rules) - len(soln_set) # |S| - size(R)\n",
        "    f.append(f0)\n",
        "    \n",
        "    # f1 term\n",
        "    Lmax = max_rule_length(list_rules)\n",
        "    sum_rule_length = 0.0\n",
        "    for rule_index in soln_set:\n",
        "        sum_rule_length += list_rules[rule_index].get_length()\n",
        "    \n",
        "    f1 = Lmax * len(list_rules) - sum_rule_length\n",
        "    f.append(f1)\n",
        "    \n",
        "    # f2 term - intraclass overlap\n",
        "    sum_overlap_intraclass = 0.0\n",
        "    for r1_index in soln_set:\n",
        "        for r2_index in soln_set:\n",
        "            if r1_index >= r2_index:\n",
        "                continue\n",
        "            if list_rules[r1_index].class_label == list_rules[r2_index].class_label:\n",
        "                sum_overlap_intraclass += len(overlap(list_rules[r1_index], list_rules[r2_index],df))\n",
        "    f2 = df.shape[0] * len(list_rules) * len(list_rules) - sum_overlap_intraclass\n",
        "    f.append(f2)\n",
        "    \n",
        "    # f3 term - interclass overlap\n",
        "    sum_overlap_interclass = 0.0\n",
        "    for r1_index in soln_set:\n",
        "        for r2_index in soln_set:\n",
        "            if r1_index >= r2_index:\n",
        "                continue\n",
        "            if list_rules[r1_index].class_label != list_rules[r2_index].class_label:\n",
        "                sum_overlap_interclass += len(overlap(list_rules[r1_index], list_rules[r2_index],df))\n",
        "    f3 = df.shape[0] * len(list_rules) * len(list_rules) - sum_overlap_interclass\n",
        "    f.append(f3)\n",
        "    \n",
        "    # f4 term - coverage of all classes\n",
        "    classes_covered = set() # set\n",
        "    for index in soln_set:\n",
        "        classes_covered.add(list_rules[index].class_label)\n",
        "    f4 = len(classes_covered)\n",
        "    f.append(f4)\n",
        "    \n",
        "    # f5 term - accuracy\n",
        "    sum_incorrect_cover = 0.0\n",
        "    for index in soln_set:\n",
        "        sum_incorrect_cover += len(list_rules[index].get_incorrect_cover(df,Y))\n",
        "    f5 = df.shape[0] * len(list_rules) - sum_incorrect_cover\n",
        "    f.append(f5)\n",
        "    \n",
        "    #f6 term - cover correctly with at least one rule\n",
        "    atleast_once_correctly_covered = set()\n",
        "    for index in soln_set:\n",
        "        correct_cover, full_cover = list_rules[index].get_correct_cover(df,Y)\n",
        "        atleast_once_correctly_covered = atleast_once_correctly_covered.union(set(correct_cover))\n",
        "    f6 = len(atleast_once_correctly_covered)\n",
        "    f.append(f6)\n",
        "    \n",
        "    obj_val = 0.0\n",
        "    for i in range(7):\n",
        "        obj_val += f[i] * lambda_array[i]\n",
        "    \n",
        "    #print(f)\n",
        "    return obj_val"
      ],
      "metadata": {
        "id": "C0ttxIRwAAoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Deterministic search specific methods: \n",
        "\n",
        "# deterministic local search algorithm which returns a solution set as well as the corresponding objective value\n",
        "def deterministic_local_search(list_rules, df, Y, lambda_array, epsilon, timeout):\n",
        "    # step by step implementation of deterministic local search algorithm in the \n",
        "    # FOCS paper: https://people.csail.mit.edu/mirrokni/focs07.pdf (page 4-5)\n",
        "    \n",
        "    t_start = datetime.datetime.now()\n",
        "\n",
        "    #initialize soln_set\n",
        "    soln_set = set()\n",
        "    n = len(list_rules)\n",
        "    \n",
        "    # step 1: find out the element with maximum objective function value and initialize soln set with it\n",
        "    each_obj_val = []\n",
        "    for ind in range(len(list_rules)):\n",
        "        each_obj_val.append(func_evaluation(set([ind]), list_rules, df, Y, lambda_array))\n",
        "        \n",
        "    best_element = np.argmax(each_obj_val)\n",
        "    soln_set.add(best_element)\n",
        "    print(\"Initial rule: \" + str(best_element))\n",
        "    S_func_val = each_obj_val[best_element]\n",
        "    \n",
        "    restart_step2 = False\n",
        "    \n",
        "    # step 2: if there exists an element which is good, add it to soln set and repeat\n",
        "    while True:\n",
        "        each_obj_val = []\n",
        "        \n",
        "        for ind in set(range(len(list_rules))) - soln_set:\n",
        "            func_val = func_evaluation(soln_set.union(set([ind])), list_rules, df, Y, lambda_array)\n",
        "            \n",
        "            if func_val > (1.0 + epsilon/(n*n)) * S_func_val:\n",
        "                soln_set.add(ind)\n",
        "                print(\"Adding rule \"+str(ind))\n",
        "                S_func_val = func_val\n",
        "                restart_step2 = True\n",
        "                break\n",
        "\n",
        "            t_end = datetime.datetime.now()\n",
        "            if ((t_end - t_start).total_seconds() * 1000) > timeout:\n",
        "                print('Deterministic search timed out in add loop, returning the current results ...')\n",
        "                return soln_set, S_func_val\n",
        "                \n",
        "        print('Add loop finished!')\n",
        "        if restart_step2:\n",
        "            print('Restarting step 2 ...')\n",
        "            restart_step2 = False\n",
        "            continue\n",
        "            \n",
        "        for ind in soln_set:\n",
        "            func_val = func_evaluation(soln_set - set([ind]), list_rules, df, Y, lambda_array)\n",
        "            \n",
        "            if func_val > (1.0 + epsilon/(n*n)) * S_func_val:\n",
        "                soln_set.remove(ind)\n",
        "                print(\"Removing rule \"+str(ind))\n",
        "                S_func_val = func_val\n",
        "                restart_step2 = True\n",
        "                break\n",
        "\n",
        "            t_end = datetime.datetime.now()\n",
        "            if ((t_end - t_start).total_seconds() * 1000) > timeout:\n",
        "                print('Deterministic search timed out in remove loop, returning the current results ...')\n",
        "                return soln_set, S_func_val\n",
        "        \n",
        "        print('Remove loop finished!')\n",
        "        if restart_step2:\n",
        "            print('Restarting step 2 ...')\n",
        "            restart_step2 = False\n",
        "            continue\n",
        "        \n",
        "        print('Evaluating s1 and s2 ...')\n",
        "        # Evaluation of s2 which is a very large set can take a very long time, and is the main performance bottleneck: \n",
        "        s1 = func_evaluation(soln_set, list_rules, df, Y, lambda_array)\n",
        "        s2 = 0   # func_evaluation(set(range(len(list_rules))) - soln_set, list_rules, df, Y, lambda_array)\n",
        "        \n",
        "        print(s1)\n",
        "        print(s2)\n",
        "        \n",
        "        if s1 >= s2:\n",
        "            return soln_set, s1\n",
        "        else: \n",
        "            return set(range(len(list_rules))) - soln_set, s2"
      ],
      "metadata": {
        "id": "GTVqHeAJAAlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Smooth search specific methods: \n",
        "\n",
        "# Helper function for smooth_local_search routine: Samples a set of elements based on delta \n",
        "def sample_random_set(soln_set, delta, len_list_rules):\n",
        "    all_rule_indexes = set(range(len_list_rules))\n",
        "    return_set = set()\n",
        "    \n",
        "    # sample in-set elements with prob. (delta + 1)/2\n",
        "    p = (delta + 1.0)/2\n",
        "    for item in soln_set:\n",
        "        random_val = np.random.uniform()\n",
        "        if random_val <= p:\n",
        "            return_set.add(item)\n",
        "    \n",
        "    # sample out-set elements with prob. (1 - delta)/2\n",
        "    p_prime = (1.0 - delta)/2\n",
        "    for item in (all_rule_indexes - soln_set):\n",
        "        random_val = np.random.uniform()\n",
        "        if random_val <= p_prime:\n",
        "            return_set.add(item)\n",
        "    \n",
        "    #print(soln_set)\n",
        "    #print(all_rule_indexes - soln_set)\n",
        "    return return_set"
      ],
      "metadata": {
        "id": "xA1FRQAJAAi1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function for smooth_local_search routine: Computes estimated gain of adding an element to the solution set\n",
        "def estimate_omega_for_element(soln_set, delta, rule_x_index, list_rules, df, Y, lambda_array, error_threshold):\n",
        "    #assumes rule_x_index is not in soln_set \n",
        "    \n",
        "    Exp1_func_vals = []\n",
        "    \n",
        "    Exp2_func_vals = []\n",
        "    \n",
        "    while(True):\n",
        "        \n",
        "        # first expectation term (include x)\n",
        "        for i in range(10):\n",
        "            temp_soln_set = sample_random_set(soln_set, delta, len(list_rules))\n",
        "            temp_soln_set.add(rule_x_index)\n",
        "            Exp1_func_vals.append(func_evaluation(temp_soln_set, list_rules, df, Y, lambda_array))\n",
        "        \n",
        "        # second expectation term (exclude x)\n",
        "        for j in range(10):\n",
        "            temp_soln_set = sample_random_set(soln_set, delta, len(list_rules))\n",
        "            if rule_x_index in temp_soln_set:\n",
        "                temp_soln_set.remove(rule_x_index)\n",
        "            Exp2_func_vals.append(func_evaluation(temp_soln_set, list_rules, df, Y, lambda_array))\n",
        "    \n",
        "        # compute standard error of mean difference\n",
        "        variance_Exp1 = np.var(Exp1_func_vals, dtype=np.float64)\n",
        "        variance_Exp2 = np.var(Exp2_func_vals, dtype=np.float64)\n",
        "        std_err = math.sqrt(variance_Exp1/len(Exp1_func_vals) + variance_Exp2/len(Exp2_func_vals))\n",
        "        print(\"Standard Error \"+str(std_err))\n",
        "        \n",
        "        if std_err <= error_threshold:\n",
        "            break\n",
        "            \n",
        "    return np.mean(Exp1_func_vals) - np.mean(Exp2_func_vals)"
      ],
      "metadata": {
        "id": "LR3nl4h_AAf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function for smooth_local_search routine: Computes the 'estimate' of optimal value using random search \n",
        "def compute_OPT(list_rules, df, Y, lambda_array):\n",
        "    opt_set = set()\n",
        "    for i in range(len(list_rules)):\n",
        "        r_val = np.random.uniform()\n",
        "        if r_val <= 0.5:\n",
        "            opt_set.add(i)\n",
        "    return func_evaluation(opt_set, list_rules, df, Y, lambda_array)"
      ],
      "metadata": {
        "id": "ysAvMRQjAAc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# smooth local search algorithm which returns a solution set\n",
        "def smooth_local_search(list_rules, df, Y, lambda_array, delta, delta_prime, timeout):\n",
        "    # step by step implementation of smooth local search algorithm in the \n",
        "    # FOCS paper: https://people.csail.mit.edu/mirrokni/focs07.pdf (page 6)\n",
        "    \n",
        "    t_start = datetime.datetime.now()\n",
        "\n",
        "    # step 1: set the value n and OPT; initialize soln_set to empty\n",
        "    n = len(list_rules)\n",
        "    OPT = compute_OPT(list_rules, df, Y, lambda_array)\n",
        "    print(\"2/n*n OPT value is \"+str(2.0/(n*n)*OPT))\n",
        "    \n",
        "    soln_set = set()\n",
        "    \n",
        "    restart_omega_computations = False\n",
        "    \n",
        "    while(True):\n",
        "        # step 2 & 3: for each element estimate omega within certain error_threshold; if estimated omega > 2/n^2 * OPT, then add \n",
        "        # the corresponding rule to soln set and recompute omega estimates again\n",
        "        omega_estimates = []\n",
        "        for rule_x_index in range(n):\n",
        "                \n",
        "            print(\"Estimating omega for rule \"+str(rule_x_index))\n",
        "            omega_est = estimate_omega_for_element(soln_set, delta, rule_x_index, list_rules, df, Y, lambda_array, 1.0/(n*n) * OPT)\n",
        "            omega_estimates.append(omega_est)\n",
        "            print(\"Omega estimate is \"+str(omega_est))\n",
        "            \n",
        "            if rule_x_index in soln_set:\n",
        "                continue\n",
        "            \n",
        "            if omega_est > 2.0/(n*n) * OPT:\n",
        "                # add this element to solution set and recompute omegas\n",
        "                soln_set.add(rule_x_index)\n",
        "                restart_omega_computations = True\n",
        "                print(\"-----------------------\")\n",
        "                print(\"Adding to the solution set rule \"+str(rule_x_index))\n",
        "                print(\"-----------------------\")\n",
        "                break    \n",
        "\n",
        "            t_end = datetime.datetime.now()\n",
        "            if ((t_end - t_start).total_seconds() * 1000) > timeout:\n",
        "                print('Smooth search timed out, returning the current results ...')\n",
        "                return sample_random_set(soln_set, delta_prime, n)\n",
        "        \n",
        "        if restart_omega_computations: \n",
        "            restart_omega_computations = False\n",
        "            continue\n",
        "            \n",
        "        # reaching this point of code means there is nothing more to add to the solution set, but we can remove elements\n",
        "        for rule_ind in soln_set:\n",
        "            if omega_estimates[rule_ind] < -2.0/(n*n) * OPT:\n",
        "                soln_set.remove(rule_ind)\n",
        "                restart_omega_computations = True\n",
        "                \n",
        "                print(\"Removing from the solution set rule \"+str(rule_ind))\n",
        "                break\n",
        "\n",
        "            t_end = datetime.datetime.now()\n",
        "            if ((t_end - t_start).total_seconds() * 1000) > timeout:\n",
        "                print('Smooth search timed out, returning the current results ...')\n",
        "                return sample_random_set(soln_set, delta_prime, n)\n",
        "                \n",
        "        if restart_omega_computations: \n",
        "            restart_omega_computations = False\n",
        "            continue\n",
        "            \n",
        "        # reaching here means there is no element to add or remove from the solution set\n",
        "        return sample_random_set(soln_set, delta_prime, n)"
      ],
      "metadata": {
        "id": "DxhCvoPpAAZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# New prediction and evaluation methods: \n",
        "\n",
        "def load_data(file_path, class_column, label_column, extra_columns=[]):\n",
        "    df = pd.read_csv(file_path)\n",
        "    Y = df[class_column]\n",
        "    Y_true = df[label_column]\n",
        "    X = df.drop([class_column, label_column], axis=1)\n",
        "    if len(extra_columns) > 0:\n",
        "        X.drop(extra_columns, axis=1, inplace=True)\n",
        "    cols = X.columns\n",
        "    X[cols] = X[cols].astype(str)\n",
        "    return X, Y, Y_true"
      ],
      "metadata": {
        "id": "WF4EV8UUAATb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_smooth_search(list_of_rules, df, Y, lambda_array, delta1, delta_prime1, delta2, delta_prime2, timeout):\n",
        "    s1 = smooth_local_search(list_of_rules, df, Y, lambda_array, delta1, delta_prime1, timeout)\n",
        "    s2 = smooth_local_search(list_of_rules, df, Y, lambda_array, delta2, delta_prime2, timeout)\n",
        "    f1 = func_evaluation(s1, list_of_rules, df, Y, lambda_array)\n",
        "    f2 = func_evaluation(s2, list_of_rules, df, Y, lambda_array)\n",
        "    if f1 > f2:\n",
        "        return s1, f1\n",
        "    else:\n",
        "        return s2, f2"
      ],
      "metadata": {
        "id": "ITgbgYl-AAEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_single_data(rules, data, majority_class):\n",
        "    rand_num = random.randrange(10)\n",
        "    #if rand_num == 0: print('Predicting data example:', data.to_dict())\n",
        "    labels = set()\n",
        "    matching_rules = []\n",
        "    \n",
        "    best_label = None\n",
        "    best_cover = 0.0\n",
        "    best_rule = None\n",
        "    for i,r in enumerate(rules):\n",
        "        if r.matches_data(data):\n",
        "            labels.add(r.class_label)\n",
        "            matching_rules.append(r)\n",
        "            \n",
        "            if r.correct_cover > best_cover: \n",
        "                best_label = r.class_label\n",
        "                best_cover = r.correct_cover\n",
        "                best_rule = r\n",
        "                \n",
        "    if best_label is None: \n",
        "        return majority_class, None\n",
        "    else: \n",
        "        return best_label, best_rule"
      ],
      "metadata": {
        "id": "OZLzFDMNAqtX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(p, q):\n",
        "    true = 0\n",
        "    for i in range(len(p)):\n",
        "        pi = p[i]\n",
        "        qi = q[i]\n",
        "        if pi == qi:\n",
        "            true += 1\n",
        "        # for j in range(len(pi)): \n",
        "            # pij = pi[j]\n",
        "            # qij = qi[j]\n",
        "            # if pij == 1: \n",
        "                # if qij == 1: \n",
        "                    # true += 1\n",
        "                # break;\n",
        "                \n",
        "    return true / len(p)"
      ],
      "metadata": {
        "id": "Jrx2IJf0ArMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def KL_divergence(p, q): \n",
        "    eps = 1e-15\n",
        "    sum = 0\n",
        "    for i in range(len(p)):  # iterating over each data example\n",
        "        pi = p[i]\n",
        "        qi = q[i]\n",
        "        for j in range(len(pi)):  # iterating over each class probability \n",
        "            pij = pi[j] + eps\n",
        "            qij = qi[j] + eps\n",
        "            sum += (pij * math.log(pij / qij))\n",
        "            \n",
        "    return sum"
      ],
      "metadata": {
        "id": "xZFSJFy3BTwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_base_predictions(X, class_rates):\n",
        "    classes = list(class_rates.keys())\n",
        "    rates = list(class_rates.values())\n",
        "\n",
        "    base_labels = []\n",
        "    for i,row in X.iterrows(): \n",
        "        b = random.choices(classes, rates)[0]\n",
        "        base_y = [class_rates[k] for k in classes]  # [1 if k==b else 0 for k in classes]\n",
        "        base_labels.append(base_y)\n",
        "        \n",
        "    return base_labels"
      ],
      "metadata": {
        "id": "l_rEh2faBToM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pattern_description (pattern, n_rows):\n",
        "\n",
        "    antecedents = []\n",
        "    for item in pattern.itemset: \n",
        "        antecedents.append(item[0] + '=' + str(item[1]))\n",
        "    \n",
        "    pred = pattern.class_label\n",
        "    sup = pattern.cover / n_rows\n",
        "    conf = pattern.correct_cover\n",
        "    acc = pattern.accurate_cover\n",
        "    desc = 'If {}, then {} (sup: {}, conf: {}, acc: {})'.format(' & '.join(antecedents), class_titles[pred], sup, conf, acc)\n",
        "    return desc"
      ],
      "metadata": {
        "id": "RPs-BxtYsgTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_predictions(rules, X, Y_list, class_rates, base_labels):\n",
        "    classes = list(class_rates.keys())  # sorted(list(class_rates.keys()))\n",
        "    \n",
        "    majority_class = None\n",
        "    max_rate = 0\n",
        "    for k,v in class_rates.items():\n",
        "        if v > max_rate: \n",
        "            max_rate = v\n",
        "            majority_class = k\n",
        "    print('Class {} is the majority class with rate {}'.format(majority_class, max_rate))\n",
        "    \n",
        "    true_labels = []\n",
        "    pred_labels = []\n",
        "    conf_labels = []\n",
        "    preds = []\n",
        "    for i,row in X.iterrows(): \n",
        "        p, rule = predict_single_data(rules, row, majority_class)\n",
        "        class_covers = rule.class_covers if rule != None else class_rates\n",
        "        \n",
        "        conf_y = [class_covers[k] if k in class_covers else 0 for k in classes]\n",
        "        conf_labels.append(conf_y)\n",
        "        \n",
        "        pred_y = [1 if k==p else 0 for k in classes]\n",
        "        pred_labels.append(pred_y)\n",
        "        preds.append(p)\n",
        "        \n",
        "        y = Y_list[i]\n",
        "        true_y = [1 if k==y else 0 for k in classes]\n",
        "        true_labels.append(true_y)\n",
        "        \n",
        "    print('True, prediction, and base labels:', list(zip(true_labels, pred_labels, conf_labels, base_labels))[:5])\n",
        "        \n",
        "    base_KL = KL_divergence(true_labels, base_labels)\n",
        "    final_KL = KL_divergence(true_labels, conf_labels)\n",
        "    info_gain = base_KL - final_KL\n",
        "    ids_acc = accuracy(Y_list, preds)  # accuracy(true_labels, pred_labels)\n",
        "    return base_KL, final_KL, info_gain, ids_acc"
      ],
      "metadata": {
        "id": "rEKzcj5SBTVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_process(X, Y, Y_true, class_rates, base_labels, use_smooth_search, search_params, n_rows):\n",
        "    print(\"----------------------\")\n",
        "    print(\"Running {} search with search params {}\".format(('smooth' if use_smooth_search else 'deterministic'), search_params))\n",
        "    t1 = datetime.datetime.now()\n",
        "    Y_list = Y.to_list()\n",
        "\n",
        "    itemsets = run_apriori(X, search_params['min_support'])\n",
        "    list_of_rules = createrules(itemsets, list(set(Y_list)))\n",
        "    \n",
        "    t2 = datetime.datetime.now()\n",
        "    print('Time for creating the candidate rules:', t2-t1)\n",
        "    print('Number of candidate rules generated:', len(list_of_rules))\n",
        "\n",
        "    if remove_inactivated_patterns:\n",
        "        filtered_list_of_rules = []\n",
        "        for i,r in enumerate(list_of_rules):\n",
        "            inactivated_rule = False\n",
        "            for attr in r.itemset: \n",
        "                if attr[1] == '0':\n",
        "                    inactivated_rule = True\n",
        "                    break\n",
        "\n",
        "            if not inactivated_rule:\n",
        "                filtered_list_of_rules.append(r)\n",
        "        \n",
        "        list_of_rules = filtered_list_of_rules\n",
        "        print('Number of candidate rules reduced to {} after removing inactivated patterns.'.format(len(list_of_rules)))\n",
        "        if len(list_of_rules) == 0: \n",
        "            return [], 0, 0\n",
        "        \n",
        "    timeout = 600000\n",
        "    soln_set = None  # [1752, 1, 19, 69]\n",
        "    obj_val = None\n",
        "\n",
        "    if soln_set is None:\n",
        "        if use_smooth_search:\n",
        "            soln_set, obj_val = apply_smooth_search(list_of_rules, X, Y_list, search_params['lambda_array'], \n",
        "                search_params['delta1'], search_params['delta_prime1'], search_params['delta2'], search_params['delta_prime2'], timeout)\n",
        "        else:\n",
        "            soln_set, obj_val = deterministic_local_search(list_of_rules, X, Y_list, search_params['lambda_array'], search_params['epsilon'], timeout)\n",
        "\n",
        "    t3 = datetime.datetime.now()\n",
        "    print('Time for local search:', t3-t2)\n",
        "    print('Best solution set indices with evaluation criteria {}: {}'.format(obj_val, soln_set))\n",
        "\n",
        "    solution_rules = [r for i,r in enumerate(list_of_rules) if i in soln_set]\n",
        "    for r in solution_rules: \n",
        "        r.compute_cover_counts(X, Y, Y_true)\n",
        "        #r.print_rule(n_rows)\n",
        "        print(get_pattern_description(r, n_rows))\n",
        "        \n",
        "    base_KL, final_KL, info_gain, ids_acc = evaluate_predictions(solution_rules, X, Y_list, class_rates, base_labels)\n",
        "    t4 = datetime.datetime.now()\n",
        "    print('Time for evaluating predictions:', t4-t3)\n",
        "    print('Total time of this execution:', t4-t1)\n",
        "    print('Base KL:', base_KL)\n",
        "    print('Final KL:', final_KL)\n",
        "    print('Info gain:', info_gain)\n",
        "    print('Accuracy:', ids_acc)\n",
        "    \n",
        "    return solution_rules, info_gain, ids_acc"
      ],
      "metadata": {
        "id": "fudDPK9pBdkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_class_titles (ds_name):\n",
        "    ctitles = {}\n",
        "    name_parts = ds_name.split('_')\n",
        "    if len(name_parts) <= 1:\n",
        "        return ctitles\n",
        "\n",
        "    n_classes = len(name_parts[1:])\n",
        "    for i,p in enumerate(name_parts[1:]):\n",
        "        ctitles[i] = p\n",
        "        if binning_classes:\n",
        "            ctitles[i + n_classes] = 'maybe ' + p\n",
        "\n",
        "    return ctitles"
      ],
      "metadata": {
        "id": "3g4sgIc_uWcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_rules(solution_rules, feature_names, n_rows, output_path):\n",
        "    rows_list = []\n",
        "    for r in solution_rules:\n",
        "        row = {c:-1 for c in feature_names}\n",
        "        row['pred'] = r.class_label\n",
        "        row['support'] = r.cover / n_rows\n",
        "        row['confidence'] = r.correct_cover\n",
        "        row['accuracy'] = r.accurate_cover\n",
        "        for item in r.itemset:\n",
        "            row[item[0]] = item[1]\n",
        "        rows_list.append(row)\n",
        "\n",
        "    df = pd.DataFrame(rows_list)\n",
        "    df.to_csv(output_path, index=False)"
      ],
      "metadata": {
        "id": "QxRn14jemB29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading data and running the process: \n",
        "\n",
        "current_setting_path = '/content/drive/My Drive/Python Projects/POEM Pipeline Results/current_setting.txt'\n",
        "with open(current_setting_path, 'r') as f:\n",
        "    current_setting_title = f.read().splitlines()[0]\n",
        "    print('Current setting:', current_setting_title)\n",
        "\n",
        "title_parts = current_setting_title.split('_')\n",
        "model_name = title_parts[0]\n",
        "dataset_name = '_'.join(title_parts[1:]) \n",
        "\n",
        "old_process = False\n",
        "binning_classes = False\n",
        "binning_features = False\n",
        "remove_inactivated_patterns = False\n",
        "class_titles = extract_class_titles(dataset_name)\n",
        "\n",
        "drive_result_path = '/content/drive/My Drive/Python Projects/POEM Pipeline Results/' + model_name + '_' + dataset_name\n",
        "if old_process:\n",
        "    drive_result_path += '_old'\n",
        "concepts_file = 'image_concepts.csv'\n",
        "concepts_path = drive_result_path + \"/\" + concepts_file\n",
        "!cp \"$concepts_path\" '.'\n",
        "\n",
        "t_start = datetime.datetime.now()\n",
        "X, Y, Y_true = load_data(concepts_file, 'pred', 'label', ['id', 'file', 'path'])\n",
        "\n",
        "n_rows = len(Y.index)\n",
        "feature_names = list(X.columns)\n",
        "class_counts = Y.value_counts().to_dict()\n",
        "class_rates = {k:(v/n_rows) for k,v in class_counts.items()}\n",
        "print('class_rates:', class_rates)\n",
        "\n",
        "base_labels = compute_base_predictions(X, class_rates)\n",
        "\n",
        "use_smooth_search = False\n",
        "params_grid = {\n",
        "    'lambda_array': [[0.5]*7],\n",
        "    'epsilon': [0.001],\n",
        "    'min_support': [0.01, 0.03, 0.05, 0.1, 0.15, 0.2]\n",
        "}\n",
        "\n",
        "# deterministic_search_params = {\n",
        "#     'lambda_array': [0.5]*7, \n",
        "#     'epsilon': 0.05\n",
        "# }\n",
        "# smooth_search_params = {\n",
        "#     'lambda_array': [1.0]*7,\n",
        "#     'delta1': 0.33,\n",
        "#     'delta_prime1': 0.33, \n",
        "#     'delta2': 0.33, \n",
        "#     'delta_prime2': -1.0\n",
        "# }\n",
        "\n",
        "#support_thres_array = [0.01]  # np.arange(0.3, 0.55, 0.05).tolist()\n",
        "#epsilon_array = [0.001]  # np.arange(0.01, 0.2, 0.02)\n",
        "\n",
        "param_keys = list(params_grid.keys())\n",
        "param_values = list(params_grid.values())\n",
        "param_combinations = list(itertools.product(*param_values))\n",
        "#output_path = 'ids_patterns.csv'\n",
        "output_path_list = []\n",
        "results = []\n",
        "print('Parameter combinations:', param_combinations)\n",
        "\n",
        "for comb in param_combinations:\n",
        "    params = {k:v for k,v in zip(param_keys, comb)}\n",
        "    solution_rules, info_gain, ids_acc = run_process(X, Y, Y_true, class_rates, base_labels, use_smooth_search, params, n_rows)\n",
        "    results.append({'params': params, 'rule_set_size': len(solution_rules), 'info_gain': info_gain})\n",
        "    output_path = 'ids_patterns_' + str(params['min_support']) + '.csv'\n",
        "    save_rules(solution_rules, feature_names, n_rows, output_path)\n",
        "    output_path_list.append(output_path)\n",
        "\n",
        "t_end = datetime.datetime.now()\n",
        "print(\"----------------------\")\n",
        "print('Total time:', t_end - t_start)\n",
        "print('Results:', results)\n",
        "\n",
        "#!cp $output_path \"$drive_result_path\"\n",
        "for path in output_path_list:\n",
        "    !cp $path \"$drive_result_path\""
      ],
      "metadata": {
        "id": "Aej7P8VEBdYD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}