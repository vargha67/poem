{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Byyz6rL5jcF"
      },
      "outputs": [],
      "source": [
        "%%writefile Includes.h\n",
        "\n",
        "#include <iostream>\n",
        "#include <iomanip>\n",
        "#include <vector>\n",
        "#include <unordered_map>\n",
        "#include <unordered_set>\n",
        "#include <algorithm>\n",
        "#include <math.h>\n",
        "#include <cmath>\n",
        "#include <random>\n",
        "//#include <intrin.h>\n",
        "#include <x86intrin.h>   // <immintrin.h> in Windows\n",
        "\n",
        "#define ind int\n",
        "#define numm double\n",
        "\n",
        "using namespace std;\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile StaticFunctions.h\n",
        "\n",
        "#ifndef STATIC_FUNCTIONS_H\n",
        "#define STATIC_FUNCTIONS_H\n",
        "\n",
        "#include \"Includes.h\"\n",
        "#include <fstream>\n",
        "#include <sstream>\n",
        "#include <chrono>\n",
        "#include <map>\n",
        "#include <set>\n",
        "\n",
        "class string_vector_hasher\n",
        "{\n",
        "public:\n",
        "\tsize_t operator()(vector<string> const &vec) const\n",
        "\t{\n",
        "\t\tsize_t seed = vec.size();\n",
        "\t\tfor (auto &i : vec)\n",
        "\t\t{\n",
        "\t\t\tseed ^= hash<string>()(i) + 0x9e3779b9 + (seed << 6) + (seed >> 2);\n",
        "\t\t}\n",
        "\t\treturn seed;\n",
        "\t}\n",
        "};\n",
        "\n",
        "class ind_vector_hasher\n",
        "{\n",
        "public:\n",
        "\tstd::size_t operator()(std::vector<ind> const &vec) const\n",
        "\t{\n",
        "\t\tstd::size_t seed = vec.size();\n",
        "\t\tfor (auto &i : vec)\n",
        "\t\t{\n",
        "\t\t\tseed ^= i + 0x9e3779b9 + (seed << 6) + (seed >> 2);\n",
        "\t\t}\n",
        "\t\treturn seed;\n",
        "\t}\n",
        "};\n",
        "\n",
        "#endif\n"
      ],
      "metadata": {
        "id": "q7He84gT6M27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile Lighthouse.h\n",
        "\n",
        "#ifndef LIGHTHOUSE_H\n",
        "#define LIGHTHOUSE_H\n",
        "\n",
        "#include \"Includes.h\"\n",
        "#include <map>\n",
        "#include \"StaticFunctions.h\"\n",
        "\n",
        "class Lighthouse\n",
        "{\n",
        "\tvector<string> attributeNames;\n",
        "\n",
        "\tunordered_map<ind, string> keyToOutcome;\n",
        "\tunordered_map<string, ind> outcomeToKey;\n",
        "\tvector<unordered_map<ind, numm>> keyToAttribute;\n",
        "\tvector<unordered_map<numm, ind>> attributeToKey;\n",
        "\n",
        "\tunordered_map<vector<ind>, vector<ind>, ind_vector_hasher> tuples; // [Attributes] -> [outcomes1, outcome2, ..., totalcount, RCTIndex]\n",
        "\n",
        "\tind D;\n",
        "\tind N;\n",
        "\tind O;\n",
        "\tind totalN;\n",
        "\n",
        "\tvector<numm> lambdas; // R_j -> Lambda_j\n",
        "\tunordered_map<ind, vector<numm>> RCT; // region -> [models, rulescounts per outcome, tuplecounts per outcome, total Tuples Counts]\n",
        "\n",
        "\tvector<vector<ind>> patterns; // [Attributes, outcome, support, correctCount]\n",
        "\tbool match(vector<ind>& tuple, vector<ind>& pattern);\n",
        "\tbool patternsEqual(vector<ind>& pattern1, vector<ind>& pattern2);\n",
        "\tvoid addRule(vector<ind>& rule);\n",
        "\tvoid iterativeScaling();\n",
        "\tnumm evalStraight(vector<ind>& pattern, vector<ind>& bestPattern, numm& bestGain);\n",
        "\tvoid generateSample(vector<vector<ind>>& sample, ind sampleSize);\n",
        "\tvoid addSimpleAncestors(vector<ind>& protoPattern, unordered_map<vector<ind>, vector<numm>, ind_vector_hasher>& candidates, ind depth, vector<ind>& data);\n",
        "public:\n",
        "\tLighthouse(string filename, vector<ind> dataColumns, ind outcomeColumn);\n",
        "\t~Lighthouse();\n",
        "\n",
        "\tnumm KLDivergence();\n",
        "\tbool originalFlashlight(ind numberRules, ind sampleSize, numm minSupport, bool removeInactivatedPatterns, bool verbose);\n",
        "\tvoid printTable(numm baseKL, string fileName);\n",
        "\tstring getTable();\n",
        "\tvoid LENS(ind numberRules, bool verbose, ind sampleSize, ind FLCandidates, ind mode);\n",
        "};\n",
        "#endif // !LIGHTHOUSE_H\n"
      ],
      "metadata": {
        "id": "fNOKyiOe6kAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile Lighthouse.cpp\n",
        "\n",
        "#include \"Lighthouse.h\"\n",
        "#include <fstream>\n",
        "#include <sstream>\n",
        "#include <deque>\n",
        "\n",
        "Lighthouse::Lighthouse(string filename, vector<ind> dataColumns, ind outcomeColumn) // parse the data\n",
        "{\n",
        "\t// build dictionaries for attributes and fill them afterwards\n",
        "\t// here the data is transformed to multisets such that for each combination of eplanatory attributes the tuples per outcome as well as total count is stored\n",
        "\tvector<unordered_set<numm>> attributeValues(dataColumns.size(), unordered_set<numm>());\n",
        "\tkeyToAttribute.resize(dataColumns.size());\n",
        "\tattributeToKey.resize(dataColumns.size());\n",
        "\tattributeNames.resize(dataColumns.size() + 1);\n",
        "\n",
        "\tvector<vector<string>> raw(0);\n",
        "\tstring line;\n",
        "\tifstream data(filename);\n",
        "\twhile (getline(data, line)) {\n",
        "\t\tstringstream lineStream(line);\n",
        "\t\tstring cell;\n",
        "\t\tvector<string> lineVec;\n",
        "\t\twhile (getline(lineStream, cell, ',')) { lineVec.push_back(cell); }\n",
        "\t\traw.push_back(lineVec);\n",
        "\t}\n",
        "\tfor (ind j = 0; j < dataColumns.size(); ++j) {\n",
        "\t\tattributeNames[j] = raw[0][dataColumns[j]];\n",
        "\t}\n",
        "\tattributeNames[dataColumns.size()] = raw[0][outcomeColumn];\n",
        "\tfor (ind i = 1; i < raw.size(); ++i) {\n",
        "\t\tstring outcome = raw[i][outcomeColumn];\n",
        "\t\tif (outcomeToKey.count(outcome) == 0) {\n",
        "\t\t\toutcomeToKey.emplace(outcome, outcomeToKey.size());\n",
        "\t\t\tkeyToOutcome.emplace(keyToOutcome.size(), outcome);\n",
        "\t\t}\n",
        "\t\tfor (ind j = 0; j < dataColumns.size(); ++j) {\n",
        "\t\t\tnumm attribute(stof(raw[i][dataColumns[j]]));\n",
        "\t\t\tif (attributeValues[j].count(attribute) == 0) {\n",
        "\t\t\t\tattributeValues[j].insert(attribute);\n",
        "\t\t\t}\n",
        "\t\t}\n",
        "\t}\n",
        "\tfor (ind j = 0; j < dataColumns.size(); ++j) {\n",
        "\t\tvector<numm> values(attributeValues[j].begin(), attributeValues[j].end());\n",
        "\t\tstd::sort(values.begin(), values.end());\n",
        "\t\tfor (ind i = 0; i < values.size(); ++i) {\n",
        "\t\t\tkeyToAttribute[j].emplace(i, values[i]);\n",
        "\t\t\tattributeToKey[j].emplace(values[i], i);\n",
        "\t\t}\n",
        "\t}\n",
        "\tfor (ind i = 1; i < raw.size(); ++i) {\n",
        "\t\tvector<ind> tuple(dataColumns.size(), 0);\n",
        "\t\tfor (ind j = 0; j < dataColumns.size(); ++j) {\n",
        "\t\t\ttuple[j] = attributeToKey[j].find(stof(raw[i][dataColumns[j]]))->second;\n",
        "\t\t}\n",
        "\t\tauto ref = tuples.find(tuple);\n",
        "\t\tif (ref == tuples.end()) { ref = tuples.emplace(tuple, vector<ind>(keyToOutcome.size() + 2, 0)).first; }\n",
        "\t\tref->second[keyToOutcome.size()] += 1;\n",
        "\t\tref->second[outcomeToKey.find(raw[i][outcomeColumn])->second] += 1;\n",
        "\t}\n",
        "\tN = tuples.size(); // number of tuples total\n",
        "\tD = dataColumns.size(); // number of dimensions\n",
        "\tO = outcomeToKey.size(); // number of outcome values\n",
        "\tvector<ind> resultcounts(O, 0);\n",
        "\ttotalN = 0;\n",
        "\tfor (auto iter = tuples.begin(); iter != tuples.end(); ++iter) {\n",
        "\t\tfor (ind o = 0; o < O; ++o) {\n",
        "\t\t\tresultcounts[o] += iter->second[o];\n",
        "\t\t}\n",
        "\t\ttotalN += iter->second[O];\n",
        "\t}\n",
        "\t// construct RCT for with default case as no rule exists yet\n",
        "\tvector<numm> rctBase(3 * O + 1);\n",
        "\tfor (ind o = 0; o < O; ++o) {\n",
        "\t\trctBase[o] = 1.0 / (numm)O;\n",
        "\t\trctBase[O + o] = 0;\n",
        "\t\trctBase[2 * O + o] = resultcounts[o];\n",
        "\t}\n",
        "\trctBase[3 * O] = totalN;\n",
        "\tRCT.emplace(0, rctBase);\n",
        "\tlambdas = vector<numm>(0); // R_j -> Lambda_j\n",
        "\tvector<ind> pattern(2 * D + 3, 0);\n",
        "\tfor (ind d = 0; d < D; ++d) {\n",
        "\t\tpattern[D + d] = attributeToKey[d].size() - 1;\n",
        "\t}\n",
        "\tpattern[2 * D + 1] = totalN;\n",
        "\tfor (ind o = 0; o < O; ++o) { // Add default rules for each outcome, i.e. one rule matching all attrbiutes per outcome value\n",
        "\t\tvector<ind> p2 = pattern;\n",
        "\t\tp2[2 * D] = o;\n",
        "\t\tp2[2 * D + 2] = resultcounts[o];\n",
        "\t\taddRule(p2);\n",
        "\t}\n",
        "\titerativeScaling();\n",
        "}\n",
        "\n",
        "bool Lighthouse::match(vector<ind>& tuple, vector<ind>& pattern) // test whether a tupple matches a pattern\n",
        "{\n",
        "\tfor (ind i = 0; i < D; ++i) {\n",
        "\t\tif (pattern[i] > tuple[i] || pattern[D+i] < tuple[i]) { return false; }\n",
        "\t}\n",
        "\treturn true;\n",
        "}\n",
        "\n",
        "bool Lighthouse::patternsEqual(vector<ind>& pattern1, vector<ind>& pattern2) // test whether two patterns are equal\n",
        "{\n",
        "\tfor (ind i = 0; i < D; ++i) {\n",
        "\t\tif (pattern1[i] != pattern2[i] || pattern1[D+i] != pattern2[D+i]) { return false; }\n",
        "\t}\n",
        "\tif (pattern1[D] != pattern2[D]) { return false; }\n",
        "\treturn true;\n",
        "}\n",
        "\n",
        "void Lighthouse::addRule(vector<ind>& rule) // insert a rule into the Explanation table\n",
        "{\n",
        "\tfor (auto iter = tuples.begin(); iter != tuples.end(); ++iter) {\n",
        "\t\tvector<ind> alocTup = iter->first; // for each affected pattern, the RCT has to be corrected\n",
        "\t\tif (match(alocTup, rule)) {\n",
        "\t\t\tind oldIndex = iter->second[O + 1];\n",
        "\t\t\tind newIndex = oldIndex + ((ind)1 << patterns.size());\n",
        "\t\t\tauto oldRef = RCT.find(oldIndex);\n",
        "\t\t\tauto newRef = RCT.find(newIndex);\n",
        "\t\t\tif (newRef == RCT.end()) { newRef = RCT.emplace(newIndex, vector<numm>(3 * O + 1, 0.0)).first; } // if new RCT entry doesn't exist yet, add it\n",
        "\t\t\tnewRef->second[3 * O] += iter->second[O];\n",
        "\t\t\toldRef->second[3 * O] -= iter->second[O];\n",
        "\t\t\tfor (ind o = 0; o < O; ++o) {\n",
        "\t\t\t\tnewRef->second[o] = oldRef->second[o];\n",
        "\t\t\t\tnewRef->second[O + o] = oldRef->second[O + o];\n",
        "\t\t\t\tif (o == rule[2*D]) { newRef->second[O + o] += 1; }\n",
        "\t\t\t\tnewRef->second[2 * O + o] += iter->second[o];\n",
        "\t\t\t\toldRef->second[2 * O + o] -= iter->second[o];\n",
        "\n",
        "\t\t\t}\n",
        "\t\t\titer->second[O + 1] = newIndex;\n",
        "\t\t\tif (oldRef->second[3 * O] <= 0) { RCT.erase(oldRef); } // if old RCT entry is empty now, clean it up\n",
        "\t\t}\n",
        "\t}\n",
        "\tpatterns.push_back(rule);\n",
        "\tlambdas.push_back(0);\n",
        "}\n",
        "\n",
        "void Lighthouse::iterativeScaling() // perform iterative scaling as described in the paper\n",
        "{\n",
        "\tnumm limit = 0.1 / (numm)totalN;\n",
        "\tbool divergent = true;\n",
        "\twhile (divergent) {\n",
        "\t\tdivergent = false;\n",
        "\t\tfor (ind p = 0; p < patterns.size(); ++p) {\n",
        "\t\t\tind outcome = patterns[p][2*D];\n",
        "\t\t\tnumm percentageApplicableTuples = (numm)patterns[p][2*D + 2] / (numm)totalN;\n",
        "\t\t\tnumm expectedSum = 0;\n",
        "\t\t\tnumm deriv = 0;\n",
        "\t\t\tnumm delta = 0;\n",
        "\t\t\tfor (auto iter = RCT.begin(); iter != RCT.end(); ++iter) {\n",
        "\t\t\t\tif (iter->first & ((ind)1 << p)) {\n",
        "\t\t\t\t\tnumm term = (iter->second[3 * O] / (numm)totalN) * iter->second[outcome] * exp(delta * iter->second[O + outcome]);\n",
        "\t\t\t\t\texpectedSum += term;\n",
        "\t\t\t\t\tderiv += iter->second[O + outcome] * term;\n",
        "\t\t\t\t}\n",
        "\t\t\t}\n",
        "\t\t\twhile (abs(expectedSum - percentageApplicableTuples) > limit && abs(delta) < 50) {\n",
        "\t\t\t\tdivergent = true;\n",
        "\t\t\t\tif (abs((expectedSum - percentageApplicableTuples) / deriv) > 100) {\n",
        "\t\t\t\t\tderiv*=100;\n",
        "\t\t\t\t}\n",
        "\t\t\t\tdelta -= (expectedSum - percentageApplicableTuples) / deriv;\n",
        "\t\t\t\texpectedSum = 0;\n",
        "\t\t\t\tderiv = 0;\n",
        "\t\t\t\tfor (auto iter = RCT.begin(); iter != RCT.end(); ++iter) {\n",
        "\t\t\t\t\tif (iter->first & ((ind)1 << p)) {\n",
        "\t\t\t\t\t\tnumm term = (iter->second[3 * O] / (numm)totalN) * iter->second[outcome] * exp(delta * iter->second[O + outcome]);\n",
        "\t\t\t\t\t\texpectedSum += term;\n",
        "\t\t\t\t\t\tderiv += iter->second[O + outcome] * term;\n",
        "\t\t\t\t\t}\n",
        "\t\t\t\t}\n",
        "\t\t\t}\n",
        "\t\t\tlambdas[p] += delta;\n",
        "\t\t\tif (delta != 0) {\n",
        "\t\t\t\tfor (auto iter = RCT.begin(); iter != RCT.end(); ++iter) {\n",
        "\t\t\t\t\tif (iter->first & ((ind)1 << p)) {\n",
        "\t\t\t\t\t\tnumm denominator = 0;\n",
        "\t\t\t\t\t\tvector<numm> numerator(O, 0);\n",
        "\t\t\t\t\t\tfor (ind o = 0; o < O; ++o) {\n",
        "\t\t\t\t\t\t\tnumm lambdaSum = 0;\n",
        "\t\t\t\t\t\t\tfor (ind p2 = 0; p2 < patterns.size(); ++p2) {\n",
        "\t\t\t\t\t\t\t\tif ((patterns[p2][2*D] == o) && (iter->first & ((ind)1 << p2))) {\n",
        "\t\t\t\t\t\t\t\t\tlambdaSum += lambdas[p2];\n",
        "\t\t\t\t\t\t\t\t}\n",
        "\t\t\t\t\t\t\t}\n",
        "\t\t\t\t\t\t\tnumerator[o] = exp(lambdaSum);\n",
        "\t\t\t\t\t\t\tdenominator += exp(lambdaSum);\n",
        "\t\t\t\t\t\t}\n",
        "\t\t\t\t\t\tfor (ind o = 0; o < O; ++o) {\n",
        "\t\t\t\t\t\t\titer->second[o] = numerator[o] / denominator;\n",
        "\t\t\t\t\t\t}\n",
        "\t\t\t\t\t}\n",
        "\t\t\t\t}\n",
        "\t\t\t}\n",
        "\t\t}\n",
        "\t}\n",
        "}\n",
        "\n",
        "numm Lighthouse::KLDivergence() // measure KL divergence of current Explanation Table to empiricial distribution\n",
        "{\n",
        "\tnumm sum = 0;\n",
        "\tnumm eps = 1e-15;\n",
        "\tfor (auto iter = tuples.begin(); iter != tuples.end(); ++iter) {\n",
        "\t\tnumm total = (numm)iter->second[O];\n",
        "\t\tfor (ind o = 0; o < O; ++o) {\n",
        "\t\t\t//numm pi = (numm)iter->second[o] / (numm)iter->second[O];\n",
        "\t\t\t//numm qi = RCT.find(iter->second[O + 1])->second[o];\n",
        "\t\t\t//pi = pi + eps;\n",
        "\t\t\t//qi = qi + eps;\n",
        "\t\t\t//if ((pi != 0) && (qi != 0)) { sum += iter->second[O] * (pi*log(pi / qi)); }\n",
        "\n",
        "\t\t\t// alternative method of computation to compare with IDS and decision trees: \n",
        "\t\t\tnumm qi = RCT.find(iter->second[O + 1])->second[o] + eps;\n",
        "\t\t\tnumm pi = 1 + eps;\n",
        "\t\t\tnumm cnt = (numm)iter->second[o];\n",
        "\t\t\tsum += (cnt * (pi * log(pi / qi))); \n",
        "\n",
        "\t\t\tpi = eps;\n",
        "\t\t\tcnt = total - cnt;\n",
        "\t\t\tsum += (cnt * (pi * log(pi / qi)));\n",
        "\t\t}\n",
        "\t}\n",
        "\treturn sum;\n",
        "}\n",
        "\n",
        "numm Lighthouse::evalStraight(vector<ind>& pattern, vector<ind>& bestPattern, numm& bestGain) // selects best outcome for pattern and stores it in bestPattern\n",
        "{\n",
        "\tnumm thisGain = 0;\n",
        "\tind sup = 0;\n",
        "\tvector<ind> hits(O, 0);\n",
        "\tvector<numm> model(O, 0);\n",
        "\tfor (auto iter = tuples.begin(); iter != tuples.end(); ++iter) {\n",
        "\t\tvector<ind> tuple = iter->first;\n",
        "\t\tif (match(tuple, pattern)) {\n",
        "\t\t\tsup += iter->second[O];\n",
        "\t\t\tfor (ind o = 0; o < O; ++o) {\n",
        "\t\t\t\thits[o] += iter->second[o];\n",
        "\t\t\t\tmodel[o] += iter->second[O] * RCT.find(iter->second[O + 1])->second[o];\n",
        "\t\t\t}\n",
        "\t\t}\n",
        "\n",
        "\t}\n",
        "\tfor (ind o = 0; o < O; ++o) {\n",
        "\t\tnumm supportF = sup;\n",
        "\t\tnumm trueRate = (numm)hits[o] / supportF;\n",
        "\t\tnumm expRate = model[o] / supportF;\n",
        "\t\tif ((trueRate != 0) && (expRate != 0)) {\n",
        "\t\t\tnumm gain = supportF * trueRate *log(trueRate / expRate);\n",
        "\t\t\tfor (ind o2 = 0; o2 < O; ++o2) {\n",
        "\t\t\t\tif (o2 != o) {\n",
        "\t\t\t\t\tnumm tr = (numm)hits[o2] / supportF;\n",
        "\t\t\t\t\tnumm oldER = model[o2] / supportF;\n",
        "\t\t\t\t\tnumm newER = (oldER / (1.0 - expRate)) * (1.0 - trueRate);\n",
        "\t\t\t\t\tif ((tr != 0) && (oldER != 0) && (newER != 0)) {\n",
        "\t\t\t\t\t\tgain += supportF * tr * (log(tr / oldER) - log(tr / newER));\n",
        "\t\t\t\t\t}\n",
        "\t\t\t\t}\n",
        "\t\t\t}\n",
        "\t\t\tthisGain = max(thisGain, gain);\n",
        "\t\t\tif (gain > bestGain) {\n",
        "\t\t\t\tbestGain = gain;\n",
        "\t\t\t\tbestPattern = pattern;\n",
        "\t\t\t\tbestPattern.push_back(o);\n",
        "\t\t\t\tbestPattern.push_back(sup);\n",
        "\t\t\t\tbestPattern.push_back(hits[o]);\n",
        "\t\t\t}\n",
        "\t\t}\n",
        "\t}\n",
        "\treturn thisGain;\n",
        "}\n",
        "\n",
        "\n",
        "void Lighthouse::generateSample(vector<vector<ind>>& sample, ind sampleSize) { // draw a sample of desired size from the data weighted by total occurence\n",
        "\t// Because we transformed teh data to a multiset, naive sample from \"tuples\" neglects more common items and is not a fair sample from the original data.\n",
        "\t// So we generate number between 0 and N and accumulate the tuple count in order until the cumulative um reaches the randum count, this yields a fair randomly drawn tuple\n",
        "\tind realSampleSize = min(totalN, sampleSize);\n",
        "\trandom_device rd;\n",
        "\tmt19937_64 gen(rd());\n",
        "\tvector<ind> thresholds(totalN);\n",
        "\tfor (ind i = 0; i < totalN; ++i) { thresholds[i] = i; }\n",
        "\tfor (ind i = 0; i < realSampleSize; ++i) {\n",
        "\t\tuniform_int_distribution<ind> indexGen(i, totalN - 1);\n",
        "\t\tind index = indexGen(gen);\n",
        "\t\tswap(thresholds[i], thresholds[index]);\n",
        "\t}\n",
        "\tthresholds.resize(realSampleSize);\n",
        "\tsort(thresholds.begin(), thresholds.end());\n",
        "\tind s = 0;\n",
        "\tind cumulative = 0;\n",
        "\tfor (auto iter = tuples.begin(); iter != tuples.end() && s < realSampleSize; ++iter) {\n",
        "\t\tcumulative += iter->second[O];\n",
        "\t\tif (cumulative >= thresholds[s]) {\n",
        "\t\t\tsample[s] = iter->first;\n",
        "\t\t\t++s;\n",
        "\t\t}\n",
        "\t}\n",
        "\tsample.resize(s);\n",
        "}\n",
        "\n",
        "void Lighthouse::addSimpleAncestors(vector<ind>& protoPattern, unordered_map<vector<ind>, vector<numm>, ind_vector_hasher>& candidates, ind depth, vector<ind>& data)\n",
        "{\n",
        "\t// generate ancestor patterns and att them to the candidate list (or update their counts if they already exist)\n",
        "\tif (depth < D) {\n",
        "\t\tif (protoPattern[depth] == protoPattern[D + depth]) {\n",
        "\t\t\tind temp = protoPattern[depth];\n",
        "\t\t\tprotoPattern[depth] = 0;\n",
        "\t\t\tprotoPattern[D + depth] = attributeToKey[depth].size() - 1;\n",
        "\t\t\taddSimpleAncestors(protoPattern, candidates, depth + 1, data);\n",
        "\t\t\tprotoPattern[depth] = temp;\n",
        "\t\t\tprotoPattern[D + depth] = temp;\n",
        "\t\t}\n",
        "\t\taddSimpleAncestors(protoPattern, candidates, depth + 1, data);\n",
        "\t}\n",
        "\telse {\n",
        "\t\tauto ref = candidates.find(protoPattern);\n",
        "\t\tif (ref == candidates.end()) {\n",
        "\t\t\tref = candidates.emplace(protoPattern, vector<numm>(2 * O + 1, 0)).first;\n",
        "\t\t}\n",
        "\t\tfor (ind o = 0; o < O; ++o) {\n",
        "\t\t\tref->second[2 * o] += data[o];\n",
        "\t\t\tref->second[2 * o + 1] += (numm)data[O] * RCT.find(data[O + 1])->second[o];\n",
        "\t\t}\n",
        "\t\tref->second[2 * O] += data[O];\n",
        "\t}\n",
        "}\n",
        "\n",
        "Lighthouse::~Lighthouse()\n",
        "{\n",
        "}\n",
        "\n",
        "bool Lighthouse::originalFlashlight(ind numberRules, ind sampleSize, numm minSupport, bool removeInactivatedPatterns, bool verbose)\n",
        "{\n",
        "\t// internal counting gain and time for verbose mode\n",
        "\tauto globalStart = chrono::high_resolution_clock::now();\n",
        "\tnumm baseKL = KLDivergence();\n",
        "\tnumm recentKL = baseKL;\n",
        "\tfor (ind rules = 0; rules < numberRules; ++rules) {\n",
        "\t\tauto start = chrono::high_resolution_clock::now();\n",
        "\n",
        "\t\t// draw sample\n",
        "\t\tvector<vector<ind>> sample(sampleSize);\n",
        "\t\tgenerateSample(sample, sampleSize);\n",
        "\n",
        "\t\t// generate LCA table immediatly with aggregates [cf. Table 7 in El Gebaly et al. 2014]\n",
        "\t\tunordered_map<vector<ind>, vector<numm>, ind_vector_hasher> patternCandidates; // Format of map: [AttributesLow, AttributesHigh] -> [Out1Count, Out1Exp, Out2Count, ..., OutxExp, TotalCount]\n",
        "\t\tfor (auto iter = tuples.begin(); iter != tuples.end(); ++iter) {\n",
        "\t\t\tfor (ind s = 0; s < sample.size(); ++s) {\n",
        "\t\t\t\tvector<ind> protoPattern(2 * D, 0);\n",
        "\t\t\t\tfor (ind k = 0; k < D; ++k) {\n",
        "\t\t\t\t\tif (iter->first[k] == sample[s][k]) {\n",
        "\t\t\t\t\t\tprotoPattern[k] = sample[s][k];\n",
        "\t\t\t\t\t\tprotoPattern[D + k] = sample[s][k];\n",
        "\t\t\t\t\t}\n",
        "\t\t\t\t\telse {\n",
        "\t\t\t\t\t\tprotoPattern[k] = 0;\n",
        "\t\t\t\t\t\tprotoPattern[D + k] = attributeToKey[k].size() - 1;\n",
        "\t\t\t\t\t}\n",
        "\t\t\t\t}\n",
        "\t\t\t\taddSimpleAncestors(protoPattern, patternCandidates, 0, iter->second);\n",
        "\t\t\t}\n",
        "\t\t}\n",
        "\t\t// evalaute gain of ancestor patterns, i.e., correct aggregates cf. [Table 8 in El Gebaly et al. 2014] and [Sec. 3.2 in Vollmer et al. 2019]\n",
        "\t\tvector<pair<numm, vector<ind>>> topTemplates;\n",
        "\t\tnumm minGain = 0;\n",
        "\t\tfor (auto iter = patternCandidates.begin(); iter != patternCandidates.end(); ++iter) {\n",
        "\t\t\tnumm matchInSample = 0;\n",
        "\t\t\tvector<ind> allocatedTuple = iter->first;\n",
        "\n",
        "\t\t\tif (removeInactivatedPatterns) {\n",
        "\t\t\t\tbool inactivatedPattern = false;\n",
        "\t\t\t\t// std::stringstream stream;\n",
        "\t\t\t\tfor (ind d = 0; d < D; ++d) {\n",
        "\t\t\t\t\t// stream << attributeNames[d] << \"=\" << allocatedTuple[d] << \", \";\n",
        "\t\t\t\t\tif ((allocatedTuple[d] == 0) && (allocatedTuple[D + d] == 0)) {\n",
        "\t\t\t\t\t\tinactivatedPattern = true;\n",
        "\t\t\t\t\t\tbreak;\n",
        "\t\t\t\t\t}\n",
        "\t\t\t\t}\n",
        "\t\t\t\tif (inactivatedPattern) {\n",
        "\t\t\t\t\t// cout << stream.str() << \"\\n\";\n",
        "\t\t\t\t\tcontinue;\n",
        "\t\t\t\t}\n",
        "\t\t\t}\n",
        "\n",
        "\t\t\tfor (ind s = 0; s < sample.size(); ++s) {\n",
        "\t\t\t\tif (match(sample[s], allocatedTuple)) { matchInSample += 1; }\n",
        "\t\t\t}\n",
        "\t\t\titer->second[2 * O] /= (numm)matchInSample;\n",
        "\t\t\tfor (ind o = 0; o < O; ++o) {\n",
        "\t\t\t\titer->second[2 * o] /= (numm)matchInSample;\n",
        "\t\t\t\titer->second[2 * o + 1] /= (numm)matchInSample;\n",
        "\t\t\t}\n",
        "\t\t\tnumm maxGain = 0;\n",
        "\t\t\tfor (ind o = 0; o < O; ++o) {\n",
        "\t\t\t\tnumm trueRate = iter->second[2 * o] / (numm)iter->second[2 * O];\n",
        "\t\t\t\tnumm expectedRate = iter->second[2 * o + 1] / (numm)iter->second[2 * O];\n",
        "\t\t\t\tnumm supportRate = (numm)iter->second[2 * O] / (numm)totalN;\n",
        "\n",
        "\t\t\t\tif ((trueRate != 0) && (expectedRate != 0) && (supportRate >= minSupport)) {\n",
        "\t\t\t\t\tnumm gain = iter->second[2 * O] * trueRate *log(trueRate / expectedRate);\n",
        "\t\t\t\t\tfor (ind o2 = 0; o2 < O; ++o2) {\n",
        "\t\t\t\t\t\tif (o2 != o) {\n",
        "\t\t\t\t\t\t\tnumm tr = iter->second[2 * o2] / (numm)iter->second[2 * O];\n",
        "\t\t\t\t\t\t\tnumm oldER = iter->second[2 * o2 + 1] / (numm)iter->second[2 * O];\n",
        "\t\t\t\t\t\t\tnumm newER = (oldER / (1.0 - expectedRate)) * (1.0 - trueRate);\n",
        "\t\t\t\t\t\t\tif ((tr != 0) && (oldER != 0) && (newER != 0)) {\n",
        "\t\t\t\t\t\t\t\tgain += iter->second[2 * O] * tr * (log(tr / oldER) - log(tr / newER));\n",
        "\t\t\t\t\t\t\t}\n",
        "\t\t\t\t\t\t}\n",
        "\t\t\t\t\t}\n",
        "\t\t\t\t\tmaxGain = max(maxGain, gain);\n",
        "\t\t\t\t}\n",
        "\t\t\t}\n",
        "\t\t\tif (maxGain > minGain) {\n",
        "\t\t\t\ttopTemplates.emplace_back(maxGain, allocatedTuple);\n",
        "\t\t\t\tif (topTemplates.size() > 10) {\n",
        "\t\t\t\t\tsort(topTemplates.rbegin(), topTemplates.rend());\n",
        "\t\t\t\t\ttopTemplates.resize(10);\n",
        "\t\t\t\t\tminGain = topTemplates[9].first;\n",
        "\t\t\t\t}\n",
        "\t\t\t}\n",
        "\t\t}\n",
        "\n",
        "\t\tif (topTemplates.size() == 0) {\n",
        "\t\t\tcout << \"No pattern candidates generated, should end the process!\" << \"\\n\";\n",
        "\t\t\treturn false;\n",
        "\t\t}\n",
        "\n",
        "\t\t// insert best pattern into the explanation table\n",
        "\t\tvector<ind> bestPattern(2 * D + 3, 0);\n",
        "\t\tnumm bestGain = 0;\n",
        "\t\tevalStraight(topTemplates[0].second, bestPattern, bestGain); // select best outcome for the pattern\n",
        "\n",
        "\t\t// If the pattern is already added to the table, the process should end: \n",
        "\t\tfor (ind p = 0; p < patterns.size(); ++p) {\n",
        "\t\t\t  vector<ind> pat = patterns[p];\n",
        "\t\t\t  if (patternsEqual(pat, bestPattern)) {\n",
        "\t\t\t\t  cout << \"New pattern already exists in the table, should end the process!\" << \"\\n\";\n",
        "\t\t\t\t  return false;\n",
        "\t\t\t  }\n",
        "\t\t}\n",
        "\n",
        "\t\taddRule(bestPattern);\n",
        "\t\titerativeScaling();\n",
        "\t\tauto end = chrono::high_resolution_clock::now();\n",
        "\t\tif (verbose) {\n",
        "\t\t\tnumm currentKL = KLDivergence();\n",
        "\t\t\tcout << \"FL Original \" << sampleSize << \",\" << rules + 1 << \",\" << chrono::duration_cast<chrono::milliseconds>(end - start).count()<<\",\" << chrono::duration_cast<chrono::milliseconds>(end - globalStart).count() << \",\";\n",
        "\t\t\tcout << recentKL - currentKL << \",\" << baseKL - currentKL << \",\" << currentKL << \"\\n\";\n",
        "\t\t\trecentKL = currentKL;\n",
        "\t\t}\n",
        "\t\treturn true;\n",
        "\t}\n",
        "}\n",
        "\n",
        "void Lighthouse::printTable(numm baseKL, string fileName) // print explanation table to console\n",
        "{\n",
        "\tofstream file;\n",
        "\tfile.open(fileName, ofstream::trunc);\n",
        "\n",
        "\tnumm KL = KLDivergence();\n",
        "\tnumm gain = baseKL - KL;\n",
        "\tcout << \"\\n===[Table Size: \" << patterns.size() << \" | Base KL-Div: \" << baseKL << \" | KL-Div: \" << KL << \" | Gain: \" << gain << \"]===\\n\";\n",
        "\tfor (ind d = 0; d < D; ++d) {\n",
        "\t\tcout<<attributeNames[d]<<\",\";\n",
        "\t\tfile << attributeNames[d] << \",\";\n",
        "\t}\n",
        "\tcout<<attributeNames[D]<<\"\\n\";\n",
        "\tfile << attributeNames[D] << \",support,confidence\" << \"\\n\" << flush;\n",
        "\n",
        "\tfor (ind p = 0; p < patterns.size(); ++p) {\n",
        "\t\tfor (ind d = 0; d < D; ++d) {\n",
        "\t\t\tif ((patterns[p][d] == 0) && (patterns[p][D + d] == keyToAttribute[d].size() - 1) && (p >= O)) {\n",
        "\t\t\t\tcout << \"[*],\";\n",
        "\t\t\t\tif (p >= O) {   // if not among two first general distribution patterns\n",
        "\t\t\t\t\tfile << \"-1,\";\n",
        "\t\t\t\t}\n",
        "\t\t\t} \n",
        "\t\t\telse {\n",
        "\t\t\t\tnumm featureValue = 0;\n",
        "\t\t\t\tnumm firstValue = keyToAttribute[d].find(patterns[p][d])->second;\n",
        "\t\t\t\tnumm secondValue = keyToAttribute[d].find(patterns[p][D+d])->second;\n",
        "\t\t\t\tif (firstValue == secondValue) {\n",
        "\t\t\t\t\tfeatureValue = firstValue;\n",
        "\t\t\t\t}\n",
        "\t\t\t\telse {     // handling the binned features case where more than one feature value may be possible in a pattern\n",
        "\t\t\t\t\tfeatureValue = ((secondValue - firstValue) / 2);\n",
        "\t\t\t\t}\n",
        "\n",
        "\t\t\t\tcout << \"[\" << firstValue << \"-\" << secondValue << \"],\";\n",
        "\t\t\t\tif (p >= O) {\n",
        "\t\t\t\t\tfile << featureValue << \",\";\n",
        "\t\t\t\t}\n",
        "\t\t\t}\n",
        "\t\t}\n",
        "\n",
        "\t\tstring outcome = keyToOutcome.find(patterns[p][2 * D])->second; \n",
        "\t\tind support = patterns[p][2 * D + 1];\n",
        "\t\tnumm supportPercentage = (numm)support / (numm)totalN;\n",
        "\t\tnumm confidence = (numm)patterns[p][2 * D + 2] / (numm)support;\n",
        "\n",
        "\t\tcout << outcome << \"  ; Sup: \" << support << \" Prec: \" << confidence << \"\\n\";\n",
        "\t\tif (p >= O) {\n",
        "\t\t\tfile << outcome << \",\" << supportPercentage << \",\" << confidence << \"\\n\" << flush;\n",
        "\t\t}\n",
        "\t}\n",
        "}\n",
        "\n",
        "string Lighthouse::getTable() //returns the entire explanation table as string\n",
        "{\n",
        "\tstring result = \"\";\n",
        "\tresult += \"\\n===[Table Size: \" + to_string(patterns.size()) + \" | KL-Div: \" + to_string(KLDivergence()) + \"]===\\n\";\n",
        "\tfor (ind d = 0; d < D; ++d) {\n",
        "\t\tresult += attributeNames[d] + \",\";\n",
        "\t}\n",
        "\tresult += attributeNames[D] + \"\\n\";\n",
        "\tfor (ind p = 0; p < patterns.size(); ++p) {\n",
        "\t\tfor (ind d = 0; d < D; ++d) {\n",
        "\t\t\tif ((patterns[p][d] == 0) && (patterns[p][D + d] == keyToAttribute[d].size() - 1) && (p >= O)) {\n",
        "\t\t\t\tresult += + \"[*],\";\n",
        "\t\t\t}\n",
        "\t\t\telse {\n",
        "\t\t\t\tresult += + \"[\" + to_string(keyToAttribute[d].find(patterns[p][d])->second) + \"-\" + to_string(keyToAttribute[d].find(patterns[p][D + d])->second) + \"],\";\n",
        "\t\t\t}\n",
        "\t\t}\n",
        "\t\tresult += keyToOutcome.find(patterns[p][2 * D])->second + \"  ; Sup: \" + to_string(patterns[p][2 * D + 1]) + \" Prec: \" + to_string((numm)patterns[p][2 * D + 2] / (numm)patterns[p][2 * D + 1]) + \"\\n\";\n",
        "\t}\n",
        "\treturn result;\n",
        "}\n",
        "\n",
        "void Lighthouse::LENS(ind numberRules, bool verbose, ind samplesize, ind FLCandidates, ind mode)\n",
        "{\n",
        "\tconst ind patternCutoff = 6; // define limit for dimensions of pattern expansion, i.e., if a simple pattern from Flashlight has 6 or more constants it is not considered for expansion\n",
        "\t// internal counting gain and time for verbose mode\n",
        "\tauto globalStart = chrono::high_resolution_clock::now();\n",
        "\tnumm baseKL = KLDivergence();\n",
        "\tnumm recentKL = baseKL;\n",
        "\tfor (ind rules = 0; rules < numberRules; ++rules) {\n",
        "\t\tauto start = chrono::high_resolution_clock::now();\n",
        "\n",
        "\t\t// draw sample for flashlight\n",
        "\t\tvector<vector<ind>> sample(samplesize);\n",
        "\t\tgenerateSample(sample, samplesize);\n",
        "\n",
        "\t\t// generate LCA table immediatly with aggregates [cf. Table 7 in El Gebaly et al. 2014]\n",
        "\t\tunordered_map<vector<ind>, vector<numm>, ind_vector_hasher> patternCandidates; // [AttributesLow, AttributesHigh] -> [Out1Count, Out1Exp, Out2Count, ..., OutxExp, TotalCount]\n",
        "\t\tfor (auto iter = tuples.begin(); iter != tuples.end(); ++iter) {\n",
        "\t\t\tfor (ind s = 0; s < sample.size(); ++s) {\n",
        "\t\t\t\tvector<ind> protoPattern(2 * D, 0);\n",
        "\t\t\t\tfor (ind k = 0; k < D; ++k) {\n",
        "\t\t\t\t\tif (iter->first[k] == sample[s][k]) {\n",
        "\t\t\t\t\t\tprotoPattern[k] = sample[s][k];\n",
        "\t\t\t\t\t\tprotoPattern[D + k] = sample[s][k];\n",
        "\t\t\t\t\t}\n",
        "\t\t\t\t\telse {\n",
        "\t\t\t\t\t\tprotoPattern[k] = 0;\n",
        "\t\t\t\t\t\tprotoPattern[D + k] = attributeToKey[k].size() - 1;\n",
        "\t\t\t\t\t}\n",
        "\t\t\t\t}\n",
        "\t\t\t\taddSimpleAncestors(protoPattern, patternCandidates, 0, iter->second);\n",
        "\t\t\t}\n",
        "\t\t}\n",
        "\n",
        "\t\tvector<pair<numm, vector<ind>>> topTemplates;\n",
        "\t\tnumm minGain = 0;\n",
        "\t\tfor (auto iter = patternCandidates.begin(); iter != patternCandidates.end(); ++iter) {\n",
        "\t\t\tind patternDim = 0;\n",
        "\t\t\tfor (ind d = 0; d < D; ++d) {\n",
        "\t\t\t\tif (iter->first[d] == iter->first[D + d]) {++patternDim;}\n",
        "\t\t\t}\n",
        "\t\t\tif (patternDim < patternCutoff) {\n",
        "\t\t\t\tnumm matchInSample = 0;\n",
        "\t\t\t\tvector<ind> allocatedTuple = iter->first;\n",
        "\t\t\t\tfor (ind s = 0; s < sample.size(); ++s) {\n",
        "\t\t\t\t\tif (match(sample[s], allocatedTuple)) { matchInSample += 1; }\n",
        "\t\t\t\t}\n",
        "\t\t\t\titer->second[2 * O] /= (numm)matchInSample;\n",
        "\t\t\t\tfor (ind o = 0; o < O; ++o) {\n",
        "\t\t\t\t\titer->second[2 * o] /= (numm)matchInSample;\n",
        "\t\t\t\t\titer->second[2 * o + 1] /= (numm)matchInSample;\n",
        "\t\t\t\t}\n",
        "\t\t\t\tnumm maxGain = 0;\n",
        "\t\t\t\tfor (ind o = 0; o < O; ++o) {\n",
        "\t\t\t\t\tnumm trueRate = iter->second[2 * o] / (numm)iter->second[2 * O];\n",
        "\t\t\t\t\tnumm expectedRate = iter->second[2 * o + 1] / (numm)iter->second[2 * O];\n",
        "\t\t\t\t\tif ((trueRate != 0) && (expectedRate != 0)) {\n",
        "\t\t\t\t\t\tnumm gain = iter->second[2 * O] * trueRate *log(trueRate / expectedRate);\n",
        "\t\t\t\t\t\tfor (ind o2 = 0; o2 < O; ++o2) {\n",
        "\t\t\t\t\t\t\tif (o2 != o) {\n",
        "\t\t\t\t\t\t\t\tnumm tr = iter->second[2 * o2] / (numm)iter->second[2 * O];\n",
        "\t\t\t\t\t\t\t\tnumm oldER = iter->second[2 * o2 + 1] / (numm)iter->second[2 * O];\n",
        "\t\t\t\t\t\t\t\tnumm newER = (oldER / (1.0 - expectedRate)) * (1.0 - trueRate);\n",
        "\t\t\t\t\t\t\t\tif ((tr != 0) && (oldER != 0) && (newER != 0)) {\n",
        "\t\t\t\t\t\t\t\t\tgain += iter->second[2 * O] * tr * (log(tr / oldER) - log(tr / newER));\n",
        "\t\t\t\t\t\t\t\t}\n",
        "\t\t\t\t\t\t\t}\n",
        "\t\t\t\t\t\t}\n",
        "\t\t\t\t\t\tmaxGain = max(maxGain, gain);\n",
        "\t\t\t\t\t}\n",
        "\t\t\t\t}\n",
        "\t\t\t\tif (maxGain > minGain) {\n",
        "\t\t\t\t\ttopTemplates.emplace_back(maxGain, allocatedTuple);\n",
        "\t\t\t\t\tif (topTemplates.size() > FLCandidates) {\n",
        "\t\t\t\t\t\tsort(topTemplates.rbegin(), topTemplates.rend());\n",
        "\t\t\t\t\t\ttopTemplates.resize(FLCandidates);\n",
        "\t\t\t\t\t\tminGain = topTemplates[FLCandidates - 1].first;\n",
        "\t\t\t\t\t}\n",
        "\t\t\t\t}\n",
        "\t\t\t}\n",
        "\t\t}\n",
        "\t\tvector<ind> bestPattern(2 * D + 3, 0);\n",
        "\t\tnumm bestGain = 0;\n",
        "\t\tfor (auto iter = topTemplates.begin(); iter != topTemplates.end(); ++iter) {\n",
        "\t\t\t/////// BuildAdaptedCube -- Note that the cube is build compactely and efficiently, i.e., aggregation over non-constant attributes and the ranges occurs with the first pass of the data.\n",
        "\t\t\t// Find Thresholds\n",
        "\t\t\tvector<ind> dimIndices; // which dimensions is this cube build with\n",
        "\t\t\tvector<vector<ind>> dimThresholds; // thresholds for cumulative sum aggregration per dimenion\n",
        "\t\t\tvector<ind> realcenters;  // list of values per attribute that are the center of expansion\n",
        "\t\t\tvector<ind> transformedCenters; // as centers but as index of the aggregated buckets of the cumulative cube\n",
        "\t\t\t\n",
        "\t\t\tfor (ind d = 0; d < D; ++d) {\n",
        "\t\t\t\tif (iter->second[d] == iter->second[D + d]) {\n",
        "\t\t\t\t\tind center = iter->second[d];\n",
        "\t\t\t\t\tvector<ind> localThresholds = {center};\n",
        "\t\t\t\t\tind smaller = center;\n",
        "\t\t\t\t\tfor (ind steps = 1; smaller > 0; steps *= 2) {\n",
        "\t\t\t\t\t\t(smaller > steps) ? (smaller = smaller - steps) : (smaller = 0);\n",
        "\t\t\t\t\t\tlocalThresholds.push_back(smaller);\n",
        "\t\t\t\t\t}\n",
        "\t\t\t\t\tind larger = center;\n",
        "\t\t\t\t\tfor (ind steps = 1; larger < keyToAttribute[d].size() - 1; steps *= 2) {\n",
        "\t\t\t\t\t\t// MY STUFF???\n",
        "\t\t\t\t\t\tlarger = min(larger + steps, (int)keyToAttribute[d].size() - 1);\n",
        "\t\t\t\t\t\t// larger = min(larger + steps, keyToAttribute[d].size() - 1);\n",
        "\t\t\t\t\t\tlocalThresholds.push_back(larger);\n",
        "\t\t\t\t\t}\n",
        "\t\t\t\t\tsort(localThresholds.begin(), localThresholds.end());\n",
        "\t\t\t\t\tdimIndices.push_back(d);\n",
        "\t\t\t\t\tdimThresholds.push_back(localThresholds);\n",
        "\n",
        "\t\t\t\t\trealcenters.push_back(center);\n",
        "\t\t\t\t\ttransformedCenters.push_back(distance(localThresholds.begin(), find(localThresholds.begin(), localThresholds.end(), center)));\n",
        "\t\t\t\t}\n",
        "\t\t\t}\n",
        "\t\t\tind patternDim = dimIndices.size();\n",
        "\t\t\tvector<ind> localBestPattern;\n",
        "\t\t\t// Build Cube\n",
        "\t\t\t// First pass on the data and consider which cell of the cumulative cube the tuples fall into, then accumulate values across dimensions\n",
        "\t\t\t// For efficiency, the high-dimensional cube (with variable dimension count) is built as one-dimensional vector using just multiplicative indexing\n",
        "\t\t\tind cubeSize = 1;\n",
        "\t\t\tfor (ind d = 0; d < patternDim; ++d) {\n",
        "\t\t\t\tcubeSize *= dimThresholds[d].size();\n",
        "\t\t\t}\n",
        "\t\t\tauto cumulativeCubeTrue = vector<vector<ind>>(O, vector<ind>(cubeSize, 0));\n",
        "\t\t\tauto cumulativeCubeModel = vector<vector<numm>>(O, vector<numm>(cubeSize, 0));\n",
        "\t\t\tauto cumulativeCubeSupport = vector<ind>(cubeSize, 0);\n",
        "\t\t\tfor (auto iter = tuples.begin(); iter != tuples.end(); ++iter) {\n",
        "\t\t\t\tind cubeID = 0;\n",
        "\t\t\t\tind multiplier = 1;\n",
        "\t\t\t\tfor (ind d = 0; d < patternDim; ++d) {\n",
        "\t\t\t\t\tind value = iter->first[dimIndices[d]];\n",
        "\t\t\t\t\tind bucket = 0;\n",
        "\t\t\t\t\tif (value < realcenters[d]) {\n",
        "\t\t\t\t\t\twhile (value >= dimThresholds[d][bucket+1]) {++bucket;}\n",
        "\t\t\t\t\t} else {\n",
        "\t\t\t\t\t\tbucket = transformedCenters[d];\n",
        "\t\t\t\t\t\twhile (value > dimThresholds[d][bucket]) { ++bucket; }\n",
        "\t\t\t\t\t}\n",
        "\t\t\t\t\tcubeID += multiplier*bucket;\n",
        "\t\t\t\t\tmultiplier *= dimThresholds[d].size();\n",
        "\t\t\t\t}\n",
        "\t\t\t\tcumulativeCubeSupport[cubeID] += iter->second[O];\n",
        "\t\t\t\tfor (ind o = 0; o < O; ++o) {\n",
        "\t\t\t\t\tcumulativeCubeModel[o][cubeID] += iter->second[O] * RCT.find(iter->second[O + 1])->second[o];\n",
        "\t\t\t\t\tcumulativeCubeTrue[o][cubeID] += iter->second[o];\n",
        "\t\t\t\t}\n",
        "\t\t\t}\n",
        "\t\t\tind stepwidth = 1;\n",
        "\t\t\tfor (ind d = 0; d < patternDim; ++d) {\n",
        "\t\t\t\tfor (ind i = 0; i + stepwidth < cumulativeCubeSupport.size(); ++i) {\n",
        "\t\t\t\t\tind target = i + stepwidth;\n",
        "\t\t\t\t\tif (target % (stepwidth * dimThresholds[d].size()) == 0) {\n",
        "\t\t\t\t\t\ti += (stepwidth - 1);\n",
        "\t\t\t\t\t} else {\n",
        "\t\t\t\t\t\tcumulativeCubeSupport[target] += cumulativeCubeSupport[i];\n",
        "\t\t\t\t\t\tfor (ind o = 0; o < O; ++o) {\n",
        "\t\t\t\t\t\t\tcumulativeCubeModel[o][target] += cumulativeCubeModel[o][i];\n",
        "\t\t\t\t\t\t\tcumulativeCubeTrue[o][target] += cumulativeCubeTrue[o][i];\n",
        "\t\t\t\t\t\t}\n",
        "\t\t\t\t\t}\n",
        "\t\t\t\t}\n",
        "\t\t\t\tstepwidth *= dimThresholds[d].size();\n",
        "\t\t\t}\n",
        "\t\t\t// Greedy BFS Exploration of patterns\n",
        "\t\t\t// Note: patterns contain only those dimensions that are present in the cube (i.e. constants in the original, simple pattern)\n",
        "\t\t\tvector<ind> protoPattern(2*patternDim);\n",
        "\t\t\tfor (ind d = 0; d < patternDim; ++d) {\n",
        "\t\t\t\tprotoPattern[d] = transformedCenters[d];\n",
        "\t\t\t\tprotoPattern[patternDim+d] = transformedCenters[d];\n",
        "\t\t\t}\n",
        "\t\t\tunordered_map<vector<ind>, numm, ind_vector_hasher> expandablePatterns;\n",
        "\t\t\texpandablePatterns.emplace(protoPattern, 0);\n",
        "\t\t\twhile (expandablePatterns.size() > 0) {\n",
        "\t\t\t\tunordered_map<vector<ind>, numm, ind_vector_hasher> nextExpandablePatterns;\n",
        "\t\t\t\tfor (auto iter = expandablePatterns.begin(); iter != expandablePatterns.end(); ++iter) {\n",
        "\t\t\t\t\tnumm currGain = 0;\n",
        "\t\t\t\t\tvector<ind> currPattern = iter->first;\n",
        "\t\t\t\t\t//evaluate gain with cumulative cubes\n",
        "\t\t\t\t\tind support = 0, supportN = 0;\n",
        "\t\t\t\t\tvector<ind> trueCount(O, 0), trueCountN(O, 0);\n",
        "\t\t\t\t\tvector<numm> modelCount(O, 0), modelCountN(O, 0);\n",
        "\t\t\t\t\tfor (ind i = 0; i < pow(2, patternDim); ++i) {\n",
        "\t\t\t\t\t\tind cubeID = 0;\n",
        "\t\t\t\t\t\tind multiplier = 1;\n",
        "\t\t\t\t\t\tind index = 0;\n",
        "\t\t\t\t\t\tbool valid = true;\n",
        "\t\t\t\t\t\tfor (ind d = 0; d < patternDim; ++d) {\n",
        "\t\t\t\t\t\t\tif ((i >> d & 1) && (currPattern[d] == 0)) { valid = false; }\n",
        "\t\t\t\t\t\t\telse { ((i >> d & 1) ? (index += multiplier*(currPattern[d] - 1)) : (index += multiplier * currPattern[patternDim + d])); }\n",
        "\t\t\t\t\t\t\tmultiplier *= dimThresholds[d].size();\n",
        "\t\t\t\t\t\t}\n",
        "\t\t\t\t\t\tif (valid) {\n",
        "\t\t\t\t\t\t\t// MY STUFF\n",
        "\t\t\t\t\t\t\tif ((_popcnt64(i) % 2) == 0) {   // __popcnt in Windows, __popcnt64 in 64-bit VC++\n",
        "\t\t\t\t\t\t\t\tsupport += cumulativeCubeSupport[index];\n",
        "\t\t\t\t\t\t\t\tfor (ind o = 0; o < O; ++o) {\n",
        "\t\t\t\t\t\t\t\t\ttrueCount[o] += cumulativeCubeTrue[o][index];\n",
        "\t\t\t\t\t\t\t\t\tmodelCount[o] += cumulativeCubeModel[o][index];\n",
        "\t\t\t\t\t\t\t\t}\n",
        "\t\t\t\t\t\t\t}\n",
        "\t\t\t\t\t\t\telse {\n",
        "\t\t\t\t\t\t\t\tsupportN += cumulativeCubeSupport[index];\n",
        "\t\t\t\t\t\t\t\tfor (ind o = 0; o < O; ++o) {\n",
        "\t\t\t\t\t\t\t\t\ttrueCountN[o] += cumulativeCubeTrue[o][index];\n",
        "\t\t\t\t\t\t\t\t\tmodelCountN[o] += cumulativeCubeModel[o][index];\n",
        "\t\t\t\t\t\t\t\t}\n",
        "\t\t\t\t\t\t\t}\n",
        "\t\t\t\t\t\t}\n",
        "\t\t\t\t\t}\n",
        "\t\t\t\t\tnumm supportF = (support - supportN);\n",
        "\t\t\t\t\tfor (ind o = 0; o < O; ++o) {\n",
        "\t\t\t\t\t\tnumm trueRate = (numm)(trueCount[o] - trueCountN[o]) / supportF;\n",
        "\t\t\t\t\t\tnumm expRate = (numm)(modelCount[o] - modelCountN[o]) / supportF;\n",
        "\t\t\t\t\t\tif ((trueRate != 0) && (expRate != 0)) {\n",
        "\t\t\t\t\t\t\tnumm gain = supportF * trueRate *log(trueRate / expRate);\n",
        "\t\t\t\t\t\t\tfor (ind o2 = 0; o2 < O; ++o2) {\n",
        "\t\t\t\t\t\t\t\tif (o2 != o) {\n",
        "\t\t\t\t\t\t\t\t\tnumm tr = (numm)(trueCount[o2] - trueCountN[o2]) / supportF;\n",
        "\t\t\t\t\t\t\t\t\tnumm oldER = (numm)(modelCount[o2] - modelCountN[o2]) / supportF;\n",
        "\t\t\t\t\t\t\t\t\tnumm newER = (oldER / (1.0 - expRate)) * (1.0 - trueRate);\n",
        "\t\t\t\t\t\t\t\t\tif ((tr != 0) && (oldER != 0) && (newER != 0)) {\n",
        "\t\t\t\t\t\t\t\t\t\tgain += supportF * tr * (log(tr / oldER) - log(tr / newER));\n",
        "\t\t\t\t\t\t\t\t\t}\n",
        "\t\t\t\t\t\t\t\t}\n",
        "\t\t\t\t\t\t\t}\n",
        "\t\t\t\t\t\t\tcurrGain = max(currGain, gain);\n",
        "\t\t\t\t\t\t\tif (gain > bestGain) {\n",
        "\t\t\t\t\t\t\t\tbestGain = gain;\n",
        "\t\t\t\t\t\t\t\tlocalBestPattern = currPattern;\n",
        "\t\t\t\t\t\t\t\tlocalBestPattern.push_back(o);\n",
        "\t\t\t\t\t\t\t\tlocalBestPattern.push_back(support - supportN);\n",
        "\t\t\t\t\t\t\t\tlocalBestPattern.push_back(trueCount[o] - trueCountN[o]);\n",
        "\t\t\t\t\t\t\t}\n",
        "\t\t\t\t\t\t}\n",
        "\t\t\t\t\t}\n",
        "\t\t\t\t\tif (currGain > iter->second) {\n",
        "\t\t\t\t\t\tfor (ind d = 0; d < patternDim; ++d) {\n",
        "\t\t\t\t\t\t\tif (currPattern[d] > 0) {\n",
        "\t\t\t\t\t\t\t\tvector<ind> nextPattern = currPattern;\n",
        "\t\t\t\t\t\t\t\tnextPattern[d] -= 1;\n",
        "\t\t\t\t\t\t\t\tauto ref = nextExpandablePatterns.emplace(nextPattern, currGain);\n",
        "\t\t\t\t\t\t\t}\n",
        "\t\t\t\t\t\t\tif (currPattern[patternDim+d] < dimThresholds[d].size()-1) {\n",
        "\t\t\t\t\t\t\t\tvector<ind> nextPattern = currPattern;\n",
        "\t\t\t\t\t\t\t\tnextPattern[patternDim +d] += 1;\n",
        "\t\t\t\t\t\t\t\tauto ref = nextExpandablePatterns.emplace(nextPattern, currGain);\n",
        "\t\t\t\t\t\t\t}\n",
        "\t\t\t\t\t\t}\n",
        "\t\t\t\t\t}\n",
        "\t\t\t\t}\n",
        "\t\t\t\texpandablePatterns = nextExpandablePatterns;\n",
        "\t\t\t}\n",
        "\t\t\t// construct best pattern from the small pattern, i.e. fill range of remaining attributes with wildcards\n",
        "\t\t\tif (localBestPattern.size() > 0) {\n",
        "\t\t\t\tfor (ind d = 0; d < D; ++d) {\n",
        "\t\t\t\t\tbestPattern[d] = 0;\n",
        "\t\t\t\t\tbestPattern[D + d] = keyToAttribute[d].size() - 1;\n",
        "\t\t\t\t}\n",
        "\t\t\t\tfor (ind d = 0; d < patternDim; ++d) {\n",
        "\t\t\t\t\tbestPattern[dimIndices[d]] = dimThresholds[d][localBestPattern[d]];\n",
        "\t\t\t\t\tbestPattern[D + dimIndices[d]] = dimThresholds[d][localBestPattern[patternDim + d]];\n",
        "\t\t\t\t}\n",
        "\t\t\t\tfor (ind extras = 0; extras < 3; ++extras) { bestPattern[2*D+extras] =  localBestPattern[2*patternDim+extras]; }\n",
        "\t\t\t}\n",
        "\t\t}\n",
        "\t\t// insert best pattern into the explanation table\n",
        "\t\taddRule(bestPattern);\n",
        "\t\titerativeScaling();\n",
        "\t\tauto end = chrono::high_resolution_clock::now();\n",
        "\t\tif (verbose) {\n",
        "\t\t\tnumm currentKL = KLDivergence();\n",
        "\t\t\tcout << \"FL+OptGrow \" << samplesize << \";\" <<FLCandidates<<\",\" << rules + 1 << \",\" << chrono::duration_cast<chrono::milliseconds>(end - start).count() << \",\" << chrono::duration_cast<chrono::milliseconds>(end - globalStart).count() << \",\";\n",
        "\t\t\tcout << recentKL - currentKL << \",\" << baseKL - currentKL << \",\" << currentKL << \"\\n\";\n",
        "\t\t\trecentKL = currentKL;\n",
        "\t\t}\n",
        "\t}\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Ks_WJ547MsL",
        "outputId": "987d61b6-8c26-4b4d-83d1-12adcf09e7be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing Lighthouse.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile Explanations.cpp\n",
        "\n",
        "#include \"Includes.h\"\n",
        "#include \"StaticFunctions.h\"\n",
        "#include \"Lighthouse.h\"\n",
        "\n",
        "int main(int argc, char *argv[])\n",
        "{\n",
        "    if (argc != 6) {\n",
        "\t\tcout << \"Not enough arguments: \" << argc << \"\\n\";\n",
        "        exit(-1);\n",
        "\t}\n",
        "    \n",
        "    string datasetName = argv[1];\n",
        "    string datasetFile = argv[2];\n",
        "    int numFeatures = atoi(argv[3]);\n",
        "\tint numPatterns = atoi(argv[4]);\n",
        "\tbool removeInactivatedPatterns = (atoi(argv[5]) == 1) ? true : false;\n",
        "\t// string patternsFile = argv[6];\n",
        "\t// numm minSupport = atof(argv[7]);\n",
        " \n",
        "    cout << datasetName << \" - \" << datasetFile << \" - \" << numFeatures << \" - \" << numPatterns << \"\\n\"; \n",
        "\n",
        "\tvector<tuple<string, string, vector<ind>, ind>> DataSets; //Name, Filepath, inputcolumns, outputcolumn\n",
        "\tvector<ind> attributeIndices;\n",
        "    for (int i = 0; i < numFeatures; i++) {\n",
        "        attributeIndices.push_back(i);\n",
        "    }\n",
        " \n",
        "    DataSets.emplace_back(datasetName, datasetFile, attributeIndices, numFeatures);\n",
        "\n",
        "\tconst ind maxRules = numPatterns; // maximum explanation table size\n",
        "\tvector<ind> tablesizes = {maxRules}; // explanation table sizes at which to evaluate time and gain\n",
        "\tvector<numm> minSupportParams = {0.01, 0.03, 0.05, 0.1, 0.15, 0.2};\n",
        "\n",
        "\tconst bool internalPrint = true; // some methods offer internal prints\n",
        "\tconst bool externalPrint = true;  // log results as blackbox evaluation\n",
        "\tconst bool printTables = true;\t  // print final explanation table to console\n",
        "\n",
        "\tconst bool TOGGLE_FL = true;   // run Flashlight on Data\n",
        "\tconst bool TOGGLE_LENS = false; // run LENS on Data\n",
        "\n",
        "\tstring logfile = \"./logs/log.csv\"; // file to save gain/time results\n",
        "\tofstream ofile;\n",
        "\tofile.open(logfile, ofstream::app);\n",
        "\t\n",
        "\tofile << \"Data, Method, TableSize, Time Total, Gain Total, Remaining Divergence,Parameters...\\n\" << flush;\n",
        "\tfor (ind repetitions = 0; repetitions < 1; ++repetitions) \n",
        "\t{ \n",
        "\t\t// enable experiment repetition\n",
        "\t\tfor (ind i = 0; i < DataSets.size(); ++i)\n",
        "\t\t{ \n",
        "\t\t\t// itrerate datasets and parse information\n",
        "\t\t\tstring dataName = get<0>(DataSets[i]);\n",
        "\t\t\tcout << \"=======\" << dataName << \"========\\n\";\n",
        "\t\t\tstring file = get<1>(DataSets[i]);\n",
        "\t\t\tvector<ind> dataColumns = get<2>(DataSets[i]);\n",
        "\t\t\tind outcome = get<3>(DataSets[i]);\n",
        "\n",
        "\t\t\t// Obtain Reference KL-Divergence\n",
        "\t\t\tLighthouse ref(file, dataColumns, outcome);\n",
        "\t\t\tnumm baseKL = ref.KLDivergence();\n",
        "\n",
        "\t\t\t//== ClassicFL == Parameters: Samplesize//\n",
        "\t\t\tif (TOGGLE_FL)\n",
        "\t\t\t{\n",
        "\t\t\t\tvector<ind> configsFL = {16}; // Enable easy way to test multiple settings for sample size\n",
        "\t\t\t\tfor (ind s : configsFL)\n",
        "\t\t\t\t{\n",
        "\t\t\t\t\tfor (numm minSupport : minSupportParams) {\n",
        "\t\t\t\t\t\t\n",
        "\t\t\t\t\t\tLighthouse A(file, dataColumns, outcome);\n",
        "\t\t\t\t\t\tcout << \"== FL Classic -- \" << s << \" -- \" << minSupport << \" ==\\n\";\n",
        "\t\t\t\t\t\tauto start = chrono::high_resolution_clock::now();\n",
        "\t\t\t\t\t\tfor (ind r = 1; r <= maxRules; ++r)\n",
        "\t\t\t\t\t\t{\n",
        "\t\t\t\t\t\t\tbool shouldContinue = A.originalFlashlight(1, s, minSupport, removeInactivatedPatterns, internalPrint);\n",
        "\t\t\t\t\t\t\tif (!shouldContinue) {\n",
        "\t\t\t\t\t\t\t\tbreak;\n",
        "\t\t\t\t\t\t\t}\n",
        "\t\t\t\t\t\t\tif (externalPrint && (find(tablesizes.begin(), tablesizes.end(), r) != tablesizes.end()))\n",
        "\t\t\t\t\t\t\t{ \n",
        "\t\t\t\t\t\t\t\t// test whether current table size should be evaluated\n",
        "\t\t\t\t\t\t\t\tauto now = chrono::high_resolution_clock::now();\n",
        "\t\t\t\t\t\t\t\tnumm KL = A.KLDivergence();\n",
        "\t\t\t\t\t\t\t\tofile << dataName << \",Flashlight,\" << r << \",\" << chrono::duration_cast<chrono::milliseconds>(now - start).count() << \",\" << baseKL - KL << \",\" << KL << \",\";\n",
        "\t\t\t\t\t\t\t\tofile << s << \"\\n\"\n",
        "\t\t\t\t\t\t\t\t\t<< flush;\n",
        "\n",
        "\t\t\t\t\t\t\t\tcout << \"Patterns: \" << r << \", Base KL: \" << baseKL << \", KL: \" << KL << \", Gain: \" << baseKL - KL << endl;\n",
        "\t\t\t\t\t\t\t}\n",
        "\t\t\t\t\t\t}\n",
        "\t\t\t\t\t\tif (printTables)\n",
        "\t\t\t\t\t\t{\n",
        "\t\t\t\t\t\t\tint precision = 2;\n",
        "\t\t\t\t\t\t\tif (std::fmod(minSupport, 0.1) == 0)\n",
        "\t   \t\t\t\t\t\t\tprecision = 1;\n",
        "\t\t\t\t\t\t\tstd::stringstream stream;\n",
        "\t\t\t\t\t\t\tstream << std::fixed << std::setprecision(precision) << minSupport;\n",
        "\t\t\t\t\t\t\tstd::string minSupportStr = stream.str();\n",
        "\t\t\t\t\t\t\tstring patternsFile = \"exp_patterns_\" + minSupportStr + \".csv\";\n",
        "\t\t\t\t\t\t\tA.printTable(baseKL, patternsFile);\n",
        "\t\t\t\t\t\t}\n",
        "\t\t\t\t\t}\n",
        "\t\t\t\t}\n",
        "\t\t\t}\n",
        "\n",
        "\t\t\t//== BottomUp with Cube == Parameters: Samplesize, Candidates//\n",
        "\t\t\tif (TOGGLE_LENS)\n",
        "\t\t\t{\n",
        "\t\t\t\tvector<pair<ind, ind>> configsLENS; // Easy way to test multiple parameter settings for LENS\n",
        "\t\t\t\tconfigsLENS.emplace_back(4, 16);\n",
        "\t\t\t\tconfigsLENS.emplace_back(16, 32);\n",
        "\t\t\t\tfor (auto config : configsLENS)\n",
        "\t\t\t\t{\n",
        "\t\t\t\t\tind s = config.first;\n",
        "\t\t\t\t\tind k = config.second;\n",
        "\t\t\t\t\tLighthouse A(file, dataColumns, outcome);\n",
        "\t\t\t\t\tcout << \"== Bottom Up Cube -- \" << s << \";\" << k << \" ==\\n\";\n",
        "\t\t\t\t\tauto start = chrono::high_resolution_clock::now();\n",
        "\t\t\t\t\tfor (ind r = 1; r <= maxRules; ++r)\n",
        "\t\t\t\t\t{\n",
        "\t\t\t\t\t\tA.LENS(1, internalPrint, s, k, 0);\n",
        "\t\t\t\t\t\tif (externalPrint && (find(tablesizes.begin(), tablesizes.end(), r) != tablesizes.end()))\n",
        "\t\t\t\t\t\t{\n",
        "\t\t\t\t\t\t\tauto now = chrono::high_resolution_clock::now();\n",
        "\t\t\t\t\t\t\tnumm KL = A.KLDivergence();\n",
        "\t\t\t\t\t\t\tofile << dataName << \",LENS,\" << r << \",\" << chrono::duration_cast<chrono::milliseconds>(now - start).count() << \",\" << baseKL - KL << \",\" << KL << \",\";\n",
        "\t\t\t\t\t\t\tofile << s << \",\" << k << \"\\n\"\n",
        "\t\t\t\t\t\t\t\t  << flush;\n",
        "\t\t\t\t\t\t}\n",
        "\t\t\t\t\t}\n",
        "\t\t\t\t\tif (printTables)\n",
        "\t\t\t\t\t{\n",
        "\t\t\t\t\t\tstring patternsFile = \"exp_patterns.csv\";\n",
        "\t\t\t\t\t\tA.printTable(baseKL, patternsFile);\n",
        "\t\t\t\t\t}\n",
        "\t\t\t\t}\n",
        "\t\t\t}\n",
        "\t\t}\n",
        "\t\tofile.close();\n",
        "\t}\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44QseW_J8paK",
        "outputId": "34507d58-2db4-44e3-9caa-69de49f71473"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing Explanations.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def find_images_supporting_pattern (image_concepts, pattern):\n",
        "\n",
        "    df = image_concepts.copy()\n",
        "    for attr in list(pattern.index): \n",
        "        pattern_value = pattern[attr]\n",
        "        if (attr not in meta_cols) and (pattern_value != -1):\n",
        "            if (not binning_features) or (pattern_value % 1 == 0): \n",
        "                df = df[df[attr] == pattern_value]\n",
        "            else:\n",
        "                # Handling the case of 0.5 or 1.5 values for a pattern feature: \n",
        "                a = math.floor(pattern_value)\n",
        "                b = math.ceil(pattern_value)\n",
        "                print('attr {} with value {}, floor {}, and ceil {}'.format(attr, pattern_value, a, b))\n",
        "                df = df[(df[attr] == a) | (df[attr] == b)]\n",
        "\n",
        "    supporting_indices = list(df.index.values)\n",
        "    return supporting_indices"
      ],
      "metadata": {
        "id": "3NKwipCJF6nI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_images_matching_pattern (image_concepts, pattern, supporting_indices=None): \n",
        "\n",
        "    if supporting_indices is None:\n",
        "        supporting_indices = find_images_supporting_pattern(image_concepts, pattern)\n",
        "    pattern_label = pattern['pred']\n",
        "\n",
        "    matching_indices = []\n",
        "    supporting_labels = list(image_concepts.iloc[supporting_indices]['pred'])\n",
        "\n",
        "    for i,label in enumerate(supporting_labels):\n",
        "        if label == pattern_label:\n",
        "            matching_indices.append(supporting_indices[i])\n",
        "\n",
        "    return matching_indices"
      ],
      "metadata": {
        "id": "esz-c7YwF73o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_class_titles (ds_name):\n",
        "    ctitles = {}\n",
        "    name_parts = ds_name.split('_')\n",
        "    if len(name_parts) <= 1:\n",
        "        return ctitles\n",
        "\n",
        "    n_classes = len(name_parts[1:])\n",
        "    for i,p in enumerate(name_parts[1:]):\n",
        "        ctitles[i] = p\n",
        "        if binning_classes:\n",
        "            ctitles[i + n_classes] = 'maybe ' + p\n",
        "\n",
        "    return ctitles"
      ],
      "metadata": {
        "id": "wJg4yOT5Xlwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_pattern_accuracies (image_concepts, patterns_file):\n",
        "\n",
        "    exp_patterns = pd.read_csv(patterns_file)\n",
        "    exp_patterns['accuracy'] = 0.0\n",
        "\n",
        "    for i,pattern in exp_patterns.iterrows():\n",
        "        pred = pattern['pred']\n",
        "        conf = pattern['confidence']\n",
        "\n",
        "        # Handling those patterns with lower than 0.5 confidence, which need to inverted to be useful (code below only works in case of not binning):\n",
        "        if conf < 0.5:\n",
        "            new_pred = pred\n",
        "            new_conf = conf\n",
        "            classes = list(class_titles.keys())\n",
        "            if len(classes) == 2:\n",
        "                new_pred = 1 if pred == 0 else 0\n",
        "                new_conf = 1.0 - conf\n",
        "            else:\n",
        "                pattern_cp = pattern.copy(deep=True)\n",
        "                for c in classes:\n",
        "                    if pred == c:\n",
        "                        continue\n",
        "                    pattern_cp['pred'] = c\n",
        "                    supporting_indices = find_images_supporting_pattern(image_concepts, pattern_cp)\n",
        "                    matching_indices = find_images_matching_pattern(image_concepts, pattern_cp, supporting_indices)\n",
        "                    temp_conf = len(matching_indices) / len(supporting_indices)\n",
        "                    if temp_conf > new_conf:\n",
        "                        new_pred = c\n",
        "                        new_conf = temp_conf\n",
        "            \n",
        "            exp_patterns.loc[i, 'pred'] = new_pred\n",
        "            exp_patterns.loc[i, 'confidence'] = new_conf\n",
        "            pattern['pred'] = new_pred\n",
        "            pattern['confidence'] = new_conf\n",
        "            print('Exp pattern with pred {} and conf {} changed to new pred {} and new conf {}'.format(pred, conf, new_pred, new_conf))\n",
        "\n",
        "        pattern_label = pattern['pred']\n",
        "        supporting_indices = find_images_supporting_pattern(image_concepts, pattern)\n",
        "        matching_indices = find_images_matching_pattern(image_concepts, pattern, supporting_indices)\n",
        "        matching_labels = list(image_concepts.iloc[matching_indices]['label'])\n",
        "\n",
        "        accurate_indices = []\n",
        "        for j,label in enumerate(matching_labels):\n",
        "            if label == pattern_label:\n",
        "                accurate_indices.append(matching_indices[j])\n",
        "\n",
        "        conf = len(matching_indices)\n",
        "        acc = len(accurate_indices) / conf\n",
        "        exp_patterns.loc[i, 'accuracy'] = acc\n",
        "\n",
        "    exp_patterns.to_csv(patterns_file, index=False)"
      ],
      "metadata": {
        "id": "Y4ZMdiHGEMJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import math\n",
        "\n",
        "current_setting_path = '/content/drive/My Drive/Python Projects/POEM Pipeline Results/current_setting.txt'\n",
        "with open(current_setting_path, 'r') as f:\n",
        "    current_setting_title = f.read().splitlines()[0]\n",
        "    print('Current setting:', current_setting_title)\n",
        "\n",
        "title_parts = current_setting_title.split('_')\n",
        "model_name = title_parts[0]\n",
        "dataset_name = '_'.join(title_parts[1:]) \n",
        "\n",
        "old_process = False\n",
        "binning_classes = False\n",
        "binning_features = False\n",
        "remove_inactivated_patterns = False\n",
        "class_titles = extract_class_titles(dataset_name)\n",
        "\n",
        "drive_result_path = '/content/drive/My Drive/Python Projects/POEM Pipeline Results/' + model_name + '_' + dataset_name\n",
        "if old_process:\n",
        "    drive_result_path += '_old'\n",
        "concepts_file = 'image_concepts.csv'\n",
        "#patterns_file = 'exp_patterns.csv'\n",
        "concepts_path = drive_result_path + \"/\" + concepts_file\n",
        "!cp \"$concepts_path\" '.'\n",
        "\n",
        "image_concepts = pd.read_csv(concepts_file)\n",
        "meta_cols = ['pred', 'support', 'confidence', 'accuracy']\n",
        "concepts_meta_cols = ['pred', 'label', 'id', 'file', 'path']\n",
        "concept_cols = list(set(image_concepts.columns) - set(concepts_meta_cols))\n",
        "num_concepts = len(concept_cols)\n",
        "num_patterns = 30\n",
        "#min_support = 0.01\n",
        "min_support_params = [0.01, 0.03, 0.05, 0.1, 0.15, 0.2]   # should be also copied to the Exp Tables code; used for computing the pattern file names below\n",
        "remove_inactivated_patterns_num = 1 if remove_inactivated_patterns else 0\n",
        "print('Arguments to the program: {} {} {} {} {}'.format(dataset_name, concepts_file, num_concepts, num_patterns, remove_inactivated_patterns_num))"
      ],
      "metadata": {
        "id": "ZGyr27kiF1zn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash -s \"$dataset_name\" \"$concepts_file\" \"$num_concepts\" \"$num_patterns\" \"$remove_inactivated_patterns_num\"\n",
        "\n",
        "g++ Explanations.cpp Lighthouse.cpp -o program\n",
        "./program $1 $2 $3 $4 $5"
      ],
      "metadata": {
        "id": "rBC4cuRl9JY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#compute_pattern_accuracies(image_concepts, patterns_file)\n",
        "#!cp $patterns_file \"$drive_result_path\"\n",
        "\n",
        "for min_support in min_support_params:\n",
        "    patterns_file = 'exp_patterns_' + str(min_support) + '.csv'\n",
        "    compute_pattern_accuracies(image_concepts, patterns_file)\n",
        "    !cp $patterns_file \"$drive_result_path\""
      ],
      "metadata": {
        "id": "1WVz9xq5XTi4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}