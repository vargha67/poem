{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "S9DFS5qE92CN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "# From PyTorch 1.11, the THC namespace is removed which prevents the PreciseRoIPooling library used in segmentation model to be built successfully\n",
        "pip install torch==1.10.1+cu102 torchvision==0.11.2+cu102 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "!(stat -t /usr/local/lib/*/dist-packages/google/colab > /dev/null 2>&1) && exit \n",
        "pip install ninja 2>> install.log\n",
        "git clone https://github.com/davidbau/dissect.git dissect 2>> install.log\n",
        "#pip list -v >> identification_packages.log"
      ],
      "metadata": {
        "id": "Mju0lk9jPIR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try: # set up path\n",
        "    import google.colab, sys, torch\n",
        "    sys.path.append('/content/dissect')\n",
        "    if not torch.cuda.is_available():\n",
        "        print(\"Change runtime type to include a GPU.\")  \n",
        "except:\n",
        "    pass"
      ],
      "metadata": {
        "id": "LuADmoVLPUdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xn7cVfu1Fct5"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "import IPython\n",
        "from importlib import reload\n",
        "\n",
        "mpl.rcParams['lines.linewidth'] = 0.25\n",
        "mpl.rcParams['axes.spines.top'] = False\n",
        "mpl.rcParams['axes.spines.right'] = False\n",
        "mpl.rcParams['axes.linewidth'] = 0.25"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, os, pickle, json, numpy, netdissect\n",
        "\n",
        "from collections import OrderedDict\n",
        "from torch import nn\n",
        "from torchvision import datasets, transforms, models\n",
        "from netdissect import pbar, nethook, renormalize, parallelfolder, upsample, tally, imgviz, imgsave, show\n",
        "from netdissect.easydict import EasyDict\n",
        "from experiment import dissect_experiment as experiment\n",
        "\n",
        "torch.backends.cudnn.benchmark = True"
      ],
      "metadata": {
        "id": "Sz5b3gN562N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXZe6A_UFct7"
      },
      "outputs": [],
      "source": [
        "settings_path = '/content/drive/My Drive/Python Projects/POEM Pipeline Results/settings.pkl'\n",
        "with open(settings_path, 'rb') as f:\n",
        "    settings = pickle.load(f)\n",
        "\n",
        "current_setting_path = '/content/drive/My Drive/Python Projects/POEM Pipeline Results/current_setting.txt'\n",
        "with open(current_setting_path, 'r') as f:\n",
        "    current_setting_title = f.read().splitlines()[0]\n",
        "    print('Current setting:', current_setting_title)\n",
        "\n",
        "model_settings = settings['model_settings']\n",
        "model_dataset_settings = settings['model_dataset_settings']\n",
        "\n",
        "title_parts = current_setting_title.split('_')\n",
        "model_name = title_parts[0]\n",
        "dataset_name = '_'.join(title_parts[1:]) \n",
        "dataset_pure_name = dataset_name.split('_')[0]\n",
        "\n",
        "current_setting = model_dataset_settings[current_setting_title] \n",
        "num_classes = current_setting['num_classes']\n",
        "use_dissection_models = False\n",
        "target_layer = model_settings[model_name]['target_layer'] if not use_dissection_models else experiment.instrumented_layername(EasyDict(model=model_name))\n",
        "\n",
        "exclude_similar_concepts = True   # It is better to exclude a concept which is very similar to the dataset classes; e.g. laptop or computer concepts in laptop vs mobile dataset\n",
        "excluded_concepts = current_setting['excluded_concepts'] if exclude_similar_concepts and ('excluded_concepts' in current_setting) else []\n",
        "\n",
        "seg_model_name = 'netpc'   # 'netpqc'\n",
        "image_size = 224\n",
        "\n",
        "min_iou = 0.04\n",
        "activation_high_thresh = 0.99\n",
        "activation_low_thresh = 0.7\n",
        "\n",
        "norm_mean = (0.485, 0.456, 0.406)\n",
        "norm_std = (0.229, 0.224, 0.225)\n",
        "\n",
        "args = EasyDict(model=model_name, dataset=dataset_name, seg=seg_model_name, layer=target_layer, quantile=activation_high_thresh)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_imported_packages(packages_path):\n",
        "\n",
        "    # Saving imported packages and their versions: \n",
        "    import sys\n",
        "    modules_info = []\n",
        "\n",
        "    for module in sys.modules:\n",
        "        if len(module.split('.')) > 1:   # ignoring subpackages\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            modules_info.append((module, sys.modules[module].__version__))\n",
        "        except:\n",
        "            try:\n",
        "                if type(sys.modules[module].version) is str:\n",
        "                    modules_info.append((module, sys.modules[module].version))\n",
        "                else:\n",
        "                    modules_info.append((module, sys.modules[module].version()))\n",
        "            except:\n",
        "                try:\n",
        "                    modules_info.append((module, sys.modules[module].VERSION))\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "    modules_info.sort(key=lambda x: x[0])\n",
        "    with open(packages_path, 'w') as f:\n",
        "        for m in modules_info:\n",
        "            f.write('{} {}\\n'.format(m[0], m[1]))"
      ],
      "metadata": {
        "id": "nF2kTNAAbwpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Method used previously to remove excluded concepts from the segmenter's list of concepts, but seems not to be effective in practice, \n",
        "# because a mismatch between the list of concepts and the output classes of the pretrained segmenter model occurs. \n",
        "def exclude_concepts_from_segmenter (seglabels_path):\n",
        "\n",
        "    f = open(seglabels_path)\n",
        "    labels_data = json.load(f)\n",
        "    f.close()\n",
        "    if 'object' not in labels_data:\n",
        "        return\n",
        "\n",
        "    new_objects = []\n",
        "    objects = labels_data['object']\n",
        "    for obj in objects:\n",
        "        if obj not in excluded_concepts:\n",
        "            new_objects.append(obj)\n",
        "\n",
        "    labels_data['object'] = new_objects\n",
        "\n",
        "    with open(seglabels_path, 'w') as f:\n",
        "        json.dump(labels_data, f)"
      ],
      "metadata": {
        "id": "YRxvRmsiY76i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_target_layer_name (final=False):\n",
        "\n",
        "    if final and use_dissection_models:\n",
        "        return experiment.instrumented_layername(args)\n",
        "    else:\n",
        "        if model_name is 'resnet18':\n",
        "            return 'layer4.1.conv2'\n",
        "        elif model_name is 'resnet50':\n",
        "            return 'layer4.2.conv3'\n",
        "        elif model_name is 'vgg16':\n",
        "            return 'features.conv5_3'\n",
        "        elif model_name is 'alexnet':\n",
        "            return 'conv5'\n",
        "\n",
        "    return None"
      ],
      "metadata": {
        "id": "YueNOWlVyn5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vgg16_model (*args, **kwargs):\n",
        "\n",
        "    # A version of vgg16 model where layers are given their research names: \n",
        "    model = models.vgg16(*args, **kwargs)\n",
        "    model.features = nn.Sequential(OrderedDict(zip([\n",
        "        'conv1_1', 'relu1_1',\n",
        "        'conv1_2', 'relu1_2',\n",
        "        'pool1',\n",
        "        'conv2_1', 'relu2_1',\n",
        "        'conv2_2', 'relu2_2',\n",
        "        'pool2',\n",
        "        'conv3_1', 'relu3_1',\n",
        "        'conv3_2', 'relu3_2',\n",
        "        'conv3_3', 'relu3_3',\n",
        "        'pool3',\n",
        "        'conv4_1', 'relu4_1',\n",
        "        'conv4_2', 'relu4_2',\n",
        "        'conv4_3', 'relu4_3',\n",
        "        'pool4',\n",
        "        'conv5_1', 'relu5_1',\n",
        "        'conv5_2', 'relu5_2',\n",
        "        'conv5_3', 'relu5_3',\n",
        "        'pool5'],\n",
        "        model.features)))\n",
        "\n",
        "    model.classifier = nn.Sequential(OrderedDict(zip([\n",
        "        'fc6', 'relu6',\n",
        "        'drop6',\n",
        "        'fc7', 'relu7',\n",
        "        'drop7',\n",
        "        'fc8a'],\n",
        "        model.classifier)))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "r-EcRVUvW0lb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model (model_file=None):\n",
        "\n",
        "    if use_dissection_models:\n",
        "        model = experiment.load_model(args)\n",
        "    else:\n",
        "        if model_name == 'vgg16':\n",
        "            model = vgg16_model(num_classes=num_classes)\n",
        "        else:\n",
        "            model = models.__dict__[model_name](num_classes=num_classes)\n",
        "        checkpoint = torch.load(model_file)\n",
        "        statedict = checkpoint\n",
        "        if 'state_dict' in checkpoint:\n",
        "            statedict = {str.replace(k,'module.',''): v for k,v in checkpoint['state_dict'].items()}\n",
        "        model.load_state_dict(statedict)\n",
        "        model = nethook.InstrumentedModel(model).cuda().eval()\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "l8GM10WR8wXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset (dataset_dir=None):\n",
        "    \n",
        "    if use_dissection_models:\n",
        "        return experiment.load_dataset(args)\n",
        "    \n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(image_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=norm_mean, std=norm_std)\n",
        "    ])\n",
        "\n",
        "    dataset = parallelfolder.ParallelImageFolders([dataset_dir], classification=True, shuffle=True, transform=transform)\n",
        "    print('Processing {} data examples from these classes: {}'.format(len(dataset), dataset.classes))\n",
        "\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "idayAuBc9Mg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_sample_images (model, dataset, sample_batch, batch_indices, classlabels):\n",
        "\n",
        "    truth = [classlabels[dataset[i][1]] for i in batch_indices]\n",
        "    preds = model(sample_batch.cuda()).max(1)[1]\n",
        "    imgs = [renormalize.as_image(t, source=dataset) for t in sample_batch]\n",
        "    prednames = [classlabels[p.item()] for p in preds]\n",
        "    show([[img, 'pred: ' + pred, 'true: ' + gt] for img, pred, gt in zip(imgs, prednames, truth)])"
      ],
      "metadata": {
        "id": "_OBNxWmD_LC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_sample_segmentations (segmodel, dataset, sample_batch, renorm):\n",
        "\n",
        "    iv = imgviz.ImageVisualizer(120, source=dataset)\n",
        "    seg = segmodel.segment_batch(renorm(sample_batch).cuda(), downsample=4)\n",
        "\n",
        "    torch.set_printoptions(profile=\"full\")\n",
        "    print('seg.shape:', seg.shape)\n",
        "    #print(seg[0])\n",
        "\n",
        "    show([(iv.image(sample_batch[i]), iv.segmentation(seg[i,0]),\n",
        "                iv.segment_key(seg[i,0], segmodel))\n",
        "                for i in range(len(seg))])"
      ],
      "metadata": {
        "id": "Sjjch52hE5wH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_sample_heatmaps (model, dataset, sample_batch):\n",
        "\n",
        "    acts = model.retained_layer(target_layer).cpu()\n",
        "    print('acts.shape:', acts.shape)\n",
        "    print('acts_reshaped.shape:', acts.view(acts.shape[0], acts.shape[1], -1).shape)\n",
        "\n",
        "    ivsmall = imgviz.ImageVisualizer((100, 100), source=dataset)\n",
        "    display(show.blocks(\n",
        "        [[[ivsmall.masked_image(sample_batch[0], acts, (0, u), percent_level=activation_high_thresh)],\n",
        "        [ivsmall.heatmap(acts, (0, u), mode='nearest')]] for u in range(min(acts.shape[1], 12))]\n",
        "    ))"
      ],
      "metadata": {
        "id": "FakN19z8HK8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_sample_image_activation (model, dataset, rq, topk, classlabels, sample_unit_number, sample_image_index):\n",
        "\n",
        "    print(topk.result()[1][sample_unit_number][sample_image_index], dataset.images[topk.result()[1][sample_unit_number][sample_image_index]])\n",
        "    image_number = topk.result()[1][sample_unit_number][sample_image_index].item()\n",
        "\n",
        "    iv = imgviz.ImageVisualizer((224, 224), source=dataset, quantiles=rq,\n",
        "            level=rq.quantiles(activation_high_thresh))\n",
        "    batch = torch.cat([dataset[i][0][None,...] for i in [image_number]])\n",
        "    truth = [classlabels[dataset[i][1]] for i in [image_number]]\n",
        "    preds = model(batch.cuda()).max(1)[1]\n",
        "    imgs = [renormalize.as_image(t, source=dataset) for t in batch]\n",
        "    prednames = [classlabels[p.item()] for p in preds]\n",
        "    acts = model.retained_layer(target_layer)\n",
        "    print('acts.shape:', acts.shape)\n",
        "    print('acts_reshaped.shape:', acts.view(acts.shape[0], acts.shape[1], -1).shape)\n",
        "    #print('acts_reshaped.max():', acts.view(acts.shape[0], acts.shape[1], -1).max(2)[0])\n",
        "    image_acts = acts[0,sample_unit_number].cpu().numpy()\n",
        "    unit_quant = rq.quantiles(activation_high_thresh)[sample_unit_number].item()\n",
        "    #print(unit_quant)\n",
        "    #print(image_acts)\n",
        "    print('number of activations higher than quantile {}: {}'.format(unit_quant, numpy.sum(image_acts > unit_quant)))\n",
        "\n",
        "    show([[img, 'pred: ' + pred, 'true: ' + gt] for img, pred, gt in zip(imgs, prednames, truth)])\n",
        "    show([[iv.masked_image(batch[0], acts, (0, sample_unit_number))]])\n",
        "    show([[iv.heatmap(acts, (0, sample_unit_number), mode='nearest')]])"
      ],
      "metadata": {
        "id": "M-QsDotVxRav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_top_channel_images (model, dataset, rq, topk):\n",
        "\n",
        "    pbar.descnext('unit_images')\n",
        "    iv = imgviz.ImageVisualizer((100, 100), source=dataset, quantiles=rq,\n",
        "            level=rq.quantiles(activation_high_thresh))\n",
        "    \n",
        "    def compute_acts(image_batch, label_batch):\n",
        "        image_batch = image_batch.cuda()\n",
        "        _ = model(image_batch)\n",
        "        acts_batch = model.retained_layer(target_layer)\n",
        "        return acts_batch\n",
        "\n",
        "    unit_images = iv.masked_images_for_topk(\n",
        "            compute_acts, dataset, topk, k=5, num_workers=2, pin_memory=True,  #num_workers=30\n",
        "            cachefile=resfile('top5images.npz'))\n",
        "\n",
        "    image_row_width = 5\n",
        "    pbar.descnext('saving images')\n",
        "    imgsave.save_image_set(unit_images, resfile('image/unit%d.jpg'),\n",
        "            sourcefile=resfile('top%dimages.npz' % image_row_width))\n",
        "    \n",
        "    return unit_images"
      ],
      "metadata": {
        "id": "PJ5BjoelO126"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_sample_channel_images (unit_images, sample_unit_numbers, unit_label_high=None):\n",
        "\n",
        "    for u in sample_unit_numbers:\n",
        "        if unit_label_high is None:\n",
        "            print('unit %d' % u)\n",
        "        else:\n",
        "            print('unit %d, label %s, iou %.3f' % (u, unit_label_high[u][1], unit_label_high[u][3]))\n",
        "        display(unit_images[u])"
      ],
      "metadata": {
        "id": "GaNWrF4DTJ0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Computes and keeps channel activations for all images in a way that any activation quantile for each channel can be computed easily\n",
        "def compute_tally_quantile (model, dataset, upfn, sample_size):\n",
        "    \n",
        "    pbar.descnext('rq')\n",
        "    def compute_samples(batch, *args):\n",
        "        image_batch = batch.cuda()\n",
        "        _ = model(image_batch)\n",
        "        acts = model.retained_layer(target_layer)\n",
        "        hacts = upfn(acts)\n",
        "        return hacts.permute(0, 2, 3, 1).contiguous().view(-1, acts.shape[1])\n",
        "\n",
        "    rq = tally.tally_quantile(compute_samples, dataset,\n",
        "                            sample_size=sample_size,\n",
        "                            r=8192,\n",
        "                            num_workers=2,  #100\n",
        "                            pin_memory=True,\n",
        "                            cachefile=resfile('rq.npz'))\n",
        "    return rq"
      ],
      "metadata": {
        "id": "T55c-EyCOqqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Computes and keeps maximum of channel activations for all images, so that the top k images with the highest maximum activation value can be identified for each channel\n",
        "def compute_tally_topk (model, dataset, sample_size):\n",
        "\n",
        "    pbar.descnext('topk')\n",
        "    def compute_image_max(batch, *args):\n",
        "        image_batch = batch.cuda()\n",
        "        _ = model(image_batch)\n",
        "        acts = model.retained_layer(target_layer)\n",
        "        acts = acts.view(acts.shape[0], acts.shape[1], -1)\n",
        "        acts = acts.max(2)[0]\n",
        "        return acts\n",
        "\n",
        "    topk = tally.tally_topk(compute_image_max, dataset, sample_size=sample_size,\n",
        "            batch_size=50, num_workers=2, pin_memory=True,  #num_workers=30\n",
        "            cachefile=resfile('topk.npz'))\n",
        "    return topk"
      ],
      "metadata": {
        "id": "g6mMtQM9O1f9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Computes the best concepts matching each channel based on IoUs between concept segmentations and channel activations\n",
        "def compute_top_channel_concepts (model, segmodel, upfn, dataset, rq, seglabels, segcatlabels, sample_size, renorm):\n",
        "\n",
        "    # \"level_high\" was formerly named \"level_at_99\"\n",
        "    # \"condi_high\" was formerly named \"condi99\"\n",
        "    # \"iou_high\" was formerly named \"iou_99\"\n",
        "    # \"unit_label_high\" was formerly named \"unit_label_99\"\n",
        "\n",
        "    # Getting the target quantile values of channels: \n",
        "    level_high = rq.quantiles(activation_high_thresh).cuda()[None,:,None,None]\n",
        "    level_low = rq.quantiles(activation_low_thresh).cuda()[None,:,None,None]\n",
        "    \n",
        "    # Computing the overlap between all the channel activations and all the image segmentations: \n",
        "    def compute_conditional_indicator(batch, *args):\n",
        "        image_batch = batch.cuda()\n",
        "        seg = segmodel.segment_batch(renorm(image_batch), downsample=4)\n",
        "        _ = model(image_batch)\n",
        "        acts = model.retained_layer(target_layer)\n",
        "        hacts = upfn(acts)\n",
        "        iacts = (hacts > level_high).float() # indicator\n",
        "        return tally.conditional_samples(iacts, seg)\n",
        "\n",
        "    pbar.descnext('condi_high')\n",
        "    condi_high = tally.tally_conditional_mean(compute_conditional_indicator,\n",
        "            dataset, sample_size=sample_size,\n",
        "            num_workers=10, pin_memory=True,  #num_workers=3\n",
        "            cachefile=resfile('condi_high.npz'))\n",
        "    \n",
        "    # Computing the IoU between each channel and all the concepts: \n",
        "    iou_high = tally.iou_from_conditional_indicator_mean(condi_high)\n",
        "\n",
        "    # Identifying the concept with max IoU for each channel: \n",
        "    # unit_label_high = [\n",
        "    #         (concept.item(), seglabels[concept], segcatlabels[concept], bestiou.item())\n",
        "    #         for (bestiou, concept) in zip(*iou_high.max(0))]\n",
        "\n",
        "    unit_label_high = []\n",
        "    for i,row in enumerate(iou_high.t()):\n",
        "        top_ious, top_concepts = row.topk(k=3)\n",
        "        top_list = [(con.item(), seglabels[con], segcatlabels[con], iou.item()) for con,iou in zip(top_concepts, top_ious)]\n",
        "        top_item = top_list[0]\n",
        "        top_label = top_item[1]\n",
        "\n",
        "        # Though not ideal, this is the best we can do to exclude concepts which are very similar to the dataset classes: \n",
        "        if (len(excluded_concepts) > 0) and (top_label in excluded_concepts):\n",
        "            print('Channel {} top concepts: {}'.format(i, top_list))\n",
        "            top_item = (0, '-', ('-','-'), 0.0)\n",
        "            for j in range(1,len(top_list)):\n",
        "                item = top_list[j]\n",
        "                label = item[1]\n",
        "                iou = item[3]\n",
        "                if label not in excluded_concepts:\n",
        "                    top_item = item\n",
        "                    break\n",
        "            print('Because top concept {} is among the excluded concepts, concept {} with iou {} is selected for channel {}'\n",
        "                .format(top_label, top_item[1], top_item[3], i))\n",
        "        unit_label_high.append(top_item)\n",
        "\n",
        "    label_list = [labelcat for concept, label, labelcat, iou in unit_label_high if iou > min_iou]\n",
        "\n",
        "    print(len(unit_label_high))\n",
        "    print(unit_label_high)\n",
        "    \n",
        "    return unit_label_high, label_list, level_high, level_low"
      ],
      "metadata": {
        "id": "gwGhWt7QXK9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_final_data (unit_label_high, label_list, level_high, level_low):\n",
        "\n",
        "    display(IPython.display.SVG(experiment.graph_conceptcatlist(label_list)))\n",
        "    experiment.save_conceptcat_graph(resfile('concepts_high.svg'), label_list)\n",
        "\n",
        "    print('level_high.shape:', level_high.shape)\n",
        "    print('level_low.shape:', level_low.shape)\n",
        "\n",
        "    high_quantiles = level_high.view(-1).cpu().numpy()\n",
        "    low_quantiles = level_low.view(-1).cpu().numpy()\n",
        "\n",
        "    print('high_quantiles.shape:', high_quantiles.shape)\n",
        "    print('low_quantiles.shape:', low_quantiles.shape)\n",
        "\n",
        "    experiment.dump_json_file(resfile('report.json'), dict(\n",
        "            header=dict(\n",
        "                name='%s %s %s' % (model_name, dataset_name, seg_model_name),\n",
        "                image='concepts_high.svg'),\n",
        "            units=[\n",
        "                dict(image='image/unit%d.jpg' % u,\n",
        "                    unit=u, iou=iou, label=label, cat=labelcat[1], high_thresh=float(high_quantiles[u]), low_thresh=float(low_quantiles[u]))\n",
        "                for u, (concept, label, labelcat, iou)\n",
        "                in enumerate(unit_label_high)])\n",
        "            )\n",
        "    \n",
        "    experiment.copy_static_file('report.html', resfile('report.html'))\n",
        "\n",
        "    # print('level_high.shape:', level_high.shape)\n",
        "    # quantiles = level_high.view(-1).cpu().numpy()\n",
        "    # print('quantiles.shape:', quantiles.shape)\n",
        "    numpy.save(resfile('channel_quantiles.npy'), high_quantiles)\n",
        "\n",
        "    print('Channel high quantiles:')\n",
        "    for i,q in enumerate(list(high_quantiles)):\n",
        "        print('{}: {}'.format(i,q))\n",
        "\n",
        "    print('Channel low quantiles:')\n",
        "    for i,q in enumerate(list(low_quantiles)):\n",
        "        print('{}: {}'.format(i,q))"
      ],
      "metadata": {
        "id": "jwsL4Yz6DJTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_file = 'model.pth'   # model_name + '_' + dataset_name + '.pth'\n",
        "dataset_dir = 'dataset'   # dataset_name\n",
        "result_dir = 'identification_results'\n",
        "drive_result_path = '/content/drive/My Drive/Python Projects/POEM Pipeline Results/' + model_name + '_' + dataset_name\n",
        "\n",
        "def resfile(f):\n",
        "    return os.path.join(result_dir, f)\n",
        "\n",
        "if not use_dissection_models:\n",
        "    dataset_file = dataset_dir + '.zip'\n",
        "    drive_dataset_dir = drive_result_path + '/' + dataset_file   # '/content/drive/My Drive/Python Projects/Other Data/' + dataset_file\n",
        "    !cp \"$drive_dataset_dir\" '.'\n",
        "    !unzip -qq -n $dataset_file -d '.'\n",
        "\n",
        "    drive_model_path = drive_result_path + '/' + model_file   # \"/content/drive/My Drive/Python Projects/Network Dissection/NetDissect-Lite-master/zoo/\" + model_file\n",
        "    !cp \"$drive_model_path\" '.'\n",
        "\n",
        "    # if dataset_pure_name in ['imagenette', 'imagewoof', 'places']:\n",
        "    #     dataset_dir = dataset_name + '/train'\n",
        "\n",
        "    #     # Removing any train.txt or val.txt file that may interfere with loading the dataset properly: \n",
        "    #     for f in os.listdir(dataset_name):\n",
        "    #         if f.endswith(\".txt\"):\n",
        "    #             os.remove(os.path.join(dataset_name, f))\n",
        "\n",
        "# Optional: loading the segmenter models to avoid downloading them from netdissect server: \n",
        "segmodel_dir = 'segmodel'\n",
        "segmodel_file = segmodel_dir + '.zip'\n",
        "drive_segmodel_path = '/content/drive/My Drive/Python Projects/Pretrained Models/' + segmodel_file\n",
        "target_segmodel_dir = 'datasets'\n",
        "if not os.path.exists(target_segmodel_dir):\n",
        "    os.makedirs(target_segmodel_dir)\n",
        "\n",
        "target_segmodel_file = target_segmodel_dir + '/' + segmodel_file\n",
        "!cp \"$drive_segmodel_path\" $target_segmodel_dir\n",
        "!unzip -qq -n $target_segmodel_file -d $target_segmodel_dir\n",
        "\n",
        "# if exclude_similar_concepts and (len(excluded_concepts) > 0):\n",
        "#     seglabels_path = target_segmodel_dir + '/' + segmodel_dir + '/upp-resnet50-upernet/labels.json'\n",
        "#     exclude_concepts_from_segmenter(seglabels_path)"
      ],
      "metadata": {
        "id": "KNfg33wXqhse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6LitNeJFct8"
      },
      "outputs": [],
      "source": [
        "model = load_model(model_file)\n",
        "model.retain_layer(target_layer)\n",
        "\n",
        "dataset = load_dataset(dataset_dir)\n",
        "classlabels = dataset.classes\n",
        "sample_size = len(dataset)\n",
        "\n",
        "print('Inspecting layer %s of model %s on dataset %s' % (target_layer, model_name, dataset_name))\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "upfn = experiment.make_upfn(args, dataset, model, target_layer)\n",
        "renorm = renormalize.renormalizer(dataset, target='zc')\n",
        "segmodel, seglabels, segcatlabels = experiment.setting.load_segmenter(seg_model_name)\n",
        "\n",
        "print('Segmentation labels:')\n",
        "for i,lbl in enumerate(seglabels):\n",
        "    print('{}: {} from category {}'.format(i, lbl, segcatlabels[i]))"
      ],
      "metadata": {
        "id": "-eb668CQ9941"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHwqIxR5FcuA"
      },
      "outputs": [],
      "source": [
        "\n",
        "batch_indices = [10, 20, 30, 40, 50, 60, 70, 80]\n",
        "batch = torch.cat([dataset[i][0][None,...] for i in batch_indices])\n",
        "show_sample_images(model, dataset, batch, batch_indices, classlabels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4CXMp3oFcuA"
      },
      "outputs": [],
      "source": [
        "\n",
        "show_sample_segmentations(segmodel, dataset, batch, renorm)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7jnQGCFFcuB"
      },
      "outputs": [],
      "source": [
        "\n",
        "show_sample_heatmaps(model, dataset, batch)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyPNvbGVFcuD"
      },
      "outputs": [],
      "source": [
        "\n",
        "rq = compute_tally_quantile(model, dataset, upfn, sample_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSLM0R-PFcuE"
      },
      "outputs": [],
      "source": [
        "\n",
        "topk = compute_tally_topk(model, dataset, sample_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4WkrbzcFcuE"
      },
      "outputs": [],
      "source": [
        "\n",
        "show_sample_image_activation(model, dataset, rq, topk, classlabels, sample_unit_number=2, sample_image_index=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7tHlCY2FcuF"
      },
      "outputs": [],
      "source": [
        "\n",
        "unit_images = save_top_channel_images(model, dataset, rq, topk)\n",
        "sample_unit_numbers = [10, 20, 30, 40]\n",
        "show_sample_channel_images(unit_images, sample_unit_numbers)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-6r5CDaFcuG"
      },
      "outputs": [],
      "source": [
        "\n",
        "unit_label_high, label_list, level_high, level_low = compute_top_channel_concepts(model, segmodel, upfn, dataset, rq, seglabels, segcatlabels, sample_size, renorm)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "V52BH0ZeFcuH"
      },
      "outputs": [],
      "source": [
        "\n",
        "show_sample_channel_images(unit_images, sample_unit_numbers, unit_label_high)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "save_final_data(unit_label_high, label_list, level_high, level_low)\n"
      ],
      "metadata": {
        "id": "zKyLsgHXl3az"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_file = result_dir + '.zip'\n",
        "!zip -qq -r $result_file $result_dir\n",
        "!cp $result_file '$drive_result_path'\n",
        "\n",
        "packages_path = drive_result_path + '/identification_packages.log'\n",
        "save_imported_packages(packages_path)"
      ],
      "metadata": {
        "id": "H3ovevIrRtUe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}