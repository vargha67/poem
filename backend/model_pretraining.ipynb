{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBbNCKx_JqzL"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "YM88WHecgeg9"
      },
      "source": [
        "import os, shutil, pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import OrderedDict\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader, Subset, sampler, random_split\n",
        "from torchvision import datasets, transforms, models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "q7GKNbqNgehF"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Cuda available:\", torch.cuda.is_available())\n",
        "\n",
        "seed = 1\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if device.type == 'cuda':\n",
        "    torch.cuda.manual_seed_all(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_settings = {\n",
        "    \"resnet18\": {'target_layer': 'layer4.1.conv2'}, \n",
        "    \"resnet50\": {'target_layer': 'layer4.2.conv3'}, \n",
        "    \"vgg16\": {'target_layer': 'features.conv5_3'},   # 'features.28'\n",
        "    \"alexnet\": {'target_layer': 'conv5'}\n",
        "}\n",
        "\n",
        "model_dataset_settings = {\n",
        "    \"resnet18_indoor_bedroom_kitchen\": {'load_model': False, 'num_classes': 2, 'base_num_classes': 1000},\n",
        "    \"resnet18_places_bedroom_kitchen\": {'load_model': True, 'num_classes': 2, 'base_num_classes': 365},\n",
        "    \"resnet18_places_bedroom_kitchen_livingroom\": {'load_model': True, 'num_classes': 3, 'base_num_classes': 365},\n",
        "    \"resnet18_places_coffeeshop_restaurant\": {'load_model': True, 'num_classes': 2, 'base_num_classes': 365},\n",
        "    \"resnet18_imagenet_minivan_pickup\": {'load_model': False, 'num_classes': 2, 'base_num_classes': 1000, 'excluded_concepts': ['pickup', 'van']},   # ['car', 'bus', 'coach', 'truck', 'van']\n",
        "    \"resnet18_imagenet_laptop_mobile\": {'load_model': False, 'num_classes': 2, 'base_num_classes': 1000, 'excluded_concepts': ['laptop', 'mobile', 'computer']},\n",
        "    \"resnet50_places_bedroom_kitchen\": {'load_model': True, 'num_classes': 2, 'base_num_classes': 365}, \n",
        "    \"resnet50_places_bedroom_kitchen_livingroom\": {'load_model': True, 'num_classes': 3, 'base_num_classes': 365},\n",
        "    \"resnet50_places_coffeeshop_restaurant\": {'load_model': True, 'num_classes': 2, 'base_num_classes': 365},\n",
        "    \"resnet50_imagenet_minivan_pickup\": {'load_model': False, 'num_classes': 2, 'base_num_classes': 1000, 'excluded_concepts': ['pickup', 'van']},\n",
        "    \"vgg16_places_bedroom_kitchen\": {'load_model': True, 'num_classes': 2, 'base_num_classes': 365}, \n",
        "    \"vgg16_places_coffeeshop_restaurant\": {'load_model': True, 'num_classes': 2, 'base_num_classes': 365},\n",
        "    \"vgg16_places_bedroom_kitchen_livingroom\": {'load_model': True, 'num_classes': 3, 'base_num_classes': 365},\n",
        "    \"vgg16_imagenet_minivan_pickup\": {'load_model': False, 'num_classes': 2, 'base_num_classes': 1000, 'excluded_concepts': ['pickup', 'van']}\n",
        "}\n",
        "\n",
        "settings = {\n",
        "    \"model_settings\": model_settings,\n",
        "    \"model_dataset_settings\": model_dataset_settings\n",
        "}\n",
        "\n",
        "settings_path = '/content/drive/My Drive/Python Projects/POEM Pipeline Results/settings.pkl' \n",
        "with open(settings_path, 'wb') as f:\n",
        "    pickle.dump(settings, f)"
      ],
      "metadata": {
        "id": "0-0NEIRuJt-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "MVzi_4f1gehG"
      },
      "source": [
        "current_setting_path = '/content/drive/My Drive/Python Projects/POEM Pipeline Results/current_setting.txt'\n",
        "with open(current_setting_path, 'r') as f:\n",
        "    current_setting_title = f.read().splitlines()[0]\n",
        "    print('Current setting:', current_setting_title)\n",
        "\n",
        "title_parts = current_setting_title.split('_')\n",
        "model_name = title_parts[0]   # custom, resnet18, resnet50, vgg16, alexnet\n",
        "dataset_name = '_'.join(title_parts[1:])   # cifar10, imagenette, imagewoof, indoor, places, imagenet\n",
        "dataset_pure_name = dataset_name.split('_')[0]\n",
        "\n",
        "current_setting = model_dataset_settings[current_setting_title] \n",
        "load_model_from_disk = current_setting['load_model']   # Is true in case of having a model file pretrained on the target dataset (e.g. Places365) rather than the default dataset of Torchvision which is ImageNet\n",
        "num_classes = current_setting['num_classes']\n",
        "base_num_classes = current_setting['base_num_classes']\n",
        "\n",
        "pretrain_mode = 'feature_extraction'   # full_fine_tuning, partial_fine_tuning, feature_extraction\n",
        "target_classes = []   # ['n02086240', 'n02087394']\n",
        "\n",
        "train_ratio = 0.7\n",
        "image_size = 224\n",
        "batch_size = 64\n",
        "epochs = 100\n",
        "stop_patience = 10\n",
        "cnn_dropout = 0.0\n",
        "dense_dropout = 0.0\n",
        "\n",
        "norm_mean = (0.485, 0.456, 0.406)\n",
        "norm_std = (0.229, 0.224, 0.225)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_imported_packages(packages_path):\n",
        "\n",
        "    # Saving imported packages and their versions: \n",
        "    import sys\n",
        "    modules_info = []\n",
        "\n",
        "    for module in sys.modules:\n",
        "        if len(module.split('.')) > 1:   # ignoring subpackages\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            modules_info.append((module, sys.modules[module].__version__))\n",
        "        except:\n",
        "            try:\n",
        "                if type(sys.modules[module].version) is str:\n",
        "                    modules_info.append((module, sys.modules[module].version))\n",
        "                else:\n",
        "                    modules_info.append((module, sys.modules[module].version()))\n",
        "            except:\n",
        "                try:\n",
        "                    modules_info.append((module, sys.modules[module].VERSION))\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "    modules_info.sort(key=lambda x: x[0])\n",
        "    with open(packages_path, 'w') as f:\n",
        "        for m in modules_info:\n",
        "            f.write('{} {}\\n'.format(m[0], m[1]))"
      ],
      "metadata": {
        "id": "KhUVhYGaKoa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "jr8Zs1AIgehH"
      },
      "source": [
        "class ImageDataset (Dataset):\n",
        "    \n",
        "    def __init__(self, images_path, file_names, labels, transform):\n",
        "        self.images_path = images_path\n",
        "        self.file_names = file_names\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.file_names)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        fname = self.file_names[index]\n",
        "        path = os.path.join(self.images_path, fname)\n",
        "        img = Image.open(path)\n",
        "        \n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        \n",
        "        if self.labels is not None:\n",
        "            label = self.labels[index]\n",
        "            return img, label\n",
        "        \n",
        "        return img, fname"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This class helps to be able to access class index, targets and file paths of the dataset, which are absent in the Subset instance itself\n",
        "class ImageSubset (Subset):\n",
        "\n",
        "    def __init__(self, dataset, indexes):\n",
        "        super().__init__(dataset, indexes)\n",
        "        self.class_to_idx = dataset.class_to_idx\n",
        "        self.targets = dataset.targets\n",
        "        self.samples = [s for i,s in enumerate(dataset.samples) if i in indexes]"
      ],
      "metadata": {
        "id": "NFfhrYofm644"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "nbjMhKpPgehH"
      },
      "source": [
        "def prepare_data (images_path=None, labels_path=None): \n",
        "\n",
        "    train_transform = transforms.Compose([\n",
        "        #transforms.RandomResizedCrop(image_size),\n",
        "        transforms.Resize(256),\n",
        "        transforms.RandomCrop(image_size),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=norm_mean, std=norm_std)\n",
        "    ])\n",
        "    \n",
        "    valid_transform = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(image_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=norm_mean, std=norm_std)\n",
        "    ])\n",
        "\n",
        "    train_set = None\n",
        "    valid_set = None\n",
        "    train_dataset_dir = dataset_name\n",
        "    valid_dataset_dir = dataset_name\n",
        "\n",
        "    if dataset_pure_name in ['places']:\n",
        "        train_dataset_dir += '/train'\n",
        "        valid_dataset_dir += '/train'   # Because the validation set is very small, we just use and split the train set\n",
        "    elif dataset_pure_name in ['imagenette', 'imagewoof']:\n",
        "        train_dataset_dir += '/train'\n",
        "        valid_dataset_dir += '/val'\n",
        "    elif dataset_pure_name in ['cifar10']:\n",
        "        train_dataset_dir = './data'\n",
        "        valid_dataset_dir = './data'\n",
        "\n",
        "    # Loading or computing the train and validation sets for different datasets: \n",
        "    if dataset_pure_name in ['indoor', 'places', 'imagenet']:\n",
        "        train_set = datasets.ImageFolder(root=train_dataset_dir, transform=train_transform)\n",
        "        valid_set = datasets.ImageFolder(root=valid_dataset_dir, transform=valid_transform)\n",
        "\n",
        "        data_size = len(train_set)\n",
        "        indexes = list(range(data_size))\n",
        "        np.random.shuffle(indexes)\n",
        "        train_size = int(train_ratio * data_size)\n",
        "        train_indexes = indexes[:train_size]\n",
        "        valid_indexes = indexes[train_size:]\n",
        "\n",
        "        train_set = ImageSubset(train_set, train_indexes)\n",
        "        valid_set = ImageSubset(valid_set, valid_indexes)\n",
        "\n",
        "        # Can't use the code below, because training and validation data have different transforms to apply: \n",
        "        # valid_size = data_size - train_size\n",
        "        # train_set, valid_set = random_split(train_set, [train_size, valid_size], generator=torch.Generator().manual_seed(seed))\n",
        "\n",
        "    elif dataset_pure_name in ['imagenette', 'imagewoof']:\n",
        "        train_set = datasets.ImageFolder(root=train_dataset_dir, transform=train_transform)\n",
        "        valid_set = datasets.ImageFolder(root=valid_dataset_dir, transform=valid_transform)\n",
        "\n",
        "    elif dataset_pure_name == 'cifar10':\n",
        "        train_set = datasets.CIFAR10(root=train_dataset_dir, train=True, transform=train_transform, download=True)\n",
        "        valid_set = datasets.CIFAR10(root=valid_dataset_dir, train=False, transform=valid_transform, download=True)\n",
        "\n",
        "    else: \n",
        "        data = pd.read_csv(labels_path)\n",
        "        file_names = data['File'].tolist()\n",
        "        labels = data['Label'].tolist()\n",
        "\n",
        "        train_file_names, valid_file_names, train_labels, valid_labels = train_test_split(file_names, labels, test_size= 1.0 - train_ratio, \n",
        "                                                                                          random_state=seed, stratify=labels)\n",
        "        \n",
        "        train_set = ImageDataset(images_path, train_file_names, train_labels, transform=train_transform)\n",
        "        valid_set = ImageDataset(images_path, valid_file_names, valid_labels, transform=valid_transform)\n",
        "\n",
        "    # In case we want to focus on selected target classes among all the classes in the dataset: \n",
        "    if (target_classes != None) and (len(target_classes) > 0):\n",
        "        class_indexes_dict = train_set.class_to_idx\n",
        "        train_indexes = train_set.targets\n",
        "        valid_indexes = valid_set.targets\n",
        "\n",
        "        target_class_indexes = [v for k,v in class_indexes_dict.items() if k in target_classes]\n",
        "        target_train_indexes = [i for i,x in enumerate(train_indexes) if x in target_class_indexes]\n",
        "        target_valid_indexes = [i for i,x in enumerate(valid_indexes) if x in target_class_indexes]\n",
        "        print('target_class_indexes:', target_class_indexes)\n",
        "        print('target_train_indexes: {}, target_valid_indexes: {}'.format(len(target_train_indexes), len(target_valid_indexes)))\n",
        "\n",
        "        train_set = ImageSubset(train_set, target_train_indexes)\n",
        "        valid_set = ImageSubset(valid_set, target_valid_indexes)\n",
        "\n",
        "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "    valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True)\n",
        "    \n",
        "    return train_loader, valid_loader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_data_subset (data_subset, dataset_dir, target_dataset_dir):\n",
        "\n",
        "    images_info = data_subset.samples\n",
        "    class_to_idx = data_subset.class_to_idx\n",
        "    print('class_to_idx:', class_to_idx)\n",
        "    #idx_to_class = {v:k for k,v in class_to_idx.items()}\n",
        "    class_names = list(class_to_idx.keys())\n",
        "    idx_to_class_dir = {}\n",
        "\n",
        "    if not os.path.exists(target_dataset_dir):\n",
        "        print('Making target dataset dir:', target_dataset_dir)\n",
        "        os.makedirs(target_dataset_dir)\n",
        "\n",
        "    for cname in class_names:\n",
        "        idx = class_to_idx[cname]\n",
        "        target_class_dir = target_dataset_dir + '/' + cname\n",
        "        idx_to_class_dir[idx] = target_class_dir\n",
        "        if not os.path.exists(target_class_dir):\n",
        "            os.makedirs(target_class_dir)\n",
        "\n",
        "    print('idx_to_class_dir:', idx_to_class_dir)\n",
        "\n",
        "    for i,(path, label) in enumerate(images_info):\n",
        "        target_class_dir = idx_to_class_dir[label]\n",
        "        ind = path.rfind('/')\n",
        "        fname = path[ind+1:]\n",
        "        target_path = target_class_dir + '/' + fname\n",
        "        if i == 0:\n",
        "            print('Label: {}, fname: {}, path: {}, target_path: {}'.format(label, fname, path, target_path))\n",
        "        shutil.copy(path, target_path)"
      ],
      "metadata": {
        "id": "OUIlTfndL2pl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "7tctKxCsgehJ"
      },
      "source": [
        "class ConvNet (nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(ConvNet, self).__init__()\n",
        "\n",
        "        self.conv_layer1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        self.conv_layer2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Dropout(p=cnn_dropout)\n",
        "        )\n",
        "\n",
        "        self.conv_layer3 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        self.conv_layer4 = nn.Sequential(\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Dropout(p=cnn_dropout)\n",
        "        )\n",
        "\n",
        "        self.conv_layer5 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        self.conv_layer6 = nn.Sequential(\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Dropout(p=cnn_dropout)\n",
        "        )\n",
        "\n",
        "        self.fc_layer1 = nn.Sequential(\n",
        "            nn.Linear(128 * 4 * 4, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=dense_dropout)\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Linear(256, num_classes)\n",
        "        \n",
        "        \n",
        "    def forward(self, x): \n",
        "        out = self.conv_layer1(x)\n",
        "        out = self.conv_layer2(out)\n",
        "        out = self.conv_layer3(out)\n",
        "        out = self.conv_layer4(out)\n",
        "        out = self.conv_layer5(out)\n",
        "        out = self.conv_layer6(out)\n",
        "\n",
        "        out = out.view(-1, 128 * 4 * 4)\n",
        "        out = self.fc_layer1(out)\n",
        "        out = self.classifier(out)\n",
        "        \n",
        "        return out\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vgg16_model (*args, **kwargs):\n",
        "\n",
        "    # A version of vgg16 model where layers are given their research names: \n",
        "    model = models.vgg16(*args, **kwargs)\n",
        "    model.features = nn.Sequential(OrderedDict(zip([\n",
        "        'conv1_1', 'relu1_1',\n",
        "        'conv1_2', 'relu1_2',\n",
        "        'pool1',\n",
        "        'conv2_1', 'relu2_1',\n",
        "        'conv2_2', 'relu2_2',\n",
        "        'pool2',\n",
        "        'conv3_1', 'relu3_1',\n",
        "        'conv3_2', 'relu3_2',\n",
        "        'conv3_3', 'relu3_3',\n",
        "        'pool3',\n",
        "        'conv4_1', 'relu4_1',\n",
        "        'conv4_2', 'relu4_2',\n",
        "        'conv4_3', 'relu4_3',\n",
        "        'pool4',\n",
        "        'conv5_1', 'relu5_1',\n",
        "        'conv5_2', 'relu5_2',\n",
        "        'conv5_3', 'relu5_3',\n",
        "        'pool5'],\n",
        "        model.features)))\n",
        "\n",
        "    model.classifier = nn.Sequential(OrderedDict(zip([\n",
        "        'fc6', 'relu6',\n",
        "        'drop6',\n",
        "        'fc7', 'relu7',\n",
        "        'drop7',\n",
        "        'fc8a'],\n",
        "        model.classifier)))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "kHt40YSBOh-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhy1QwfMkPMx"
      },
      "source": [
        "def pretrained_model (base_model_file=None):\n",
        "    \n",
        "    # When loading the model from disk, the \"pretrained\" parameter should be false and \"num_classes\" should be set based on the classes in the loaded model file\n",
        "    # When not loading the model from disk, the \"pretrained\" parameter should be true, and \"num_classes\" should not be provided, \n",
        "    # or it should be equal to the classes in the pretrained model (e.g. ImageNet in PyTorch)\n",
        "    pretrained = (not load_model_from_disk)\n",
        "    model = None\n",
        "    partial_target_layers = []\n",
        "\n",
        "    if model_name == 'vgg16':\n",
        "        model = vgg16_model(pretrained=pretrained, num_classes=base_num_classes)\n",
        "    else:\n",
        "        model = models.__dict__[model_name](pretrained=pretrained, num_classes=base_num_classes)\n",
        "\n",
        "    if load_model_from_disk:\n",
        "        #model = models.__dict__[model_name](num_classes=base_num_classes)\n",
        "        checkpoint = torch.load(base_model_file)\n",
        "        statedict = checkpoint\n",
        "        if 'state_dict' in checkpoint:\n",
        "            statedict = {str.replace(k,'module.',''): v for k,v in checkpoint['state_dict'].items()}\n",
        "        model.load_state_dict(statedict)\n",
        "    # else:\n",
        "    #     model = models.__dict__[model_name](pretrained=pretrained)\n",
        "\n",
        "    if model_name == 'resnet18':\n",
        "        partial_target_layers = ['layer3', 'layer4', 'avgpool']\n",
        "\n",
        "    if pretrain_mode == 'partial_fine_tuning':\n",
        "        for name, child in model.named_children():\n",
        "            if name in partial_target_layers:\n",
        "                for p in child.parameters():\n",
        "                    p.requires_grad = True\n",
        "            else:\n",
        "                for p in child.parameters():\n",
        "                    p.requires_grad = False\n",
        "    elif pretrain_mode == 'feature_extraction':\n",
        "        for p in model.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "    if model_name.startswith('resnet'):\n",
        "        model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "    elif model_name == 'alexnet':\n",
        "        model.classifier[6] = nn.Linear(model.classifier[6].in_features, num_classes)\n",
        "    elif model_name == 'vgg16':\n",
        "        model.classifier.fc8a = nn.Linear(model.classifier.fc8a.in_features, num_classes)   # model.classifier[6] = nn.Linear(model.classifier[6].in_features, num_classes)\n",
        "        \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "fDG2732WgehJ"
      },
      "source": [
        "def train_epoch (model, optimizer, criterion, train_loader, valid_loader):\n",
        "    \n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    train_acc = 0\n",
        "    train_steps = 0\n",
        "    train_size = 0\n",
        "    \n",
        "    for i, (images, labels) in tqdm(enumerate(train_loader)):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "    \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        #labels = labels.unsqueeze(1).float()   # reshape to 2 dimensions\n",
        "        output = model(images)\n",
        "        loss = criterion(output, labels)\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        loss = loss.cpu()\n",
        "        train_loss += loss.item()\n",
        "        _, preds = torch.max(output, 1)\n",
        "        preds = preds.cpu()\n",
        "        labels = labels.cpu()\n",
        "        train_acc += (preds == labels).float().sum()\n",
        "\n",
        "        train_steps += 1\n",
        "        train_size += len(preds)\n",
        "    \n",
        "    train_loss = train_loss / train_steps\n",
        "    train_acc = train_acc / train_size   #len(train_loader.dataset)\n",
        "    \n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    val_acc = 0\n",
        "    val_steps = 0\n",
        "    val_size = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i, (images, labels) in tqdm(enumerate(valid_loader)):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            #labels = labels.unsqueeze(1).float()\n",
        "            output = model(images)\n",
        "            vloss = criterion(output, labels)\n",
        "            \n",
        "            vloss = vloss.cpu()\n",
        "            val_loss += vloss.item()\n",
        "            \n",
        "            _, preds = torch.max(output, 1)\n",
        "            preds = preds.cpu()\n",
        "            labels = labels.cpu()\n",
        "            val_acc += (preds == labels).float().sum()\n",
        "\n",
        "            val_steps += 1\n",
        "            val_size += len(preds)\n",
        "            \n",
        "    val_loss = val_loss / val_steps\n",
        "    val_acc = val_acc / val_size   #len(valid_loader.dataset)\n",
        "    \n",
        "    return train_loss, train_acc, val_loss, val_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "9S8bS7TEgehK"
      },
      "source": [
        "def train (model, model_path, train_loader, valid_loader): \n",
        "\n",
        "    model_params = [p for p in model.parameters() if p.requires_grad]\n",
        "    print('Number of params to learn:', len(model_params))\n",
        "    \n",
        "    optimizer = optim.Adam(model_params, lr=0.001, weight_decay=1e-6)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    train_losses = []\n",
        "    train_accs = []\n",
        "    val_losses = []\n",
        "    val_accs = []\n",
        "    best_acc = 0\n",
        "    best_epoch = 0\n",
        "    no_progress = 0\n",
        "    \n",
        "    for e in range(epochs):\n",
        "        print('\\nEpoch {}/{}'.format(e+1, epochs))\n",
        "        train_loss, train_acc, val_loss, val_acc = train_epoch(model, optimizer, criterion, train_loader, valid_loader)\n",
        "        \n",
        "        print('loss: {:.3f} - acc: {:.3f} - val loss: {:.3f} - val acc: {:.3f}'.format(train_loss, train_acc, val_loss, val_acc))\n",
        "        \n",
        "        train_losses.append(train_loss)\n",
        "        train_accs.append(train_acc)\n",
        "        val_losses.append(val_loss)\n",
        "        val_accs.append(val_acc)\n",
        "        \n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            best_epoch = e\n",
        "            no_progress = 0\n",
        "            torch.save(model.state_dict(), model_path)\n",
        "        else:\n",
        "            no_progress += 1\n",
        "            \n",
        "        if no_progress >= stop_patience:\n",
        "            print('Finished training in epoch {} because of no progress in {} consecutive epochs'.format(e, no_progress))\n",
        "            break;\n",
        "    \n",
        "    print('Best validation accuracy: {:.3f} (epoch {})'.format(best_acc, best_epoch))\n",
        "    return train_losses, train_accs, val_losses, val_accs\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "FQiAPjaHgehM"
      },
      "source": [
        "def plot_results (train_losses, train_accs, val_losses, val_accs):\n",
        "    \n",
        "    # loss: \n",
        "    plt.plot(train_losses)\n",
        "    plt.plot(val_losses)\n",
        "    plt.title('Loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train loss', 'validation loss'], loc='upper right')\n",
        "    plt.show()\n",
        "    \n",
        "    # accuracy: \n",
        "    plt.plot(train_accs)\n",
        "    plt.plot(val_accs)\n",
        "    plt.title('Accuracy')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train accuracy', 'validation accuracy'], loc='lower right')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VooAAmeCaOaB"
      },
      "source": [
        "dataset_file = dataset_name + '.zip'\n",
        "drive_dataset_dir = '/content/drive/My Drive/Python Projects/Datasets/' + dataset_file\n",
        "!cp \"$drive_dataset_dir\" '.'\n",
        "!unzip -qq -n $dataset_file -d '.'\n",
        "\n",
        "base_model_file = None\n",
        "if load_model_from_disk:\n",
        "    base_model_file = model_name + '_' + dataset_pure_name + '.pth'\n",
        "    base_model_path = \"/content/drive/My Drive/Python Projects/Pretrained Models/\" + base_model_file\n",
        "    !cp \"$base_model_path\" '.'\n",
        "\n",
        "# if dataset_name == 'imagenette':\n",
        "#     !cp '/content/drive/My Drive/Python Projects/Other Data/imagenette2-320.tgz' '.'\n",
        "#     !tar -xf 'imagenette2-320.tgz'\n",
        "\n",
        "# elif dataset_name == 'imagewoof':\n",
        "#     !cp '/content/drive/My Drive/Python Projects/Other Data/imagewoof2-320.tgz' '.'\n",
        "#     !tar -xf 'imagewoof2-320.tgz'\n",
        "\n",
        "# elif dataset_name == 'indoor':\n",
        "#     !cp '/content/drive/My Drive/Python Projects/Other Data/indoor_subset.zip' '.'\n",
        "#     !unzip 'indoor_subset.zip' -d '.'\n",
        "\n",
        "# elif dataset_name == 'places2':\n",
        "#     !cp '/content/drive/My Drive/Python Projects/Other Data/places2.zip' '.'\n",
        "#     !unzip 'places2.zip' -d '.'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "E_IAeViogehM"
      },
      "source": [
        "train_loader, valid_loader = prepare_data()\n",
        "valid_dataset_dir = 'dataset'\n",
        "save_data_subset(valid_loader.dataset, dataset_name, valid_dataset_dir)\n",
        "\n",
        "model = None\n",
        "if model_name == 'custom':\n",
        "    model = ConvNet()\n",
        "else:\n",
        "    model = pretrained_model(base_model_file)\n",
        "\n",
        "print(model)\n",
        "model = model.to(device)\n",
        "\n",
        "model_path = 'model.pth'   # model_name + '_' + dataset_name + '.pth'\n",
        "train_losses, train_accs, val_losses, val_accs = train(model, model_path, train_loader, valid_loader)\n",
        "\n",
        "plot_results(train_losses, train_accs, val_losses, val_accs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive_result_path = '/content/drive/My Drive/Python Projects/POEM Pipeline Results/' + model_name + '_' + dataset_name\n",
        "if not os.path.exists(drive_result_path):\n",
        "    os.makedirs(drive_result_path)\n",
        "\n",
        "!cp $model_path '$drive_result_path'\n",
        "\n",
        "valid_dataset_file = valid_dataset_dir + '.zip'\n",
        "!zip -qq -r $valid_dataset_file $valid_dataset_dir\n",
        "!cp $valid_dataset_file '$drive_result_path'\n",
        "\n",
        "packages_path = drive_result_path + '/pretraining_packages.log'\n",
        "save_imported_packages(packages_path)"
      ],
      "metadata": {
        "id": "mgLQd_apfOYj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}